# LLM_based_Segmentation
Comparison of LLM-based semantic segmentation strategies for medical images

This project utilizes Large Language Models (LLMs) to generate code for medical image segmentation tasks using U-Net architectures. We compare the performance of various LLMs in generating code for U-Net-based segmentation models and track the results on multiple datasets. The models are evaluated on their ability to generate functional code out-of-the-box, accuracy in segmentation, and error frequency during development.

---

## Table of Contents
- [Project Overview](#project-overview)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Models and LLMs](#models-and-llms)
- [Datasets](#datasets)
- [Results](#results)
- [License](#license)


---

## Project Overview
This repository explores how Large Language Models (LLMs) perform when tasked with generating code for medical image segmentation. We use open and closed-source LLMs to create U-Net-based models for semantic segmentation on several medical image datasets, focusing on the ease of generation, minimum required modifications and error frequency, and compare the performance across the different LLM generated model outputs.
The models were prompted with the same engineered prompt, and asked to generate a dataloader, training, model and main script. Errors were feeded back to the model and suggested fixes were used to debug the codes untill it ran error-free.

We use the **Dice coefficient** as the key evaluation metric for segmentation quality and track error rates and types, and interactions required with each LLM to make the code run through. The final models are evaluated on three datasets to measure their performance on real-world medical image segmentation tasks.

---

## Features
- **Multiple LLM Comparisons**: Assess how different LLMs perform in generating code for a U-Net-based segmentation model.
- **Medical Datasets**: Utilize real-world medical datasets for evaluation.
- **Dice Score Evaluation**: Compare Dice scores for each model on different datasets.
- **Error Tracking**: Record error rates and debugging frequency for each LLM.
- **Generated U-Net Architectures**: Each model is trained using the architecture generated by different LLMs.

---

## Installation

### Prerequisites
Ensure you have the following installed:
- Python 3.6 or higher
- PyTorch
- CUDA (optional for GPU support)

### Steps to Install

1. Clone the repository:
   ```bash
   git clone https://github.com/ankilab/LLM_based_Segmentation.git
   cd LLM_based_Segmentation
2. Installing required packages:
   pip install -r requirements.txt

3. Running the Models:
You can train and evaluate the models using the scripts and main.py provided for each LLM-generated architecture, in it's respective folder.

## Models and LLMs
This project uses the following LLMs to generate U-Net architectures:

GPT-4 (Closed-source)
GPT-4o (Open-source)
Claude 3.5 Sonnet (Open-source)
LLAMA 3.1 405B (Open-source)
Gemini 1.5 Pro (Open-source)
Bing Microsoft Copilot (Closed-source)
Copilot (Closed-source)
GPT-o1 Preview (Open-source)
Key differences between these models include the number of encoder/decoder stages, convolutional block design, use of batch normalization, skip connections, bottleneck size, and final activation functions. These variations contribute to differences in segmentation performance and ease of model generation.

## Results
### Key Metrics
Error Comparison: Models such as GPT-o1 Preview and Claude had 0 errors and ran successfully without modifications, while others like Gemini and Copilot required more fixes.
Training & Validation Losses: Validation losses varied across epochs for each model.
Test Dice Scores: Dice scores varied significantly across datasets, with GPT-4o and Claude models showing better performance overall.
Visualized Outputs: Visualizations of segmentation outputs were saved and compared across models.

## Summary
We found that while most LLMs generated functional code, some models required more debugging than others. GPT-o1 Preview and Claude generated highly efficient architectures with minimal intervention. In contrast, models like LLAMA 3.1 performed poorly in comparison, particularly on complex datasets like the Brain Tumor dataset.






