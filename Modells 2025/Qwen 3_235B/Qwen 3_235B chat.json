ï¶	provide a complete ready-to-run Python implementation for a U-Net architecture using PyTorch, for binary segmentation of grayscale images. The dataset should consist of grayscale images and corresponding binary masks, which are located in 2 different folders or in the same path (the masks have the same name as the images, or with a suffix, like: "number.png" and the masks "number_seg.png", around 5000 samples in total. access and split directly using train_test_split on the index list and Subset, and do NOT duplicate or copy to new directory). It should work and provide correct predictions, and save the losses and dice scores as instructed below. The code should be structured into importable scripts with the following:

1.	A custom Dataset class for loading the grayscale images and corresponding binary masks. (only the png files, ignore meta files or additional files in the directory)
2.	A UNet model class definition using PyTorch, optimized for binary segmentation (with small foreground, mostly background)
3.	Functions for the training procedure, validation procedure, and Testing procedure.

Considerations for each part:
4.	Ensure needed transforms on input image and mask pairs (Re-size images correctly, ensure gray scale and binaryâ€¦)
5.	Avoid overfitting and data leakage (without using augmentations), choose appropriate hyperparameters, learning rate, batch size, etc

â€¢	For Training and Validation:

6.	Compute training and validation losses.
7.	Track and save the average training loss for each epoch in a separate Excel file (train_losses.xlsx) in a given save path. The first row should contain the epoch numbers, and the second row should contain the corresponding average training loss for each epoch.
8.	Similarly, save the validation loss for each epoch in an Excel file (val_losses.xlsx) with the same structure.
9.	At the end of training, save both the entire model and the model's state dictionary (.pth files) in the save path
10.	Calculate total training time from start to end, and print at the end of training
11.	The progress, phase (train/validation), and all info should be shown epoch by epoch using `tqdm` progress bars during training, validation, and testing.
12.	A function to visualize training losses and validation losses over epochs afterwards, on the same plot. Save plot in a specified save path as png. 
13.	During Validation: mean Dice scores should be calculated over each batch, for each epoch, and stored in an Excel file (epochs as rows and batches as columns). The Excel file (named â€œvalidation_dice_scoresâ€ should be saved at the specified save path after validation is done.
14.	Do the same for the Testing and store dice the Dice scores the same way and save the excel file as â€œtest_dice_scoresâ€ after testing is done.

â€¢	For Testing:

15.	A function to visualize the predicted masks, the input image, and the ground truth, for 5 random samples, after testing is done. (subplots for 5 random samples in one plot, in 5 rows and 3 columns (title at top: Input Image, Ground Truth, Prediction). Also display image file name string above each image and mask. save the plot as `.png`).

â€¢	For Main:

16.	Shuffle the data, split the dataset into: 80% training, 10% validation, and 10% test sets using `train_test_split`. Print the sizes of each (how many images), and copy the splitted data (images and their respective masks) 
17.	DataLoader setup for training, validation, and testing sets.
18.	Ensure correct data loading, print data loader sizes.
19.	Also usig torchinfo, print model summary, and total number of model parameters (n learnable params)

20.	All the above functionalities should be implemented in a modular way so that they can be imported as scripts or classes.
21.	All visualizations should just be saved as png. No need to display the plots during running.
22.	Ensure that everything can be executed in an `if __name__ == "__main__":` block with instructions for loading the dataset, configuring hyperparameters, and saving the trained model.
The directory structure should only be:
unet_segmentation/
â”œâ”€â”€ dataset.py
â”œâ”€â”€ model.py
â”œâ”€â”€ train.py
â”œâ”€â”€ main.py

profile
Qwen3-235B-A22B
1:54 am
Thinking completed
Â·
38,912 tokens budget
Okay, let's tackle this problem. The user wants a complete U-Net implementation in PyTorch for binary segmentation of grayscale images. The code needs to be split into several modules and follow specific guidelines. Let me start by breaking down each part.

First, the custom Dataset class. The dataset has grayscale images and binary masks. The masks are either in the same folder with a suffix like "_seg.png" or in separate folders. The Dataset needs to load image-mask pairs correctly. I need to make sure that only .png files are considered, ignoring other files. Also, transforms should be applied to both image and mask. Since the images are grayscale, I should read them as single-channel. The masks are binary, so converting them to 0s and 1s is necessary.

Next, the U-Net model. Binary segmentation with small foreground. The standard U-Net has an encoder-decoder structure with skip connections. For binary output, the final layer should have a single channel with a sigmoid activation. Since the foreground is small, maybe using a dice loss or BCEWithLogitsLoss with class weights. But the user didn't specify loss, so perhaps default to BCEWithLogitsLoss. Also, the model should be efficient but not too big, considering around 5000 samples.

Training, validation, testing functions. Need to handle the loops for each phase. For training, optimizer, loss function, backprop. For validation and testing, calculate Dice score. The Dice score is calculated per batch and stored in Excel. Also, saving losses and Dice scores each epoch. Using tqdm to show progress bars.

Data loading: Split into 80% train, 10% val, 10% test using train_test_split. Since the data isn't copied, the indices are split. Using Subset to create datasets from the original dataset. DataLoader setup with appropriate batch sizes. Transforms: resize, convert to tensor, normalize for images (maybe to [0,1], but since grayscale, normalization might be done via ToTensor). For masks, ensure they are binary.

Hyperparameters: Learning rate, batch size, epochs. Need to choose reasonable defaults. Maybe batch size 32, learning rate 0.001. Adam optimizer. Early stopping or checkpointing could help prevent overfitting, but user didn't mention, so maybe not. Training time tracking.

Model summary using torchinfo. Need to install torchinfo. The model input should be (1, H, W), assuming images are resized to a fixed size. User didn't specify image size, so maybe default to 256x256. Or let the user choose, but code should handle resizing. So in transforms, Resize to a fixed size.

Excel files: Using pandas to write DataFrames to Excel. Each epoch's loss values are appended as rows. For Dice scores, each batch's score per epoch is saved as columns. So for validation_dice_scores.xlsx, each row is an epoch, each column a batch. Similarly for test.

Visualization functions: After training, plot training and validation losses on the same graph. Save as png. For testing, visualize 5 random samples with input, ground truth, prediction. Each subplot row has 3 images with titles. Also display filenames as titles. Saving that plot.

Main script: Putting everything together. Loading dataset, splitting, creating DataLoaders, initializing model, training, validation, testing, saving everything. Also printing model summary, parameters, data sizes, etc.

Potential issues: Ensuring that transforms are applied correctly to both image and mask. For example, random cropping or flipping would require the same transform for both. But user said no augmentations, so just Resize and ToTensor. Also, handling different mask naming conventions. The Dataset class should accept a mask suffix parameter.

Testing the code structure. Each module (dataset.py, model.py, train.py, main.py) should have their respective parts. The main.py runs the training loop when executed.

Let me outline each part step by step.

For dataset.py: Create a GrayscaleBinaryMaskDataset class. The __init__ takes image_dir, mask_dir (could be same as image_dir), mask_suffix. The __getitem__ loads image and corresponding mask. Apply transforms if any. The transforms should be composed of functions that handle both image and mask, like Resize, ToTensor. Wait, but PyTorch transforms like ToTensor() automatically handle PIL Images. So for transforms that affect both, like Resize, they can be applied to both image and mask. But for transforms that need to be same for both (like random flip), need to apply same random seed. But since no augmentations, just Resize and ToTensor.

Wait, the user says "Ensure needed transforms on input image and mask pairs (Re-size images correctly, ensure gray scale and binaryâ€¦)". So Resize to a fixed size, ToTensor for images (converts to tensor and scales to [0,1], since it's a PIL Image). For masks, ToTensor converts to tensor, but since they are binary, it's okay. But maybe after loading, the mask is converted to a binary mask (0 and 1). So in the Dataset, after loading the mask image, convert it to binary. Because sometimes masks might be saved as 0 and 255. So in the Dataset, after loading the mask, do mask = mask.convert('L') and then threshold to 0 and 1. Or divide by 255 to get 0 or 1. Because if the mask is saved as 0 and 255, dividing by 255 would make it 0 and 1. So in the Dataset's __getitem__, after loading mask as a grayscale image, convert to numpy array, threshold at 0.5, then convert to tensor. Alternatively, use transforms.Lambda to do that. But how?

Alternatively, in the Dataset's __getitem__, after loading the mask image, process it:

mask = Image.open(mask_path).convert('L')
mask = np.array(mask)
mask = (mask > 127.5).astype(np.uint8) # assuming 0 and 255, threshold to 0 or 1
mask = torch.from_numpy(mask).long()

But ToTensor would convert the image to float32, but for mask, we need to keep it as long. So perhaps in the Dataset, the image is converted to tensor and normalized (maybe not necessary if using ToTensor which scales to [0,1]), and the mask is converted to a binary tensor.

So the Dataset class should handle that.

For the model.py: Implement U-Net. The standard architecture has encoder blocks with double convolutions, downsampling, decoder blocks with upsampling and skip connections. For binary segmentation, the output channel is 1. The model should be defined as a class with encoder and decoder parts. Also, for small foreground, maybe using Dice loss or BCEWithLogitsLoss. But the loss function is handled in the training script.

In train.py: Functions for train, validate, test. Each function would loop over the DataLoader, compute loss, track metrics. For training, backprop; for validation/testing, compute Dice. The Dice score is calculated per batch. For each batch, Dice is (2*intersection + smooth)/(union + smooth). Need to implement that. Also, during validation, collect all Dice scores per batch, and save to Excel. Similarly for testing.

For saving losses and Dice scores: Use pandas DataFrames. Each epoch's average loss is saved as a row in the Excel file. For Dice, each batch's score is a column, with epochs as rows.

In main.py: All the setup. Load dataset, split into train/val/test. Create DataLoaders. Initialize model, optimizer, loss function. Train the model, track time. Save model, state dict. Plot losses. Test the model, save Dice scores and visualize predictions.

Hyperparameters: Learning rate 0.001, batch size 16 or 32. Epochs maybe 20. Image size, say, 256x256. The transforms.Resize((256, 256)).

Now, putting all together.

Potential pitfalls:

- Handling the mask filenames. If the images are in the same directory, mask_suffix is needed. For example, if image is 'img.png', mask could be 'img_seg.png'. So in the Dataset class, mask name is image name + suffix.

- DataLoader workers: If using transforms with NumPy operations, need to ensure that the Dataset is picklable. But using standard transforms should be okay.

- Model summary with torchinfo: The input size needs to be specified. For example, (1, 256, 256).

- Dice calculation: Need to flatten the tensors and compute per batch. Also, smooth to avoid division by zero.

- Saving Excel files: Each time appending a new row. However, the current plan is to collect all losses during training and save at the end. Or after each epoch, append to the Excel. Using pandas, perhaps create a DataFrame each time and save with mode 'a' or 'w'. But better to collect all losses during training (as lists) and save at the end.

Wait, the user says:

For each epoch, track and save average training loss in train_losses.xlsx, same for val. So after each epoch, append the loss to the Excel. But how to do that? Maybe collect all the losses during training (as lists), and after training, save to Excel. But the user instruction says during training, save each epoch. So perhaps after each epoch, write the current epoch's loss to the Excel. Similarly for validation.

But writing to Excel during training could be slow. Maybe better to collect all losses in lists, then save at the end. But the user says "track and save the average training loss for each epoch in a separate Excel file...". So probably collect during training and save after each epoch. But maybe using pandas, create a DataFrame where each column is an epoch, and rows are epoch numbers and losses. Wait, the first row contains epoch numbers, second the loss. So for train_losses.xlsx, sheet has row 0: [1,2,3,...], row 1: [loss1, loss2, ...].

Wait, the user says: first row epoch numbers, second average loss. So each Excel file has two rows: epochs and losses. Similarly for validation. So for each new epoch, append the value to the list, and when saving, overwrite the Excel file with updated data. That way, if training stops, the saved file has up to the last completed epoch.

But how to handle this in code. Each time, create a new DataFrame with the current epochs and losses, and save it. So for example, after epoch 1, the Excel has epochs 1, loss. After epoch 2, epochs 1,2 and losses. So during training, for each epoch, append the average loss to a list, then at the end of the epoch, save the entire list into the Excel.

Same for validation losses.

For Dice scores during validation and testing: each epoch's batches' Dice scores are stored as columns. So for each epoch, collect a list of Dice scores per batch, then save them as a row in the Excel. So each row in 'validation_dice_scores.xlsx' corresponds to an epoch, each column a batch's Dice.

Implementing that requires collecting all batch Dice scores for each validation epoch, then saving them as a DataFrame with each row being the epoch's batch results.

Now, coding each part.

Starting with dataset.py:

Implement the Dataset class. The __init__ reads all image files in image_dir, filtering .png. Then, for each image, the mask path is determined based on mask_dir and mask_suffix. The __getitem__ loads image and mask, applies transforms (same for both), converts image to grayscale, mask to binary.

Wait, but transforms in PyTorch are applied separately. So if there's a Resize, it should be applied to both image and mask. So the transforms should be a list that can handle both. For example:

transform = transforms.Compose([
transforms.Resize((img_size, img_size)),
transforms.ToTensor()
])

But for the mask, after resizing, it should remain binary. However, when using transforms.Resize for masks, using bilinear interpolation might create non-binary values. So to avoid that, after resizing, the mask should be thresholded again.

Ah, right! So during resizing of the mask, we need to use nearest neighbor interpolation to preserve the binary nature.

Therefore, in the Dataset, during transforms, if there's a Resize, apply it to image with bilinear and to mask with nearest. But how to do that in the transform pipeline?

This suggests that the transforms should be applied separately to image and mask. Therefore, the Dataset class might need a paired transform approach. However, since the user didn't ask for augmentations, perhaps the transforms are just Resize and ToTensor. To handle this correctly, perhaps split the transforms into those that can be applied to both, and those that are separate.

Alternatively, in the Dataset's __getitem__, apply Resize to image and mask with appropriate interpolation.

For example:

In __getitem__:

image = Image.open(image_path).convert('L') # grayscale
mask = Image.open(mask_path).convert('L') # assuming mask is grayscale, 0 and 255

if self.transform:
image = self.transform(image)
mask = self.transform(mask)

But then Resize would use bilinear for both, which is bad for masks. So need to apply Resize to image with bilinear and mask with nearest.

Hmm, this complicates things. Therefore, perhaps the transform should be applied after resizing both image and mask appropriately.

Alternative approach: in the Dataset's __getitem__, after loading image and mask as PIL Images, perform resizing manually.

But that would require passing the desired size to the Dataset.

So maybe the Dataset class has a parameter for target size, and inside __getitem__, it resizes both image and mask.

But that would hardcode the transforms into the Dataset. Alternatively, use a custom transform that applies resizing with different interpolation methods for image and mask.

Alternatively, use a paired transform function. But how to implement that.

Perhaps create a custom transform function that takes both image and mask as inputs. For example:

class ResizePair:
def __init__(self, size):
self.size = size
self.resize_image = transforms.Resize(size, interpolation=Image.BILINEAR)
self.resize_mask = transforms.Resize(size, interpolation=Image.NEAREST)

def __call__(self, image, mask):
return self.resize_image(image), self.resize_mask(mask)

Similarly, other transforms can be handled. Then in the Dataset's __init__, the transform would be applied to both image and mask.

So the Dataset class could have a transform for both image and mask. Then, when applying transforms, they are done in a way that operations like Resize use correct interpolation for each.

But how to structure this. The Dataset class's __getitem__ would apply the transform to both image and mask.

So the Dataset's code could look like:

class GrayscaleBinaryMaskDataset(Dataset):
def __init__(self, image_dir, mask_dir=None, mask_suffix='_seg', transform=None):
self.image_dir = image_dir
self.mask_dir = mask_dir if mask_dir is not None else image_dir
self.mask_suffix = mask_suffix
self.transform = transform
# Get all .png files in image_dir, ignoring others
self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]
# Filter mask files? Or assume that for each image, there's a corresponding mask

def __len__(self):
return len(self.image_files)

def __getitem__(self, idx):
image_name = self.image_files[idx]
image_path = os.path.join(self.image_dir, image_name)
# Generate mask name
base_name = os.path.splitext(image_name)[0]
mask_name = base_name + self.mask_suffix + '.png'
mask_path = os.path.join(self.mask_dir, mask_name)
# Load image and mask
image = Image.open(image_path).convert('L') # grayscale
mask = Image.open(mask_path).convert('L') # assuming mask is grayscale, 0 or 255
# Convert mask to binary (0 or 1)
mask = np.array(mask)
mask = (mask > 127.5).astype(np.uint8) # threshold
mask = Image.fromarray(mask)
# Apply transforms if any
if self.transform:
image, mask = self.transform(image, mask)
# Convert to tensor
image = transforms.ToTensor()(image)
# Mask to tensor, long type for loss
mask = transforms.ToTensor()(mask).squeeze(0) # remove channel dim, shape (H,W)
mask = mask.long()
return image, mask

Wait, but the transform here is supposed to handle both image and mask. However, the standard transforms in torchvision don't do that. So the transform passed to the Dataset should be a function that takes both image and mask. Therefore, we need to define a composed transform that does that.

For example, using the above ResizePair class. So in the main code, when creating transforms, it would be something like:

transform = transforms.Compose([
ResizePair((256, 256)),
# other paired transforms
])

But how to handle ToTensor? Because ToTensor converts a PIL Image to tensor. So after resizing, we can apply ToTensor separately.

Alternatively, separate transforms for image and mask. Maybe split the transforms into two parts: paired transforms (like Resize) and individual transforms (like ToTensor).

Alternatively, handle paired transforms in the Dataset's __getitem__:

In __init__, have separate transforms for image and mask, but that might complicate things.

Alternatively, in the transform passed to Dataset, it's a function that takes image and mask and returns transformed image and mask. So the user would define transforms as a list of such paired transforms.

But implementing that would require using something like Albumentations, but since the user wants to avoid additional libraries, perhaps better to handle it manually.

Alternatively, apply Resize with nearest for mask and bilinear for image. So inside the Dataset's __getitem__, after loading, resize both:

if self.transform is not None:
# assume transform is a ResizePair instance
image, mask = self.transform(image, mask)

But this approach requires defining custom transform classes.

Alternatively, handle resizing in the Dataset's __getitem__:

def __init__(self, ... , img_size=256):
...
self.img_size = img_size

...

def __getitem__(self, idx):
...
# resize
image = image.resize((self.img_size, self.img_size), Image.BILINEAR)
mask = mask.resize((self.img_size, self.img_size), Image.NEAREST)
...

Then, other transforms like ToTensor can be applied separately. Wait, but then the resizing is hardcoded. So maybe the Dataset can take a transform argument, which is a function applied to both image and mask. But how to compose multiple transforms.

This seems complicated. Maybe for simplicity, since the user says no augmentations, only resizing, we can handle resizing directly in the __getitem__.

But perhaps better to use transforms.Resize for image, and for mask, use Resize with nearest neighbor. So separate transforms:

from torchvision import transforms

class GrayscaleBinaryMaskDataset(Dataset):
def __init__(self, image_dir, mask_dir=None, mask_suffix='_seg', img_size=256):
self.image_dir = image_dir
self.mask_dir = mask_dir if mask_dir is not None else image_dir
self.mask_suffix = mask_suffix
self.img_size = img_size
self.image_transform = transforms.Compose([
transforms.Resize((img_size, img_size)),
transforms.ToTensor()
])
self.mask_transform = transforms.Compose([
transforms.Resize((img_size, img_size), interpolation=Image.NEAREST),
transforms.ToTensor()
])

def __getitem__(self, idx):
...
# after loading image and mask as PIL images
image = self.image_transform(image)
mask = self.mask_transform(mask)
# mask is tensor, but ToTensor scales to [0,1], so threshold again
mask = (mask > 0.5).float()
# but we need to have mask as 0 and 1, but also as long for loss
mask = mask.squeeze().long() # assuming mask is (1, H, W)
return image, mask

Wait, but when using transforms.ToTensor(), the mask (which is already binary 0 and 255) becomes [0,1] tensor. Then resizing with nearest neighbor preserves the binary nature. However, if the mask was resized with bilinear, it could have intermediate values, but with nearest, it remains binary. Then, ToTensor() would give 0. or 1. (if original was 0 and 255). But maybe better to threshold again after ToTensor.

Alternatively, after applying mask_transform, which is Resize and ToTensor:

mask = mask > 0.5 # converts to boolean tensor
mask = mask.long() # 0 and 1

This ensures that even if some values are between 0 and 1 due to resizing (unlikely if Resize with NEAREST), they are thresholded.

This approach may work.

Now, moving to the model. Implementing U-Net.

The U-Net architecture has an encoder with downsampling and double convolution blocks, then a decoder with upsampling and skip connections.

The code for the model would be:

class UNet(nn.Module):
def __init__(self, in_channels=1, out_channels=1):
super(UNet, self).__init__()
# Define encoder layers
# Each block is two conv layers
def CBR(in_channels, out_channels):
return nn.Sequential(
nn.Conv2d(in_channels, out_channels, 3, padding=1),
nn.BatchNorm2d(out_channels),
nn.ReLU(inplace=True),
nn.Conv2d(out_channels, out_channels, 3, padding=1),
nn.BatchNorm2d(out_channels),
nn.ReLU(inplace=True)
)

self.enc1 = CBR(in_channels, 64)
self.enc2 = CBR(64, 128)
self.enc3 = CBR(128, 256)
self.enc4 = CBR(256, 512)
self.bottleneck = CBR(512, 1024)

self.pool = nn.MaxPool2d(2)

# Decoder
self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
self.dec4 = CBR(1024, 512)
self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
self.dec3 = CBR(512, 256)
self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
self.dec2 = CBR(256, 128)
self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
self.dec1 = CBR(128, 64)

self.final_conv = nn.Conv2d(64, out_channels, 1)

def forward(self, x):
# Encoder
conv1 = self.enc1(x)
x = self.pool(conv1)
conv2 = self.enc2(x)
x = self.pool(conv2)
conv3 = self.enc3(x)
x = self.pool(conv3)
conv4 = self.enc4(x)
x = self.pool(conv4)

x = self.bottleneck(x)

# Decoder
x = self.upconv4(x)
x = torch.cat([x, conv4], dim=1)
x = self.dec4(x)

x = self.upconv3(x)
x = torch.cat([x, conv3], dim=1)
x = self.dec3(x)

x = self.upconv2(x)
x = torch.cat([x, conv2], dim=1)
x = self.dec2(x)

x = self.upconv1(x)
x = torch.cat([x, conv1], dim=1)
x = self.dec1(x)

return self.final_conv(x)

Wait, but for binary segmentation, the output should be logits (single channel), and then apply sigmoid. But the loss function BCEWithLogitsLoss combines sigmoid and BCE loss, so the model outputs the logits. So the final layer outputs (batch, 1, H, W). The model's forward returns that.

Now, for training. The loss function would be BCEWithLogitsLoss. Also, considering class imbalance (small foreground), maybe weight the loss. But user didn't specify, so proceed with default.

Dice coefficient calculation:

def dice_score(pred, target, smooth=1e-6):
pred = torch.sigmoid(pred) > 0.5 # convert to binary
intersection = (pred * target).sum(dim=(1, 2))
union = pred.sum(dim=(1, 2)) + target.sum(dim=(1, 2))
dice = (2. * intersection + smooth) / (union + smooth)
return dice.mean()

Wait, but during validation/testing, the predictions are thresholded. However, during training, we don't threshold because loss uses logits. For Dice score calculation, yes, threshold.

But during validation, the model outputs logits, so we need to apply sigmoid and threshold.

But also, the target should be flattened or not? The Dice function should compute per batch.

Another approach:

def dice_coefficient(y_pred, y_true, smooth=1e-6):
# y_pred is logits, shape (B, 1, H, W)
# convert to probabilities
y_pred = torch.sigmoid(y_pred)
y_pred = y_pred.view(y_pred.size(0), -1)
y_true = y_true.view(y_true.size(0), -1)
intersection = (y_pred * y_true).sum(dim=1)
union = y_pred.sum(dim=1) + y_true.sum(dim=1)
dice = (2. * intersection + smooth) / (union + smooth)
return dice.mean()

But for Dice score during validation, per batch, we can compute this.

Now, the training loop:

In each epoch, iterate over the DataLoader. For each batch, compute output, loss, backprop. Track losses.

Validation loop: same but no backprop, compute Dice scores for each batch.

Testing loop: same as validation.

Saving losses and Dice scores:

After each epoch, append the average loss to the train_losses list, and same for val. Then, after training, save to Excel.

For Dice scores in validation and testing: collect a list of lists. Each epoch's list of batch Dice scores.

Now, implementing saving to Excel:

Using pandas:

import pandas as pd

train_losses = [] # collected after each epoch
val_losses = []
val_dice_scores = [] # list of lists
test_dice_scores = []

At the end of training:

df_train = pd.DataFrame([range(1, epochs+1), train_losses])
df_train.to_excel('train_losses.xlsx', index=False)

Similarly for val_losses. But wait, the first row should be epoch numbers, second the loss. So the DataFrame should have two rows. However, pandas DataFrames have rows as observations. So transposing:

df_train = pd.DataFrame({'Epoch': range(1, epochs+1), 'Loss': train_losses})
df_train.to_excel('train_losses.xlsx', index=False) # but this saves as two columns. Not matching user's instruction.

Wait, user says first row is epoch numbers, second row is losses. So each Excel file has two rows. So for example:

Row 0: 1, 2, 3, ..., epoch count
Row 1: loss1, loss2, ..., lossN

So the data is structured as two rows. So the DataFrame should have two rows and columns for each epoch.

So:

train_epochs = list(range(1, epochs+1))
train_losses_list = [...] # collected during training
df_train = pd.DataFrame([train_epochs, train_losses_list])
df_train.to_excel('train_losses.xlsx', index=False, header=False)

Similarly for val.

But then the Excel file's first row is epochs, second losses. This matches the user's requirement.

For Dice scores during validation: each epoch's batch Dice scores are a row. So for each epoch, collect a list of Dice scores (each batch's average Dice), then append that list to val_dice_scores. Then, when saving to Excel, each row is an epoch's list of batch Dice scores. Using pandas:

df_val_dice = pd.DataFrame(val_dice_scores)
df_val_dice.to_excel('validation_dice_scores.xlsx', index=False)

Same for test.

Now, the training procedure:

Implement training and validation loops with tqdm. For each batch in DataLoader:

with tqdm(...) as pbar:
for batch in dataloader:
# process

In the train.py functions:

def train_one_epoch(model, dataloader, criterion, optimizer, device):
model.train()
running_loss = 0.0
for images, masks in tqdm(dataloader, desc='Training'):
images, masks = images.to(device), masks.to(device)
optimizer.zero_grad()
outputs = model(images)
loss = criterion(outputs, masks.float())
loss.backward()
optimizer.step()
running_loss += loss.item() * images.size(0)
epoch_loss = running_loss / len(dataloader.dataset)
return epoch_loss

Similarly for validation:

def validate(model, dataloader, criterion, device):
model.eval()
running_loss = 0.0
dice_scores = []
with torch.no_grad():
for images, masks in tqdm(dataloader, desc='Validating'):
images, masks = images.to(device), masks.to(device)
outputs = model(images)
loss = criterion(outputs, masks.float())
running_loss += loss.item() * images.size(0)
# Compute Dice
dice = dice_coefficient(outputs, masks)
dice_scores.append(dice.item())
epoch_loss = running_loss / len(dataloader.dataset)
mean_dice = np.mean(dice_scores)
return epoch_loss, mean_dice, dice_scores

Wait, but the user wants to store each batch's Dice score. So in validate function, return the list of batch Dice scores.

But the function above returns the epoch loss, mean_dice (average over all batches?), and the dice_scores list. But according to user instruction, during validation, collect Dice per batch per epoch, and store in Excel. So each epoch's list of batch Dice scores.

Similarly for testing.

Testing function:

def test(model, dataloader, device):
model.eval()
dice_scores = []
all_preds = []
all_masks = []
all_filenames = []
with torch.no_grad():
for images, masks, filenames in tqdm(dataloader, desc='Testing'):
# Wait, how to get filenames? The Dataset doesn't return them. Need to modify Dataset to return filename.
Ah, here's a problem. For the visualization of test predictions, we need to know the filenames. But currently, the Dataset's __getitem__ returns image and mask only. So we need to modify the Dataset to also return the image filename.

Modify dataset.py's __getitem__:

return image, mask, image_name (without path)

So in Dataset's __getitem__:

def __getitem__(self, idx):
...
return image, mask, image_name

But mask is processed as before. Then, in the DataLoader, each batch will have images, masks, filenames.

But this requires modifying the Dataset class. So adding returning the image name.

Thus, in dataset.py:

class GrayscaleBinaryMaskDataset(Dataset):
...
def __getitem__(self, idx):
...
return image, mask, image_name

But then, mask processing needs to be adjusted. Previously, mask was transformed to tensor and long. Also, in the Dataset's __getitem__, after transforming:

mask = (mask > 0.5).float().squeeze().long()

Wait, no, earlier code steps:

In the __getitem__:

mask is loaded as PIL Image, converted to array thresholded, then back to PIL, then transformed with mask_transform (Resize and ToTensor), which gives mask as tensor of shape (1, H, W), values 0 or 1. Then:

mask = mask.squeeze().long() # shape (H, W)

But BCEWithLogitsLoss expects masks to be (B, H, W) for input (B, 1, H, W) outputs. Wait, the loss function:

output shape: (batch, 1, H, W)
masks shape: (batch, H, W) as long or float?

The BCEWithLogitsLoss expects targets to be the same shape as output. Wait, no:

BCEWithLogitsLoss: the targets should be the same shape as the input (logits). So output is (B, 1, H, W), masks should be (B, 1, H, W). But masks are binary.

Ah, this is a mistake. So in the Dataset, the mask should be a tensor of shape (1, H, W), with values 0 and 1.

So adjusting the Dataset's __getitem__:

mask = self.mask_transform(mask) # shape (1, H, W), float 0. or 1.
mask = (mask > 0.5).float() # ensure binary
mask = mask.long() # but BCEWithLogitsLoss expects float targets. Wait, no:

Wait, BCEWithLogitsLoss combines sigmoid and BCE loss. The targets should be the same shape as the output, with values 0. or 1. (float). So the mask should be float.

So in Dataset:

mask = self.mask_transform(mask)
mask = mask.float() # convert to float (already 0. or 1.)

So the mask tensor has shape (1, H, W), float.

Therefore, in the __getitem__:

return image, mask, image_name

Where image is (1, H, W), float.

Now, back to testing. The test function needs to collect predictions, ground truths, and filenames to visualize later.

So modifying the test function:

def test(model, dataloader, device, save_dir):
model.eval()
dice_scores = []
all_preds = []
all_masks = []
all_filenames = []
with torch.no_grad():
for images, masks, filenames in tqdm(dataloader, desc='Testing'):
images, masks = images.to(device), masks.to(device)
outputs = model(images)
# Compute Dice for each batch
batch_dice = dice_coefficient(outputs, masks)
dice_scores.extend(batch_dice.tolist())
# Save predictions and masks for visualization
preds = torch.sigmoid(outputs) > 0.5
all_preds.append(preds.cpu())
all_masks.append(masks.cpu())
all_filenames.extend(filenames)
# Concatenate all batches
all_preds = torch.cat(all_preds)
all_masks = torch.cat(all_masks)
# Visualize random 5 samples
visualize_predictions(all_preds, all_masks, all_filenames, save_dir)
# Save Dice scores to Excel
return dice_scores

But need to implement visualize_predictions, which creates a plot with 5 random samples.

Implementing visualize_predictions:

def visualize_predictions(preds, masks, filenames, save_dir):
indices = np.random.choice(len(preds), size=5, replace=False)
fig, axes = plt.subplots(5, 3, figsize=(15, 25))
for i, idx in enumerate(indices):
pred = preds[idx].squeeze().numpy()
mask = masks[idx].squeeze().numpy()
image_path = filenames[idx]
# Assuming images are in a directory, but during testing, how to get the actual image?
# Wait, no. The test function only has preds, masks, and filenames. But to visualize the input image, need to load it again. Which is not feasible here. Alternatively, during testing, also collect the images.

This suggests that the test function needs to collect the original images as well. But that would require storing all images in memory, which is not feasible for large datasets. Alternatively, during testing, for visualization, save the sample indices and then reload the images.

But this complicates things. Perhaps better to modify the test function to collect the images. However, this could consume memory. Alternatively, during testing, for each batch, store the images along with preds and masks.

Modify the test function:

def test(...):
...
all_images = []
...
for images, masks, filenames in ...:
...
all_images.append(images.cpu())
...
all_images = torch.cat(all_images)
visualize_predictions(all_images, all_masks, all_preds, all_filenames, indices, save_dir)

But this requires storing all test images in memory. If the test set is large, this might not be feasible. However, given the dataset size of 5000 samples and assuming resized to 256x256, it's manageable.

Alternatively, during visualization, select 5 random indices and load those images from disk. But how to get the image data without reloading? Since the visualization is done after testing, perhaps during testing, store the indices of the samples to visualize and then reload them.

But this adds complexity. Perhaps the simplest way is to collect images, masks, preds, and filenames during testing. So:

In test function:

all_images = []
all_masks = []
all_preds = []
all_filenames = []

Within the loop:

all_images.append(images.cpu())
all_masks.append(masks.cpu())
all_preds.append(preds.cpu())
all_filenames.extend(filenames)

After collecting all:

all_images = torch.cat(all_images)
all_masks = torch.cat(all_masks)
all_preds = torch.cat(all_preds)

Then, in visualize_predictions:

def visualize_predictions(images, masks, preds, filenames, save_dir):
indices = np.random.choice(len(images), size=5, replace=False)
fig, axes = plt.subplots(5, 3, figsize=(15, 25))
for row, idx in enumerate(indices):
image = images[idx].squeeze().numpy()
mask = masks[idx].squeeze().numpy()
pred = preds[idx].squeeze().numpy()
filename = filenames[idx]
# Plot each in subplot
axes[row, 0].imshow(image, cmap='gray')
axes[row, 0].set_title(f'Input: {filename}')
axes[row, 0].axis('off')
axes[row, 1].imshow(mask, cmap='gray')
axes[row, 1].set_title('Ground Truth')
axes[row, 1].axis('off')
axes[row, 2].imshow(pred, cmap='gray')
axes[row, 2].set_title('Prediction')
axes[row, 2].axis('off')
plt.tight_layout()
plt.savefig(os.path.join(save_dir, 'test_predictions.png'))
plt.close()

This way, the images are collected during testing and used for visualization.

Now, the main.py:

Handle data splitting using train_test_split. The dataset is loaded, then indices are split into train, val, test.

from sklearn.model_selection import train_test_split

dataset = GrayscaleBinaryMaskDataset(...)
indices = list(range(len(dataset)))
train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)
train_idx, val_idx = train_test_split(train_idx, test_size=0.1/0.8, random_state=42) # 0.1/0.8 = 12.5% of 80% is 10%

Wait, original split: 80% train, 10% val, 10% test. So total data is 100%. So first split into train+val and test (80%+10% vs 10% â†’ test_size=0.1). No, user said split into 80% train, 10% val, 10% test. So first split into 80% train and 20% remaining. Then split remaining into 50% val and 50% test.

train_size=0.8, test_size=0.2. Then split test into two equal parts.

train_idx, temp_idx = train_test_split(indices, test_size=0.2, random_state=42)
val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)

Yes. So:

train_dataset = Subset(dataset, train_idx)
val_dataset = Subset(dataset, val_idx)
test_dataset = Subset(dataset, test_idx)

DataLoaders are created with batch_size and shuffle.

Now, model summary using torchinfo:

from torchinfo import summary

model = UNet().to(device)
summary(model, input_size=(1, 1, 256, 256)) # batch_size, channels, H, W

Print the total number of parameters.

Putting all together in main.py:

if __name__ == '__main__':
# Parameters
image_dir = 'path/to/images'
mask_dir = None # same as image_dir
mask_suffix = '_seg'
img_size = 256
batch_size = 16
epochs = 20
lr = 0.001
save_path = 'results/'
os.makedirs(save_path, exist_ok=True)

# Dataset and DataLoader
dataset = GrayscaleBinaryMaskDataset(image_dir, mask_dir, mask_suffix, img_size)
indices = list(range(len(dataset)))
train_idx, temp_idx = train_test_split(indices, test_size=0.2, random_state=42)
val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)

train_loader = DataLoader(Subset(dataset, train_idx), batch_size=batch_size, shuffle=True)
val_loader = DataLoader(Subset(dataset, val_idx), batch_size=batch_size)
test_loader = DataLoader(Subset(dataset, test_idx), batch_size=batch_size)

# Model, loss, optimizer
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = UNet().to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# Training loop
train_losses = []
val_losses = []
val_dice_scores = []
start_time = time.time()

for epoch in range(epochs):
print(f'Epoch {epoch+1}/{epochs}')
train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)
val_loss, val_dice, dice_list = validate(model, val_loader, criterion, device)

train_losses.append(train_loss)
val_losses.append(val_loss)
val_dice_scores.append(dice_list)

# Save losses to Excel
pd.DataFrame([range(1, epoch+2), train_losses]).to_excel(os.path.join(save_path, 'train_losses.xlsx'), index=False, header=False)
pd.DataFrame([range(1, epoch+2), val_losses]).to_excel(os.path.join(save_path, 'val_losses.xlsx'), index=False, header=False)

# Save Dice scores for validation
pd.DataFrame(val_dice_scores).to_excel(os.path.join(save_path, 'validation_dice_scores.xlsx'), index=False)

total_time = time.time() - start_time
print(f'Training completed in {total_time:.2f} seconds')

# Save model
torch.save(model, os.path.join(save_path, 'unet_model.pth'))
torch.save(model.state_dict(), os.path.join(save_path, 'unet_model_state.pth'))

# Plot losses
plot_losses(train_losses, val_losses, save_path)

# Testing
test_dice = test(model, test_loader, device, save_path)
pd.DataFrame([test_dice]).to_excel(os.path.join(save_path, 'test_dice_scores.xlsx'), index=False)

# Model summary
from torchinfo import summary
summary(model, input_size=(1, 1, img_size, img_size))

But there are many missing functions here. Need to implement plot_losses, and the validate and test functions to return batch Dice scores.

Now, putting all these pieces into the respective files.

Potential errors and checks:

- The Dataset returns image, mask, filename. The mask must be a tensor of shape (1, H, W), float 0/1.

- BCEWithLogitsLoss expects the mask to be float.

- Device management (moving to GPU).

- Properly flattening and concatenating tensors during visualization.

- Handling the Excel files correctly, especially for Dice scores which are lists of lists.

- The validation function should return the epoch loss, mean Dice, and list of batch Dice scores.

Modifying validate function:

def validate(model, dataloader, criterion, device):
model.eval()
running_loss = 0.0
batch_dice_scores = []
with torch.no_grad():
for images, masks, _ in tqdm(dataloader, desc='Validating'):
images, masks = images.to(device), masks.to(device)
outputs = model(images)
loss = criterion(outputs, masks)
running_loss += loss.item() * images.size(0)
# Compute Dice per batch
dice = dice_coefficient(outputs, masks)
batch_dice_scores.extend(dice.tolist())
epoch_loss = running_loss / len(dataloader.dataset)
return epoch_loss, batch_dice_scores

Similarly, the dice_coefficient function:

def dice_coefficient(y_pred, y_true, smooth=1e-6):
y_pred = torch.sigmoid(y_pred)
y_pred_binary = (y_pred > 0.5).float()
intersection = (y_pred_binary * y_true).sum(dim=(1, 2, 3)) # (batch, 1, H, W)
union = y_pred_binary.sum(dim=(1, 2, 3)) + y_true.sum(dim=(1, 2, 3))
dice = (2. * intersection + smooth) / (union + smooth)
return dice

This assumes y_true has the same shape as y_pred (batch, 1, H, W).

Now, in the validate function, batch_dice_scores collects all batch Dice scores as a list. For each epoch, append this list to val_dice_scores.

In the main loop:

val_loss, batch_dice = validate(...)
val_losses.append(val_loss)
val_dice_scores.append(batch_dice)

Then, after each epoch, save the validation_dice_scores.xlsx as a DataFrame where each row is an epoch's batch Dice scores.

Similarly for testing.

Finally, the visualization function.

Now, considering all of this, the final code would be structured as follows.

But given the complexity, the code may have bugs that need to be ironed out, but the overall structure should be as outlined.
Below is a modular and comprehensive implementation of a U-Net model for binary segmentation of grayscale images using PyTorch, structured into four Python files as requested.

ðŸ“ dataset.py â€“ Custom Dataset Class

import os
import torch
from torch.utils.data import Dataset, Subset
from torchvision import transforms
from PIL import Image
import numpy as np
from sklearn.model_selection import train_test_split

class GrayscaleBinaryMaskDataset(Dataset):
    def __init__(self, image_dir, mask_dir=None, mask_suffix='_seg', img_size=256):
        self.image_dir = image_dir
        self.mask_dir = mask_dir if mask_dir else image_dir
        self.mask_suffix = mask_suffix
        self.img_size = img_size
        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]

        # Transformations
        self.image_transform = transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
        ])
        self.mask_transform = transforms.Compose([
            transforms.Resize((img_size, img_size), interpolation=Image.NEAREST),
            transforms.ToTensor(),
        ])

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        image_name = self.image_files[idx]
        base_name = os.path.splitext(image_name)[0]
        mask_name = f"{base_name}{self.mask_suffix}.png"

        image_path = os.path.join(self.image_dir, image_name)
        mask_path = os.path.join(self.mask_dir, mask_name)

        # Load images
        image = Image.open(image_path).convert('L')
        mask = Image.open(mask_path).convert('L')

        # Apply transformations
        image = self.image_transform(image)
        mask = self.mask_transform(mask)

        # Threshold mask to binary (0 or 1)
        mask = (mask > 0.5).float()

        return image, mask, image_name

profile
Qwen3-235B-A22B
1:54 am
Thinking completed
Â·
38,912 tokens budget
Okay, let's tackle this problem. The user wants a complete U-Net implementation in PyTorch for binary segmentation of grayscale images. The code needs to be split into several modules and follow specific guidelines. Let me start by breaking down each part.

First, the custom Dataset class. The dataset has grayscale images and binary masks. The masks are either in the same folder with a suffix like "_seg.png" or in separate folders. The Dataset needs to load image-mask pairs correctly. I need to make sure that only .png files are considered, ignoring other files. Also, transforms should be applied to both image and mask. Since the images are grayscale, I should read them as single-channel. The masks are binary, so converting them to 0s and 1s is necessary.

Next, the U-Net model. Binary segmentation with small foreground. The standard U-Net has an encoder-decoder structure with skip connections. For binary output, the final layer should have a single channel with a sigmoid activation. Since the foreground is small, maybe using a dice loss or BCEWithLogitsLoss with class weights. But the user didn't specify loss, so perhaps default to BCEWithLogitsLoss. Also, the model should be efficient but not too big, considering around 5000 samples.

Training, validation, testing functions. Need to handle the loops for each phase. For training, optimizer, loss function, backprop. For validation and testing, calculate Dice score. The Dice score is calculated per batch and stored in Excel. Also, saving losses and Dice scores each epoch. Using tqdm to show progress bars.

Data loading: Split into 80% train, 10% val, 10% test using train_test_split. Since the data isn't copied, the indices are split. Using Subset to create datasets from the original dataset. DataLoader setup with appropriate batch sizes. Transforms: resize, convert to tensor, normalize for images (maybe to [0,1], but since grayscale, normalization might be done via ToTensor). For masks, ensure they are binary.

Hyperparameters: Learning rate, batch size, epochs. Need to choose reasonable defaults. Maybe batch size 32, learning rate 0.001. Adam optimizer. Early stopping or checkpointing could help prevent overfitting, but user didn't mention, so maybe not. Training time tracking.

Model summary using torchinfo. Need to install torchinfo. The model input should be (1, H, W), assuming images are resized to a fixed size. User didn't specify image size, so maybe default to 256x256. Or let the user choose, but code should handle resizing. So in transforms, Resize to a fixed size.

Excel files: Using pandas to write DataFrames to Excel. Each epoch's loss values are appended as rows. For Dice scores, each batch's score per epoch is saved as columns. So for validation_dice_scores.xlsx, each row is an epoch, each column a batch. Similarly for test.

Visualization functions: After training, plot training and validation losses on the same graph. Save as png. For testing, visualize 5 random samples with input, ground truth, prediction. Each subplot row has 3 images with titles. Also display filenames as titles. Saving that plot.

Main script: Putting everything together. Loading dataset, splitting, creating DataLoaders, initializing model, training, validation, testing, saving everything. Also printing model summary, parameters, data sizes, etc.

Potential issues: Ensuring that transforms are applied correctly to both image and mask. For example, random cropping or flipping would require the same transform for both. But user said no augmentations, so just Resize and ToTensor. Also, handling different mask naming conventions. The Dataset class should accept a mask suffix parameter.

Testing the code structure. Each module (dataset.py, model.py, train.py, main.py) should have their respective parts. The main.py runs the training loop when executed.

Let me outline each part step by step.

For dataset.py: Create a GrayscaleBinaryMaskDataset class. The __init__ takes image_dir, mask_dir (could be same as image_dir), mask_suffix. The __getitem__ loads image and corresponding mask. Apply transforms if any. The transforms should be composed of functions that handle both image and mask, like Resize, ToTensor. Wait, but PyTorch transforms like ToTensor() automatically handle PIL Images. So for transforms that affect both, like Resize, they can be applied to both image and mask. But for transforms that need to be same for both (like random flip), need to apply same random seed. But since no augmentations, just Resize and ToTensor.

Wait, the user says "Ensure needed transforms on input image and mask pairs (Re-size images correctly, ensure gray scale and binaryâ€¦)". So Resize to a fixed size, ToTensor for images (converts to tensor and scales to [0,1], since it's a PIL Image). For masks, ToTensor converts to tensor, but since they are binary, it's okay. But maybe after loading, the mask is converted to a binary mask (0 and 1). So in the Dataset, after loading the mask image, convert it to binary. Because sometimes masks might be saved as 0 and 255. So in the Dataset, after loading the mask, do mask = mask.convert('L') and then threshold to 0 and 1. Or divide by 255 to get 0 or 1. Because if the mask is saved as 0 and 255, dividing by 255 would make it 0 and 1. So in the Dataset's __getitem__, after loading mask as a grayscale image, convert to numpy array, threshold at 0.5, then convert to tensor. Alternatively, use transforms.Lambda to do that. But how?

Alternatively, in the Dataset's __getitem__, after loading the mask image, process it:

mask = Image.open(mask_path).convert('L')
mask = np.array(mask)
mask = (mask > 127.5).astype(np.uint8) # assuming 0 and 255, threshold to 0 or 1
mask = torch.from_numpy(mask).long()

But ToTensor would convert the image to float32, but for mask, we need to keep it as long. So perhaps in the Dataset, the image is converted to tensor and normalized (maybe not necessary if using ToTensor which scales to [0,1]), and the mask is converted to a binary tensor.

So the Dataset class should handle that.

For the model.py: Implement U-Net. The standard architecture has encoder blocks with double convolutions, downsampling, decoder blocks with upsampling and skip connections. For binary segmentation, the output channel is 1. The model should be defined as a class with encoder and decoder parts. Also, for small foreground, maybe using Dice loss or BCEWithLogitsLoss. But the loss function is handled in the training script.

In train.py: Functions for train, validate, test. Each function would loop over the DataLoader, compute loss, track metrics. For training, backprop; for validation/testing, compute Dice. The Dice score is calculated per batch. For each batch, Dice is (2*intersection + smooth)/(union + smooth). Need to implement that. Also, during validation, collect all Dice scores per batch, and save to Excel. Similarly for testing.

For saving losses and Dice scores: Use pandas DataFrames. Each epoch's average loss is saved as a row in the Excel file. For Dice, each batch's score is a column, with epochs as rows.

In main.py: All the setup. Load dataset, split into train/val/test. Create DataLoaders. Initialize model, optimizer, loss function. Train the model, track time. Save model, state dict. Plot losses. Test the model, save Dice scores and visualize predictions.

Hyperparameters: Learning rate 0.001, batch size 16 or 32. Epochs maybe 20. Image size, say, 256x256. The transforms.Resize((256, 256)).

Now, putting all together.

Potential pitfalls:

- Handling the mask filenames. If the images are in the same directory, mask_suffix is needed. For example, if image is 'img.png', mask could be 'img_seg.png'. So in the Dataset class, mask name is image name + suffix.

- DataLoader workers: If using transforms with NumPy operations, need to ensure that the Dataset is picklable. But using standard transforms should be okay.

- Model summary with torchinfo: The input size needs to be specified. For example, (1, 256, 256).

- Dice calculation: Need to flatten the tensors and compute per batch. Also, smooth to avoid division by zero.

- Saving Excel files: Each time appending a new row. However, the current plan is to collect all losses during training and save at the end. Or after each epoch, append to the Excel. Using pandas, perhaps create a DataFrame each time and save with mode 'a' or 'w'. But better to collect all losses during training (as lists) and save at the end.

Wait, the user says:

For each epoch, track and save average training loss in train_losses.xlsx, same for val. So after each epoch, append the loss to the Excel. But how to do that? Maybe collect all the losses during training (as lists), and after training, save to Excel. But the user instruction says during training, save each epoch. So perhaps after each epoch, write the current epoch's loss to the Excel. Similarly for validation.

But writing to Excel during training could be slow. Maybe better to collect all losses in lists, then save at the end. But the user says "track and save the average training loss for each epoch in a separate Excel file...". So probably collect during training and save after each epoch. But maybe using pandas, create a DataFrame where each column is an epoch, and rows are epoch numbers and losses. Wait, the first row contains epoch numbers, second the loss. So for train_losses.xlsx, sheet has row 0: [1,2,3,...], row 1: [loss1, loss2, ...].

Wait, the user says: first row epoch numbers, second average loss. So each Excel file has two rows: epochs and losses. Similarly for validation. So for each new epoch, append the value to the list, and when saving, overwrite the Excel file with updated data. That way, if training stops, the saved file has up to the last completed epoch.

But how to handle this in code. Each time, create a new DataFrame with the current epochs and losses, and save it. So for example, after epoch 1, the Excel has epochs 1, loss. After epoch 2, epochs 1,2 and losses. So during training, for each epoch, append the average loss to a list, then at the end of the epoch, save the entire list into the Excel.

Same for validation losses.

For Dice scores during validation and testing: each epoch's batches' Dice scores are stored as columns. So for each epoch, collect a list of Dice scores per batch, then save them as a row in the Excel. So each row in 'validation_dice_scores.xlsx' corresponds to an epoch, each column a batch's Dice.

Implementing that requires collecting all batch Dice scores for each validation epoch, then saving them as a DataFrame with each row being the epoch's batch results.

Now, coding each part.

Starting with dataset.py:

Implement the Dataset class. The __init__ reads all image files in image_dir, filtering .png. Then, for each image, the mask path is determined based on mask_dir and mask_suffix. The __getitem__ loads image and mask, applies transforms (same for both), converts image to grayscale, mask to binary.

Wait, but transforms in PyTorch are applied separately. So if there's a Resize, it should be applied to both image and mask. So the transforms should be a list that can handle both. For example:

transform = transforms.Compose([
transforms.Resize((img_size, img_size)),
transforms.ToTensor()
])

But for the mask, after resizing, it should remain binary. However, when using transforms.Resize for masks, using bilinear interpolation might create non-binary values. So to avoid that, after resizing, the mask should be thresholded again.

Ah, right! So during resizing of the mask, we need to use nearest neighbor interpolation to preserve the binary nature.

Therefore, in the Dataset, during transforms, if there's a Resize, apply it to image with bilinear and to mask with nearest. But how to do that in the transform pipeline?

This suggests that the transforms should be applied separately to image and mask. Therefore, the Dataset class might need a paired transform approach. However, since the user didn't ask for augmentations, perhaps the transforms are just Resize and ToTensor. To handle this correctly, perhaps split the transforms into those that can be applied to both, and those that are separate.

Alternatively, in the Dataset's __getitem__, apply Resize to image and mask with appropriate interpolation.

For example:

In __getitem__:

image = Image.open(image_path).convert('L') # grayscale
mask = Image.open(mask_path).convert('L') # assuming mask is grayscale, 0 and 255

if self.transform:
image = self.transform(image)
mask = self.transform(mask)

But then Resize would use bilinear for both, which is bad for masks. So need to apply Resize to image with bilinear and mask with nearest.

Hmm, this complicates things. Therefore, perhaps the transform should be applied after resizing both image and mask appropriately.

Alternative approach: in the Dataset's __getitem__, after loading image and mask as PIL Images, perform resizing manually.

But that would require passing the desired size to the Dataset.

So maybe the Dataset class has a parameter for target size, and inside __getitem__, it resizes both image and mask.

But that would hardcode the transforms into the Dataset. Alternatively, use a custom transform that applies resizing with different interpolation methods for image and mask.

Alternatively, use a paired transform function. But how to implement that.

Perhaps create a custom transform function that takes both image and mask as inputs. For example:

class ResizePair:
def __init__(self, size):
self.size = size
self.resize_image = transforms.Resize(size, interpolation=Image.BILINEAR)
self.resize_mask = transforms.Resize(size, interpolation=Image.NEAREST)

def __call__(self, image, mask):
return self.resize_image(image), self.resize_mask(mask)

Similarly, other transforms can be handled. Then in the Dataset's __init__, the transform would be applied to both image and mask.

So the Dataset class could have a transform for both image and mask. Then, when applying transforms, they are done in a way that operations like Resize use correct interpolation for each.

But how to structure this. The Dataset class's __getitem__ would apply the transform to both image and mask.

So the Dataset's code could look like:

class GrayscaleBinaryMaskDataset(Dataset):
def __init__(self, image_dir, mask_dir=None, mask_suffix='_seg', transform=None):
self.image_dir = image_dir
self.mask_dir = mask_dir if mask_dir is not None else image_dir
self.mask_suffix = mask_suffix
self.transform = transform
# Get all .png files in image_dir, ignoring others
self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]
# Filter mask files? Or assume that for each image, there's a corresponding mask

def __len__(self):
return len(self.image_files)

def __getitem__(self, idx):
image_name = self.image_files[idx]
image_path = os.path.join(self.image_dir, image_name)
# Generate mask name
base_name = os.path.splitext(image_name)[0]
mask_name = base_name + self.mask_suffix + '.png'
mask_path = os.path.join(self.mask_dir, mask_name)
# Load image and mask
image = Image.open(image_path).convert('L') # grayscale
mask = Image.open(mask_path).convert('L') # assuming mask is grayscale, 0 or 255
# Convert mask to binary (0 or 1)
mask = np.array(mask)
mask = (mask > 127.5).astype(np.uint8) # threshold
mask = Image.fromarray(mask)
# Apply transforms if any
if self.transform:
image, mask = self.transform(image, mask)
# Convert to tensor
image = transforms.ToTensor()(image)
# Mask to tensor, long type for loss
mask = transforms.ToTensor()(mask).squeeze(0) # remove channel dim, shape (H,W)
mask = mask.long()
return image, mask

Wait, but the transform here is supposed to handle both image and mask. However, the standard transforms in torchvision don't do that. So the transform passed to the Dataset should be a function that takes both image and mask. Therefore, we need to define a composed transform that does that.

For example, using the above ResizePair class. So in the main code, when creating transforms, it would be something like:

transform = transforms.Compose([
ResizePair((256, 256)),
# other paired transforms
])

But how to handle ToTensor? Because ToTensor converts a PIL Image to tensor. So after resizing, we can apply ToTensor separately.

Alternatively, separate transforms for image and mask. Maybe split the transforms into two parts: paired transforms (like Resize) and individual transforms (like ToTensor).

Alternatively, handle paired transforms in the Dataset's __getitem__:

In __init__, have separate transforms for image and mask, but that might complicate things.

Alternatively, in the transform passed to Dataset, it's a function that takes image and mask and returns transformed image and mask. So the user would define transforms as a list of such paired transforms.

But implementing that would require using something like Albumentations, but since the user wants to avoid additional libraries, perhaps better to handle it manually.

Alternatively, apply Resize with nearest for mask and bilinear for image. So inside the Dataset's __getitem__, after loading, resize both:

if self.transform is not None:
# assume transform is a ResizePair instance
image, mask = self.transform(image, mask)

But this approach requires defining custom transform classes.

Alternatively, handle resizing in the Dataset's __getitem__:

def __init__(self, ... , img_size=256):
...
self.img_size = img_size

...

def __getitem__(self, idx):
...
# resize
image = image.resize((self.img_size, self.img_size), Image.BILINEAR)
mask = mask.resize((self.img_size, self.img_size), Image.NEAREST)
...

Then, other transforms like ToTensor can be applied separately. Wait, but then the resizing is hardcoded. So maybe the Dataset can take a transform argument, which is a function applied to both image and mask. But how to compose multiple transforms.

This seems complicated. Maybe for simplicity, since the user says no augmentations, only resizing, we can handle resizing directly in the __getitem__.

But perhaps better to use transforms.Resize for image, and for mask, use Resize with nearest neighbor. So separate transforms:

from torchvision import transforms

class GrayscaleBinaryMaskDataset(Dataset):
def __init__(self, image_dir, mask_dir=None, mask_suffix='_seg', img_size=256):
self.image_dir = image_dir
self.mask_dir = mask_dir if mask_dir is not None else image_dir
self.mask_suffix = mask_suffix
self.img_size = img_size
self.image_transform = transforms.Compose([
transforms.Resize((img_size, img_size)),
transforms.ToTensor()
])
self.mask_transform = transforms.Compose([
transforms.Resize((img_size, img_size), interpolation=Image.NEAREST),
transforms.ToTensor()
])

def __getitem__(self, idx):
...
# after loading image and mask as PIL images
image = self.image_transform(image)
mask = self.mask_transform(mask)
# mask is tensor, but ToTensor scales to [0,1], so threshold again
mask = (mask > 0.5).float()
# but we need to have mask as 0 and 1, but also as long for loss
mask = mask.squeeze().long() # assuming mask is (1, H, W)
return image, mask

Wait, but when using transforms.ToTensor(), the mask (which is already binary 0 and 255) becomes [0,1] tensor. Then resizing with nearest neighbor preserves the binary nature. However, if the mask was resized with bilinear, it could have intermediate values, but with nearest, it remains binary. Then, ToTensor() would give 0. or 1. (if original was 0 and 255). But maybe better to threshold again after ToTensor.

Alternatively, after applying mask_transform, which is Resize and ToTensor:

mask = mask > 0.5 # converts to boolean tensor
mask = mask.long() # 0 and 1

This ensures that even if some values are between 0 and 1 due to resizing (unlikely if Resize with NEAREST), they are thresholded.

This approach may work.

Now, moving to the model. Implementing U-Net.

The U-Net architecture has an encoder with downsampling and double convolution blocks, then a decoder with upsampling and skip connections.

The code for the model would be:

class UNet(nn.Module):
def __init__(self, in_channels=1, out_channels=1):
super(UNet, self).__init__()
# Define encoder layers
# Each block is two conv layers
def CBR(in_channels, out_channels):
return nn.Sequential(
nn.Conv2d(in_channels, out_channels, 3, padding=1),
nn.BatchNorm2d(out_channels),
nn.ReLU(inplace=True),
nn.Conv2d(out_channels, out_channels, 3, padding=1),
nn.BatchNorm2d(out_channels),
nn.ReLU(inplace=True)
)

self.enc1 = CBR(in_channels, 64)
self.enc2 = CBR(64, 128)
self.enc3 = CBR(128, 256)
self.enc4 = CBR(256, 512)
self.bottleneck = CBR(512, 1024)

self.pool = nn.MaxPool2d(2)

# Decoder
self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
self.dec4 = CBR(1024, 512)
self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
self.dec3 = CBR(512, 256)
self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
self.dec2 = CBR(256, 128)
self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
self.dec1 = CBR(128, 64)

self.final_conv = nn.Conv2d(64, out_channels, 1)

def forward(self, x):
# Encoder
conv1 = self.enc1(x)
x = self.pool(conv1)
conv2 = self.enc2(x)
x = self.pool(conv2)
conv3 = self.enc3(x)
x = self.pool(conv3)
conv4 = self.enc4(x)
x = self.pool(conv4)

x = self.bottleneck(x)

# Decoder
x = self.upconv4(x)
x = torch.cat([x, conv4], dim=1)
x = self.dec4(x)

x = self.upconv3(x)
x = torch.cat([x, conv3], dim=1)
x = self.dec3(x)

x = self.upconv2(x)
x = torch.cat([x, conv2], dim=1)
x = self.dec2(x)

x = self.upconv1(x)
x = torch.cat([x, conv1], dim=1)
x = self.dec1(x)

return self.final_conv(x)

Wait, but for binary segmentation, the output should be logits (single channel), and then apply sigmoid. But the loss function BCEWithLogitsLoss combines sigmoid and BCE loss, so the model outputs the logits. So the final layer outputs (batch, 1, H, W). The model's forward returns that.

Now, for training. The loss function would be BCEWithLogitsLoss. Also, considering class imbalance (small foreground), maybe weight the loss. But user didn't specify, so proceed with default.

Dice coefficient calculation:

def dice_score(pred, target, smooth=1e-6):
pred = torch.sigmoid(pred) > 0.5 # convert to binary
intersection = (pred * target).sum(dim=(1, 2))
union = pred.sum(dim=(1, 2)) + target.sum(dim=(1, 2))
dice = (2. * intersection + smooth) / (union + smooth)
return dice.mean()

Wait, but during validation/testing, the predictions are thresholded. However, during training, we don't threshold because loss uses logits. For Dice score calculation, yes, threshold.

But during validation, the model outputs logits, so we need to apply sigmoid and threshold.

But also, the target should be flattened or not? The Dice function should compute per batch.

Another approach:

def dice_coefficient(y_pred, y_true, smooth=1e-6):
# y_pred is logits, shape (B, 1, H, W)
# convert to probabilities
y_pred = torch.sigmoid(y_pred)
y_pred = y_pred.view(y_pred.size(0), -1)
y_true = y_true.view(y_true.size(0), -1)
intersection = (y_pred * y_true).sum(dim=1)
union = y_pred.sum(dim=1) + y_true.sum(dim=1)
dice = (2. * intersection + smooth) / (union + smooth)
return dice.mean()

But for Dice score during validation, per batch, we can compute this.

Now, the training loop:

In each epoch, iterate over the DataLoader. For each batch, compute output, loss, backprop. Track losses.

Validation loop: same but no backprop, compute Dice scores for each batch.

Testing loop: same as validation.

Saving losses and Dice scores:

After each epoch, append the average loss to the train_losses list, and same for val. Then, after training, save to Excel.

For Dice scores in validation and testing: collect a list of lists. Each epoch's list of batch Dice scores.

Now, implementing saving to Excel:

Using pandas:

import pandas as pd

train_losses = [] # collected after each epoch
val_losses = []
val_dice_scores = [] # list of lists
test_dice_scores = []

At the end of training:

df_train = pd.DataFrame([range(1, epochs+1), train_losses])
df_train.to_excel('train_losses.xlsx', index=False)

Similarly for val_losses. But wait, the first row should be epoch numbers, second the loss. So the DataFrame should have two rows. However, pandas DataFrames have rows as observations. So transposing:

df_train = pd.DataFrame({'Epoch': range(1, epochs+1), 'Loss': train_losses})
df_train.to_excel('train_losses.xlsx', index=False) # but this saves as two columns. Not matching user's instruction.

Wait, user says first row is epoch numbers, second row is losses. So each Excel file has two rows. So for example:

Row 0: 1, 2, 3, ..., epoch count
Row 1: loss1, loss2, ..., lossN

So the data is structured as two rows. So the DataFrame should have two rows and columns for each epoch.

So:

train_epochs = list(range(1, epochs+1))
train_losses_list = [...] # collected during training
df_train = pd.DataFrame([train_epochs, train_losses_list])
df_train.to_excel('train_losses.xlsx', index=False, header=False)

Similarly for val.

But then the Excel file's first row is epochs, second losses. This matches the user's requirement.

For Dice scores during validation: each epoch's batch Dice scores are a row. So for each epoch, collect a list of Dice scores (each batch's average Dice), then append that list to val_dice_scores. Then, when saving to Excel, each row is an epoch's list of batch Dice scores. Using pandas:

df_val_dice = pd.DataFrame(val_dice_scores)
df_val_dice.to_excel('validation_dice_scores.xlsx', index=False)

Same for test.

Now, the training procedure:

Implement training and validation loops with tqdm. For each batch in DataLoader:

with tqdm(...) as pbar:
for batch in dataloader:
# process

In the train.py functions:

def train_one_epoch(model, dataloader, criterion, optimizer, device):
model.train()
running_loss = 0.0
for images, masks in tqdm(dataloader, desc='Training'):
images, masks = images.to(device), masks.to(device)
optimizer.zero_grad()
outputs = model(images)
loss = criterion(outputs, masks.float())
loss.backward()
optimizer.step()
running_loss += loss.item() * images.size(0)
epoch_loss = running_loss / len(dataloader.dataset)
return epoch_loss

Similarly for validation:

def validate(model, dataloader, criterion, device):
model.eval()
running_loss = 0.0
dice_scores = []
with torch.no_grad():
for images, masks in tqdm(dataloader, desc='Validating'):
images, masks = images.to(device), masks.to(device)
outputs = model(images)
loss = criterion(outputs, masks.float())
running_loss += loss.item() * images.size(0)
# Compute Dice
dice = dice_coefficient(outputs, masks)
dice_scores.append(dice.item())
epoch_loss = running_loss / len(dataloader.dataset)
mean_dice = np.mean(dice_scores)
return epoch_loss, mean_dice, dice_scores

Wait, but the user wants to store each batch's Dice score. So in validate function, return the list of batch Dice scores.

But the function above returns the epoch loss, mean_dice (average over all batches?), and the dice_scores list. But according to user instruction, during validation, collect Dice per batch per epoch, and store in Excel. So each epoch's list of batch Dice scores.

Similarly for testing.

Testing function:

def test(model, dataloader, device):
model.eval()
dice_scores = []
all_preds = []
all_masks = []
all_filenames = []
with torch.no_grad():
for images, masks, filenames in tqdm(dataloader, desc='Testing'):
# Wait, how to get filenames? The Dataset doesn't return them. Need to modify Dataset to return filename.
Ah, here's a problem. For the visualization of test predictions, we need to know the filenames. But currently, the Dataset's __getitem__ returns image and mask only. So we need to modify the Dataset to also return the image filename.

Modify dataset.py's __getitem__:

return image, mask, image_name (without path)

So in Dataset's __getitem__:

def __getitem__(self, idx):
...
return image, mask, image_name

But mask is processed as before. Then, in the DataLoader, each batch will have images, masks, filenames.

But this requires modifying the Dataset class. So adding returning the image name.

Thus, in dataset.py:

class GrayscaleBinaryMaskDataset(Dataset):
...
def __getitem__(self, idx):
...
return image, mask, image_name

But then, mask processing needs to be adjusted. Previously, mask was transformed to tensor and long. Also, in the Dataset's __getitem__, after transforming:

mask = (mask > 0.5).float().squeeze().long()

Wait, no, earlier code steps:

In the __getitem__:

mask is loaded as PIL Image, converted to array thresholded, then back to PIL, then transformed with mask_transform (Resize and ToTensor), which gives mask as tensor of shape (1, H, W), values 0 or 1. Then:

mask = mask.squeeze().long() # shape (H, W)

But BCEWithLogitsLoss expects masks to be (B, H, W) for input (B, 1, H, W) outputs. Wait, the loss function:

output shape: (batch, 1, H, W)
masks shape: (batch, H, W) as long or float?

The BCEWithLogitsLoss expects targets to be the same shape as output. Wait, no:

BCEWithLogitsLoss: the targets should be the same shape as the input (logits). So output is (B, 1, H, W), masks should be (B, 1, H, W). But masks are binary.

Ah, this is a mistake. So in the Dataset, the mask should be a tensor of shape (1, H, W), with values 0 and 1.

So adjusting the Dataset's __getitem__:

mask = self.mask_transform(mask) # shape (1, H, W), float 0. or 1.
mask = (mask > 0.5).float() # ensure binary
mask = mask.long() # but BCEWithLogitsLoss expects float targets. Wait, no:

Wait, BCEWithLogitsLoss combines sigmoid and BCE loss. The targets should be the same shape as the output, with values 0. or 1. (float). So the mask should be float.

So in Dataset:

mask = self.mask_transform(mask)
mask = mask.float() # convert to float (already 0. or 1.)

So the mask tensor has shape (1, H, W), float.

Therefore, in the __getitem__:

return image, mask, image_name

Where image is (1, H, W), float.

Now, back to testing. The test function needs to collect predictions, ground truths, and filenames to visualize later.

So modifying the test function:

def test(model, dataloader, device, save_dir):
model.eval()
dice_scores = []
all_preds = []
all_masks = []
all_filenames = []
with torch.no_grad():
for images, masks, filenames in tqdm(dataloader, desc='Testing'):
images, masks = images.to(device), masks.to(device)
outputs = model(images)
# Compute Dice for each batch
batch_dice = dice_coefficient(outputs, masks)
dice_scores.extend(batch_dice.tolist())
# Save predictions and masks for visualization
preds = torch.sigmoid(outputs) > 0.5
all_preds.append(preds.cpu())
all_masks.append(masks.cpu())
all_filenames.extend(filenames)
# Concatenate all batches
all_preds = torch.cat(all_preds)
all_masks = torch.cat(all_masks)
# Visualize random 5 samples
visualize_predictions(all_preds, all_masks, all_filenames, save_dir)
# Save Dice scores to Excel
return dice_scores

But need to implement visualize_predictions, which creates a plot with 5 random samples.

Implementing visualize_predictions:

def visualize_predictions(preds, masks, filenames, save_dir):
indices = np.random.choice(len(preds), size=5, replace=False)
fig, axes = plt.subplots(5, 3, figsize=(15, 25))
for i, idx in enumerate(indices):
pred = preds[idx].squeeze().numpy()
mask = masks[idx].squeeze().numpy()
image_path = filenames[idx]
# Assuming images are in a directory, but during testing, how to get the actual image?
# Wait, no. The test function only has preds, masks, and filenames. But to visualize the input image, need to load it again. Which is not feasible here. Alternatively, during testing, also collect the images.

This suggests that the test function needs to collect the original images as well. But that would require storing all images in memory, which is not feasible for large datasets. Alternatively, during testing, for visualization, save the sample indices and then reload the images.

But this complicates things. Perhaps better to modify the test function to collect the images. However, this could consume memory. Alternatively, during testing, for each batch, store the images along with preds and masks.

Modify the test function:

def test(...):
...
all_images = []
...
for images, masks, filenames in ...:
...
all_images.append(images.cpu())
...
all_images = torch.cat(all_images)
visualize_predictions(all_images, all_masks, all_preds, all_filenames, indices, save_dir)

But this requires storing all test images in memory. If the test set is large, this might not be feasible. However, given the dataset size of 5000 samples and assuming resized to 256x256, it's manageable.

Alternatively, during visualization, select 5 random indices and load those images from disk. But how to get the image data without reloading? Since the visualization is done after testing, perhaps during testing, store the indices of the samples to visualize and then reload them.

But this adds complexity. Perhaps the simplest way is to collect images, masks, preds, and filenames during testing. So:

In test function:

all_images = []
all_masks = []
all_preds = []
all_filenames = []

Within the loop:

all_images.append(images.cpu())
all_masks.append(masks.cpu())
all_preds.append(preds.cpu())
all_filenames.extend(filenames)

After collecting all:

all_images = torch.cat(all_images)
all_masks = torch.cat(all_masks)
all_preds = torch.cat(all_preds)

Then, in visualize_predictions:

def visualize_predictions(images, masks, preds, filenames, save_dir):
indices = np.random.choice(len(images), size=5, replace=False)
fig, axes = plt.subplots(5, 3, figsize=(15, 25))
for row, idx in enumerate(indices):
image = images[idx].squeeze().numpy()
mask = masks[idx].squeeze().numpy()
pred = preds[idx].squeeze().numpy()
filename = filenames[idx]
# Plot each in subplot
axes[row, 0].imshow(image, cmap='gray')
axes[row, 0].set_title(f'Input: {filename}')
axes[row, 0].axis('off')
axes[row, 1].imshow(mask, cmap='gray')
axes[row, 1].set_title('Ground Truth')
axes[row, 1].axis('off')
axes[row, 2].imshow(pred, cmap='gray')
axes[row, 2].set_title('Prediction')
axes[row, 2].axis('off')
plt.tight_layout()
plt.savefig(os.path.join(save_dir, 'test_predictions.png'))
plt.close()

This way, the images are collected during testing and used for visualization.

Now, the main.py:

Handle data splitting using train_test_split. The dataset is loaded, then indices are split into train, val, test.

from sklearn.model_selection import train_test_split

dataset = GrayscaleBinaryMaskDataset(...)
indices = list(range(len(dataset)))
train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)
train_idx, val_idx = train_test_split(train_idx, test_size=0.1/0.8, random_state=42) # 0.1/0.8 = 12.5% of 80% is 10%

Wait, original split: 80% train, 10% val, 10% test. So total data is 100%. So first split into train+val and test (80%+10% vs 10% â†’ test_size=0.1). No, user said split into 80% train, 10% val, 10% test. So first split into 80% train and 20% remaining. Then split remaining into 50% val and 50% test.

train_size=0.8, test_size=0.2. Then split test into two equal parts.

train_idx, temp_idx = train_test_split(indices, test_size=0.2, random_state=42)
val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)

Yes. So:

train_dataset = Subset(dataset, train_idx)
val_dataset = Subset(dataset, val_idx)
test_dataset = Subset(dataset, test_idx)

DataLoaders are created with batch_size and shuffle.

Now, model summary using torchinfo:

from torchinfo import summary

model = UNet().to(device)
summary(model, input_size=(1, 1, 256, 256)) # batch_size, channels, H, W

Print the total number of parameters.

Putting all together in main.py:

if __name__ == '__main__':
# Parameters
image_dir = 'path/to/images'
mask_dir = None # same as image_dir
mask_suffix = '_seg'
img_size = 256
batch_size = 16
epochs = 20
lr = 0.001
save_path = 'results/'
os.makedirs(save_path, exist_ok=True)

# Dataset and DataLoader
dataset = GrayscaleBinaryMaskDataset(image_dir, mask_dir, mask_suffix, img_size)
indices = list(range(len(dataset)))
train_idx, temp_idx = train_test_split(indices, test_size=0.2, random_state=42)
val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)

train_loader = DataLoader(Subset(dataset, train_idx), batch_size=batch_size, shuffle=True)
val_loader = DataLoader(Subset(dataset, val_idx), batch_size=batch_size)
test_loader = DataLoader(Subset(dataset, test_idx), batch_size=batch_size)

# Model, loss, optimizer
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = UNet().to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# Training loop
train_losses = []
val_losses = []
val_dice_scores = []
start_time = time.time()

for epoch in range(epochs):
print(f'Epoch {epoch+1}/{epochs}')
train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)
val_loss, val_dice, dice_list = validate(model, val_loader, criterion, device)

train_losses.append(train_loss)
val_losses.append(val_loss)
val_dice_scores.append(dice_list)

# Save losses to Excel
pd.DataFrame([range(1, epoch+2), train_losses]).to_excel(os.path.join(save_path, 'train_losses.xlsx'), index=False, header=False)
pd.DataFrame([range(1, epoch+2), val_losses]).to_excel(os.path.join(save_path, 'val_losses.xlsx'), index=False, header=False)

# Save Dice scores for validation
pd.DataFrame(val_dice_scores).to_excel(os.path.join(save_path, 'validation_dice_scores.xlsx'), index=False)

total_time = time.time() - start_time
print(f'Training completed in {total_time:.2f} seconds')

# Save model
torch.save(model, os.path.join(save_path, 'unet_model.pth'))
torch.save(model.state_dict(), os.path.join(save_path, 'unet_model_state.pth'))

# Plot losses
plot_losses(train_losses, val_losses, save_path)

# Testing
test_dice = test(model, test_loader, device, save_path)
pd.DataFrame([test_dice]).to_excel(os.path.join(save_path, 'test_dice_scores.xlsx'), index=False)

# Model summary
from torchinfo import summary
summary(model, input_size=(1, 1, img_size, img_size))

But there are many missing functions here. Need to implement plot_losses, and the validate and test functions to return batch Dice scores.

Now, putting all these pieces into the respective files.

Potential errors and checks:

- The Dataset returns image, mask, filename. The mask must be a tensor of shape (1, H, W), float 0/1.

- BCEWithLogitsLoss expects the mask to be float.

- Device management (moving to GPU).

- Properly flattening and concatenating tensors during visualization.

- Handling the Excel files correctly, especially for Dice scores which are lists of lists.

- The validation function should return the epoch loss, mean Dice, and list of batch Dice scores.

Modifying validate function:

def validate(model, dataloader, criterion, device):
model.eval()
running_loss = 0.0
batch_dice_scores = []
with torch.no_grad():
for images, masks, _ in tqdm(dataloader, desc='Validating'):
images, masks = images.to(device), masks.to(device)
outputs = model(images)
loss = criterion(outputs, masks)
running_loss += loss.item() * images.size(0)
# Compute Dice per batch
dice = dice_coefficient(outputs, masks)
batch_dice_scores.extend(dice.tolist())
epoch_loss = running_loss / len(dataloader.dataset)
return epoch_loss, batch_dice_scores

Similarly, the dice_coefficient function:

def dice_coefficient(y_pred, y_true, smooth=1e-6):
y_pred = torch.sigmoid(y_pred)
y_pred_binary = (y_pred > 0.5).float()
intersection = (y_pred_binary * y_true).sum(dim=(1, 2, 3)) # (batch, 1, H, W)
union = y_pred_binary.sum(dim=(1, 2, 3)) + y_true.sum(dim=(1, 2, 3))
dice = (2. * intersection + smooth) / (union + smooth)
return dice

This assumes y_true has the same shape as y_pred (batch, 1, H, W).

Now, in the validate function, batch_dice_scores collects all batch Dice scores as a list. For each epoch, append this list to val_dice_scores.

In the main loop:

val_loss, batch_dice = validate(...)
val_losses.append(val_loss)
val_dice_scores.append(batch_dice)

Then, after each epoch, save the validation_dice_scores.xlsx as a DataFrame where each row is an epoch's batch Dice scores.

Similarly for testing.

Finally, the visualization function.

Now, considering all of this, the final code would be structured as follows.

But given the complexity, the code may have bugs that need to be ironed out, but the overall structure should be as outlined.
Below is a modular and comprehensive implementation of a U-Net model for binary segmentation of grayscale images using PyTorch, structured into four Python files as requested.

ðŸ“ dataset.py â€“ Custom Dataset Class
python


1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
âŒ„
âŒ„
âŒ„
âŒ„
âŒ„
âŒ„
import os
import torch
from torch.utils.data import Dataset, Subset
from torchvision import transforms
from PIL import Image
import numpy as np
from sklearn.model_selection import train_test_split

class GrayscaleBinaryMaskDataset(Dataset):
    def __init__(self, image_dir, mask_dir=None, mask_suffix='_seg', img_size=256):
        self.image_dir = image_dir
        self.mask_dir = mask_dir if mask_dir else image_dir
        self.mask_suffix = mask_suffix
        self.img_size = img_size
        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]

        # Transformations
        self.image_transform = transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
        ])
        self.mask_transform = transforms.Compose([
            transforms.Resize((img_size, img_size), interpolation=Image.NEAREST),
            transforms.ToTensor(),
        ])

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        image_name = self.image_files[idx]
        base_name = os.path.splitext(image_name)[0]
        mask_name = f"{base_name}{self.mask_suffix}.png"

        image_path = os.path.join(self.image_dir, image_name)
        mask_path = os.path.join(self.mask_dir, mask_name)

        # Load images
        image = Image.open(image_path).convert('L')
        mask = Image.open(mask_path).convert('L')

        # Apply transformations
        image = self.image_transform(image)
        mask = self.mask_transform(mask)

        # Threshold mask to binary (0 or 1)
        mask = (mask > 0.5).float()

        return image, mask, image_name

ðŸ“ model.py â€“ U-Net Architecture
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchinfo import summary

class UNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=1):
        super(UNet, self).__init__()

        def CBR(in_channels, out_channels):
            return nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 3, padding=1),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True),
                nn.Conv2d(out_channels, out_channels, 3, padding=1),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True)
            )

        # Encoder
        self.enc1 = CBR(in_channels, 64)
        self.enc2 = CBR(64, 128)
        self.enc3 = CBR(128, 256)
        self.enc4 = CBR(256, 512)
        self.bottleneck = CBR(512, 1024)

        self.pool = nn.MaxPool2d(2)

        # Decoder
        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
        self.dec4 = CBR(1024, 512)
        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.dec3 = CBR(512, 256)
        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec2 = CBR(256, 128)
        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec1 = CBR(128, 64)

        self.final_conv = nn.Conv2d(64, out_channels, 1)

    def forward(self, x):
        # Encoder
        conv1 = self.enc1(x)
        x = self.pool(conv1)
        conv2 = self.enc2(x)
        x = self.pool(conv2)
        conv3 = self.enc3(x)
        x = self.pool(conv3)
        conv4 = self.enc4(x)
        x = self.pool(conv4)

        x = self.bottleneck(x)

        # Decoder
        x = self.upconv4(x)
        x = torch.cat([x, conv4], dim=1)
        x = self.dec4(x)

        x = self.upconv3(x)
        x = torch.cat([x, conv3], dim=1)
        x = self.dec3(x)

        x = self.upconv2(x)
        x = torch.cat([x, conv2], dim=1)
        x = self.dec2(x)

        x = self.upconv1(x)
        x = torch.cat([x, conv1], dim=1)
        x = self.dec1(x)

        return self.final_conv(x)

profile
Qwen3-235B-A22B
1:54 am
Thinking completed
Â·
38,912 tokens budget
Okay, let's tackle this problem. The user wants a complete U-Net implementation in PyTorch for binary segmentation of grayscale images. The code needs to be split into several modules and follow specific guidelines. Let me start by breaking down each part.

First, the custom Dataset class. The dataset has grayscale images and binary masks. The masks are either in the same folder with a suffix like "_seg.png" or in separate folders. The Dataset needs to load image-mask pairs correctly. I need to make sure that only .png files are considered, ignoring other files. Also, transforms should be applied to both image and mask. Since the images are grayscale, I should read them as single-channel. The masks are binary, so converting them to 0s and 1s is necessary.

Next, the U-Net model. Binary segmentation with small foreground. The standard U-Net has an encoder-decoder structure with skip connections. For binary output, the final layer should have a single channel with a sigmoid activation. Since the foreground is small, maybe using a dice loss or BCEWithLogitsLoss with class weights. But the user didn't specify loss, so perhaps default to BCEWithLogitsLoss. Also, the model should be efficient but not too big, considering around 5000 samples.

Training, validation, testing functions. Need to handle the loops for each phase. For training, optimizer, loss function, backprop. For validation and testing, calculate Dice score. The Dice score is calculated per batch and stored in Excel. Also, saving losses and Dice scores each epoch. Using tqdm to show progress bars.

Data loading: Split into 80% train, 10% val, 10% test using train_test_split. Since the data isn't copied, the indices are split. Using Subset to create datasets from the original dataset. DataLoader setup with appropriate batch sizes. Transforms: resize, convert to tensor, normalize for images (maybe to [0,1], but since grayscale, normalization might be done via ToTensor). For masks, ensure they are binary.

Hyperparameters: Learning rate, batch size, epochs. Need to choose reasonable defaults. Maybe batch size 32, learning rate 0.001. Adam optimizer. Early stopping or checkpointing could help prevent overfitting, but user didn't mention, so maybe not. Training time tracking.

Model summary using torchinfo. Need to install torchinfo. The model input should be (1, H, W), assuming images are resized to a fixed size. User didn't specify image size, so maybe default to 256x256. Or let the user choose, but code should handle resizing. So in transforms, Resize to a fixed size.

Excel files: Using pandas to write DataFrames to Excel. Each epoch's loss values are appended as rows. For Dice scores, each batch's score per epoch is saved as columns. So for validation_dice_scores.xlsx, each row is an epoch, each column a batch. Similarly for test.

Visualization functions: After training, plot training and validation losses on the same graph. Save as png. For testing, visualize 5 random samples with input, ground truth, prediction. Each subplot row has 3 images with titles. Also display filenames as titles. Saving that plot.

Main script: Putting everything together. Loading dataset, splitting, creating DataLoaders, initializing model, training, validation, testing, saving everything. Also printing model summary, parameters, data sizes, etc.

Potential issues: Ensuring that transforms are applied correctly to both image and mask. For example, random cropping or flipping would require the same transform for both. But user said no augmentations, so just Resize and ToTensor. Also, handling different mask naming conventions. The Dataset class should accept a mask suffix parameter.

Testing the code structure. Each module (dataset.py, model.py, train.py, main.py) should have their respective parts. The main.py runs the training loop when executed.

Let me outline each part step by step.

For dataset.py: Create a GrayscaleBinaryMaskDataset class. The __init__ takes image_dir, mask_dir (could be same as image_dir), mask_suffix. The __getitem__ loads image and corresponding mask. Apply transforms if any. The transforms should be composed of functions that handle both image and mask, like Resize, ToTensor. Wait, but PyTorch transforms like ToTensor() automatically handle PIL Images. So for transforms that affect both, like Resize, they can be applied to both image and mask. But for transforms that need to be same for both (like random flip), need to apply same random seed. But since no augmentations, just Resize and ToTensor.

Wait, the user says "Ensure needed transforms on input image and mask pairs (Re-size images correctly, ensure gray scale and binaryâ€¦)". So Resize to a fixed size, ToTensor for images (converts to tensor and scales to [0,1], since it's a PIL Image). For masks, ToTensor converts to tensor, but since they are binary, it's okay. But maybe after loading, the mask is converted to a binary mask (0 and 1). So in the Dataset, after loading the mask image, convert it to binary. Because sometimes masks might be saved as 0 and 255. So in the Dataset, after loading the mask, do mask = mask.convert('L') and then threshold to 0 and 1. Or divide by 255 to get 0 or 1. Because if the mask is saved as 0 and 255, dividing by 255 would make it 0 and 1. So in the Dataset's __getitem__, after loading mask as a grayscale image, convert to numpy array, threshold at 0.5, then convert to tensor. Alternatively, use transforms.Lambda to do that. But how?

Alternatively, in the Dataset's __getitem__, after loading the mask image, process it:

mask = Image.open(mask_path).convert('L')
mask = np.array(mask)
mask = (mask > 127.5).astype(np.uint8) # assuming 0 and 255, threshold to 0 or 1
mask = torch.from_numpy(mask).long()

But ToTensor would convert the image to float32, but for mask, we need to keep it as long. So perhaps in the Dataset, the image is converted to tensor and normalized (maybe not necessary if using ToTensor which scales to [0,1]), and the mask is converted to a binary tensor.

So the Dataset class should handle that.

For the model.py: Implement U-Net. The standard architecture has encoder blocks with double convolutions, downsampling, decoder blocks with upsampling and skip connections. For binary segmentation, the output channel is 1. The model should be defined as a class with encoder and decoder parts. Also, for small foreground, maybe using Dice loss or BCEWithLogitsLoss. But the loss function is handled in the training script.

In train.py: Functions for train, validate, test. Each function would loop over the DataLoader, compute loss, track metrics. For training, backprop; for validation/testing, compute Dice. The Dice score is calculated per batch. For each batch, Dice is (2*intersection + smooth)/(union + smooth). Need to implement that. Also, during validation, collect all Dice scores per batch, and save to Excel. Similarly for testing.

For saving losses and Dice scores: Use pandas DataFrames. Each epoch's average loss is saved as a row in the Excel file. For Dice, each batch's score is a column, with epochs as rows.

In main.py: All the setup. Load dataset, split into train/val/test. Create DataLoaders. Initialize model, optimizer, loss function. Train the model, track time. Save model, state dict. Plot losses. Test the model, save Dice scores and visualize predictions.

Hyperparameters: Learning rate 0.001, batch size 16 or 32. Epochs maybe 20. Image size, say, 256x256. The transforms.Resize((256, 256)).

Now, putting all together.

Potential pitfalls:

- Handling the mask filenames. If the images are in the same directory, mask_suffix is needed. For example, if image is 'img.png', mask could be 'img_seg.png'. So in the Dataset class, mask name is image name + suffix.

- DataLoader workers: If using transforms with NumPy operations, need to ensure that the Dataset is picklable. But using standard transforms should be okay.

- Model summary with torchinfo: The input size needs to be specified. For example, (1, 256, 256).

- Dice calculation: Need to flatten the tensors and compute per batch. Also, smooth to avoid division by zero.

- Saving Excel files: Each time appending a new row. However, the current plan is to collect all losses during training and save at the end. Or after each epoch, append to the Excel. Using pandas, perhaps create a DataFrame each time and save with mode 'a' or 'w'. But better to collect all losses during training (as lists) and save at the end.

Wait, the user says:

For each epoch, track and save average training loss in train_losses.xlsx, same for val. So after each epoch, append the loss to the Excel. But how to do that? Maybe collect all the losses during training (as lists), and after training, save to Excel. But the user instruction says during training, save each epoch. So perhaps after each epoch, write the current epoch's loss to the Excel. Similarly for validation.

But writing to Excel during training could be slow. Maybe better to collect all losses in lists, then save at the end. But the user says "track and save the average training loss for each epoch in a separate Excel file...". So probably collect during training and save after each epoch. But maybe using pandas, create a DataFrame where each column is an epoch, and rows are epoch numbers and losses. Wait, the first row contains epoch numbers, second the loss. So for train_losses.xlsx, sheet has row 0: [1,2,3,...], row 1: [loss1, loss2, ...].

Wait, the user says: first row epoch numbers, second average loss. So each Excel file has two rows: epochs and losses. Similarly for validation. So for each new epoch, append the value to the list, and when saving, overwrite the Excel file with updated data. That way, if training stops, the saved file has up to the last completed epoch.

But how to handle this in code. Each time, create a new DataFrame with the current epochs and losses, and save it. So for example, after epoch 1, the Excel has epochs 1, loss. After epoch 2, epochs 1,2 and losses. So during training, for each epoch, append the average loss to a list, then at the end of the epoch, save the entire list into the Excel.

Same for validation losses.

For Dice scores during validation and testing: each epoch's batches' Dice scores are stored as columns. So for each epoch, collect a list of Dice scores per batch, then save them as a row in the Excel. So each row in 'validation_dice_scores.xlsx' corresponds to an epoch, each column a batch's Dice.

Implementing that requires collecting all batch Dice scores for each validation epoch, then saving them as a DataFrame with each row being the epoch's batch results.

Now, coding each part.

Starting with dataset.py:

Implement the Dataset class. The __init__ reads all image files in image_dir, filtering .png. Then, for each image, the mask path is determined based on mask_dir and mask_suffix. The __getitem__ loads image and mask, applies transforms (same for both), converts image to grayscale, mask to binary.

Wait, but transforms in PyTorch are applied separately. So if there's a Resize, it should be applied to both image and mask. So the transforms should be a list that can handle both. For example:

transform = transforms.Compose([
transforms.Resize((img_size, img_size)),
transforms.ToTensor()
])

But for the mask, after resizing, it should remain binary. However, when using transforms.Resize for masks, using bilinear interpolation might create non-binary values. So to avoid that, after resizing, the mask should be thresholded again.

Ah, right! So during resizing of the mask, we need to use nearest neighbor interpolation to preserve the binary nature.

Therefore, in the Dataset, during transforms, if there's a Resize, apply it to image with bilinear and to mask with nearest. But how to do that in the transform pipeline?

This suggests that the transforms should be applied separately to image and mask. Therefore, the Dataset class might need a paired transform approach. However, since the user didn't ask for augmentations, perhaps the transforms are just Resize and ToTensor. To handle this correctly, perhaps split the transforms into those that can be applied to both, and those that are separate.

Alternatively, in the Dataset's __getitem__, apply Resize to image and mask with appropriate interpolation.

For example:

In __getitem__:

image = Image.open(image_path).convert('L') # grayscale
mask = Image.open(mask_path).convert('L') # assuming mask is grayscale, 0 and 255

if self.transform:
image = self.transform(image)
mask = self.transform(mask)

But then Resize would use bilinear for both, which is bad for masks. So need to apply Resize to image with bilinear and mask with nearest.

Hmm, this complicates things. Therefore, perhaps the transform should be applied after resizing both image and mask appropriately.

Alternative approach: in the Dataset's __getitem__, after loading image and mask as PIL Images, perform resizing manually.

But that would require passing the desired size to the Dataset.

So maybe the Dataset class has a parameter for target size, and inside __getitem__, it resizes both image and mask.

But that would hardcode the transforms into the Dataset. Alternatively, use a custom transform that applies resizing with different interpolation methods for image and mask.

Alternatively, use a paired transform function. But how to implement that.

Perhaps create a custom transform function that takes both image and mask as inputs. For example:

class ResizePair:
def __init__(self, size):
self.size = size
self.resize_image = transforms.Resize(size, interpolation=Image.BILINEAR)
self.resize_mask = transforms.Resize(size, interpolation=Image.NEAREST)

def __call__(self, image, mask):
return self.resize_image(image), self.resize_mask(mask)

Similarly, other transforms can be handled. Then in the Dataset's __init__, the transform would be applied to both image and mask.

So the Dataset class could have a transform for both image and mask. Then, when applying transforms, they are done in a way that operations like Resize use correct interpolation for each.

But how to structure this. The Dataset class's __getitem__ would apply the transform to both image and mask.

So the Dataset's code could look like:

class GrayscaleBinaryMaskDataset(Dataset):
def __init__(self, image_dir, mask_dir=None, mask_suffix='_seg', transform=None):
self.image_dir = image_dir
self.mask_dir = mask_dir if mask_dir is not None else image_dir
self.mask_suffix = mask_suffix
self.transform = transform
# Get all .png files in image_dir, ignoring others
self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]
# Filter mask files? Or assume that for each image, there's a corresponding mask

def __len__(self):
return len(self.image_files)

def __getitem__(self, idx):
image_name = self.image_files[idx]
image_path = os.path.join(self.image_dir, image_name)
# Generate mask name
base_name = os.path.splitext(image_name)[0]
mask_name = base_name + self.mask_suffix + '.png'
mask_path = os.path.join(self.mask_dir, mask_name)
# Load image and mask
image = Image.open(image_path).convert('L') # grayscale
mask = Image.open(mask_path).convert('L') # assuming mask is grayscale, 0 or 255
# Convert mask to binary (0 or 1)
mask = np.array(mask)
mask = (mask > 127.5).astype(np.uint8) # threshold
mask = Image.fromarray(mask)
# Apply transforms if any
if self.transform:
image, mask = self.transform(image, mask)
# Convert to tensor
image = transforms.ToTensor()(image)
# Mask to tensor, long type for loss
mask = transforms.ToTensor()(mask).squeeze(0) # remove channel dim, shape (H,W)
mask = mask.long()
return image, mask

Wait, but the transform here is supposed to handle both image and mask. However, the standard transforms in torchvision don't do that. So the transform passed to the Dataset should be a function that takes both image and mask. Therefore, we need to define a composed transform that does that.

For example, using the above ResizePair class. So in the main code, when creating transforms, it would be something like:

transform = transforms.Compose([
ResizePair((256, 256)),
# other paired transforms
])

But how to handle ToTensor? Because ToTensor converts a PIL Image to tensor. So after resizing, we can apply ToTensor separately.

Alternatively, separate transforms for image and mask. Maybe split the transforms into two parts: paired transforms (like Resize) and individual transforms (like ToTensor).

Alternatively, handle paired transforms in the Dataset's __getitem__:

In __init__, have separate transforms for image and mask, but that might complicate things.

Alternatively, in the transform passed to Dataset, it's a function that takes image and mask and returns transformed image and mask. So the user would define transforms as a list of such paired transforms.

But implementing that would require using something like Albumentations, but since the user wants to avoid additional libraries, perhaps better to handle it manually.

Alternatively, apply Resize with nearest for mask and bilinear for image. So inside the Dataset's __getitem__, after loading, resize both:

if self.transform is not None:
# assume transform is a ResizePair instance
image, mask = self.transform(image, mask)

But this approach requires defining custom transform classes.

Alternatively, handle resizing in the Dataset's __getitem__:

def __init__(self, ... , img_size=256):
...
self.img_size = img_size

...

def __getitem__(self, idx):
...
# resize
image = image.resize((self.img_size, self.img_size), Image.BILINEAR)
mask = mask.resize((self.img_size, self.img_size), Image.NEAREST)
...

Then, other transforms like ToTensor can be applied separately. Wait, but then the resizing is hardcoded. So maybe the Dataset can take a transform argument, which is a function applied to both image and mask. But how to compose multiple transforms.

This seems complicated. Maybe for simplicity, since the user says no augmentations, only resizing, we can handle resizing directly in the __getitem__.

But perhaps better to use transforms.Resize for image, and for mask, use Resize with nearest neighbor. So separate transforms:

from torchvision import transforms

class GrayscaleBinaryMaskDataset(Dataset):
def __init__(self, image_dir, mask_dir=None, mask_suffix='_seg', img_size=256):
self.image_dir = image_dir
self.mask_dir = mask_dir if mask_dir is not None else image_dir
self.mask_suffix = mask_suffix
self.img_size = img_size
self.image_transform = transforms.Compose([
transforms.Resize((img_size, img_size)),
transforms.ToTensor()
])
self.mask_transform = transforms.Compose([
transforms.Resize((img_size, img_size), interpolation=Image.NEAREST),
transforms.ToTensor()
])

def __getitem__(self, idx):
...
# after loading image and mask as PIL images
image = self.image_transform(image)
mask = self.mask_transform(mask)
# mask is tensor, but ToTensor scales to [0,1], so threshold again
mask = (mask > 0.5).float()
# but we need to have mask as 0 and 1, but also as long for loss
mask = mask.squeeze().long() # assuming mask is (1, H, W)
return image, mask

Wait, but when using transforms.ToTensor(), the mask (which is already binary 0 and 255) becomes [0,1] tensor. Then resizing with nearest neighbor preserves the binary nature. However, if the mask was resized with bilinear, it could have intermediate values, but with nearest, it remains binary. Then, ToTensor() would give 0. or 1. (if original was 0 and 255). But maybe better to threshold again after ToTensor.

Alternatively, after applying mask_transform, which is Resize and ToTensor:

mask = mask > 0.5 # converts to boolean tensor
mask = mask.long() # 0 and 1

This ensures that even if some values are between 0 and 1 due to resizing (unlikely if Resize with NEAREST), they are thresholded.

This approach may work.

Now, moving to the model. Implementing U-Net.

The U-Net architecture has an encoder with downsampling and double convolution blocks, then a decoder with upsampling and skip connections.

The code for the model would be:

class UNet(nn.Module):
def __init__(self, in_channels=1, out_channels=1):
super(UNet, self).__init__()
# Define encoder layers
# Each block is two conv layers
def CBR(in_channels, out_channels):
return nn.Sequential(
nn.Conv2d(in_channels, out_channels, 3, padding=1),
nn.BatchNorm2d(out_channels),
nn.ReLU(inplace=True),
nn.Conv2d(out_channels, out_channels, 3, padding=1),
nn.BatchNorm2d(out_channels),
nn.ReLU(inplace=True)
)

self.enc1 = CBR(in_channels, 64)
self.enc2 = CBR(64, 128)
self.enc3 = CBR(128, 256)
self.enc4 = CBR(256, 512)
self.bottleneck = CBR(512, 1024)

self.pool = nn.MaxPool2d(2)

# Decoder
self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
self.dec4 = CBR(1024, 512)
self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
self.dec3 = CBR(512, 256)
self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
self.dec2 = CBR(256, 128)
self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
self.dec1 = CBR(128, 64)

self.final_conv = nn.Conv2d(64, out_channels, 1)

def forward(self, x):
# Encoder
conv1 = self.enc1(x)
x = self.pool(conv1)
conv2 = self.enc2(x)
x = self.pool(conv2)
conv3 = self.enc3(x)
x = self.pool(conv3)
conv4 = self.enc4(x)
x = self.pool(conv4)

x = self.bottleneck(x)

# Decoder
x = self.upconv4(x)
x = torch.cat([x, conv4], dim=1)
x = self.dec4(x)

x = self.upconv3(x)
x = torch.cat([x, conv3], dim=1)
x = self.dec3(x)

x = self.upconv2(x)
x = torch.cat([x, conv2], dim=1)
x = self.dec2(x)

x = self.upconv1(x)
x = torch.cat([x, conv1], dim=1)
x = self.dec1(x)

return self.final_conv(x)

Wait, but for binary segmentation, the output should be logits (single channel), and then apply sigmoid. But the loss function BCEWithLogitsLoss combines sigmoid and BCE loss, so the model outputs the logits. So the final layer outputs (batch, 1, H, W). The model's forward returns that.

Now, for training. The loss function would be BCEWithLogitsLoss. Also, considering class imbalance (small foreground), maybe weight the loss. But user didn't specify, so proceed with default.

Dice coefficient calculation:

def dice_score(pred, target, smooth=1e-6):
pred = torch.sigmoid(pred) > 0.5 # convert to binary
intersection = (pred * target).sum(dim=(1, 2))
union = pred.sum(dim=(1, 2)) + target.sum(dim=(1, 2))
dice = (2. * intersection + smooth) / (union + smooth)
return dice.mean()

Wait, but during validation/testing, the predictions are thresholded. However, during training, we don't threshold because loss uses logits. For Dice score calculation, yes, threshold.

But during validation, the model outputs logits, so we need to apply sigmoid and threshold.

But also, the target should be flattened or not? The Dice function should compute per batch.

Another approach:

def dice_coefficient(y_pred, y_true, smooth=1e-6):
# y_pred is logits, shape (B, 1, H, W)
# convert to probabilities
y_pred = torch.sigmoid(y_pred)
y_pred = y_pred.view(y_pred.size(0), -1)
y_true = y_true.view(y_true.size(0), -1)
intersection = (y_pred * y_true).sum(dim=1)
union = y_pred.sum(dim=1) + y_true.sum(dim=1)
dice = (2. * intersection + smooth) / (union + smooth)
return dice.mean()

But for Dice score during validation, per batch, we can compute this.

Now, the training loop:

In each epoch, iterate over the DataLoader. For each batch, compute output, loss, backprop. Track losses.

Validation loop: same but no backprop, compute Dice scores for each batch.

Testing loop: same as validation.

Saving losses and Dice scores:

After each epoch, append the average loss to the train_losses list, and same for val. Then, after training, save to Excel.

For Dice scores in validation and testing: collect a list of lists. Each epoch's list of batch Dice scores.

Now, implementing saving to Excel:

Using pandas:

import pandas as pd

train_losses = [] # collected after each epoch
val_losses = []
val_dice_scores = [] # list of lists
test_dice_scores = []

At the end of training:

df_train = pd.DataFrame([range(1, epochs+1), train_losses])
df_train.to_excel('train_losses.xlsx', index=False)

Similarly for val_losses. But wait, the first row should be epoch numbers, second the loss. So the DataFrame should have two rows. However, pandas DataFrames have rows as observations. So transposing:

df_train = pd.DataFrame({'Epoch': range(1, epochs+1), 'Loss': train_losses})
df_train.to_excel('train_losses.xlsx', index=False) # but this saves as two columns. Not matching user's instruction.

Wait, user says first row is epoch numbers, second row is losses. So each Excel file has two rows. So for example:

Row 0: 1, 2, 3, ..., epoch count
Row 1: loss1, loss2, ..., lossN

So the data is structured as two rows. So the DataFrame should have two rows and columns for each epoch.

So:

train_epochs = list(range(1, epochs+1))
train_losses_list = [...] # collected during training
df_train = pd.DataFrame([train_epochs, train_losses_list])
df_train.to_excel('train_losses.xlsx', index=False, header=False)

Similarly for val.

But then the Excel file's first row is epochs, second losses. This matches the user's requirement.

For Dice scores during validation: each epoch's batch Dice scores are a row. So for each epoch, collect a list of Dice scores (each batch's average Dice), then append that list to val_dice_scores. Then, when saving to Excel, each row is an epoch's list of batch Dice scores. Using pandas:

df_val_dice = pd.DataFrame(val_dice_scores)
df_val_dice.to_excel('validation_dice_scores.xlsx', index=False)

Same for test.

Now, the training procedure:

Implement training and validation loops with tqdm. For each batch in DataLoader:

with tqdm(...) as pbar:
for batch in dataloader:
# process

In the train.py functions:

def train_one_epoch(model, dataloader, criterion, optimizer, device):
model.train()
running_loss = 0.0
for images, masks in tqdm(dataloader, desc='Training'):
images, masks = images.to(device), masks.to(device)
optimizer.zero_grad()
outputs = model(images)
loss = criterion(outputs, masks.float())
loss.backward()
optimizer.step()
running_loss += loss.item() * images.size(0)
epoch_loss = running_loss / len(dataloader.dataset)
return epoch_loss

Similarly for validation:

def validate(model, dataloader, criterion, device):
model.eval()
running_loss = 0.0
dice_scores = []
with torch.no_grad():
for images, masks in tqdm(dataloader, desc='Validating'):
images, masks = images.to(device), masks.to(device)
outputs = model(images)
loss = criterion(outputs, masks.float())
running_loss += loss.item() * images.size(0)
# Compute Dice
dice = dice_coefficient(outputs, masks)
dice_scores.append(dice.item())
epoch_loss = running_loss / len(dataloader.dataset)
mean_dice = np.mean(dice_scores)
return epoch_loss, mean_dice, dice_scores

Wait, but the user wants to store each batch's Dice score. So in validate function, return the list of batch Dice scores.

But the function above returns the epoch loss, mean_dice (average over all batches?), and the dice_scores list. But according to user instruction, during validation, collect Dice per batch per epoch, and store in Excel. So each epoch's list of batch Dice scores.

Similarly for testing.

Testing function:

def test(model, dataloader, device):
model.eval()
dice_scores = []
all_preds = []
all_masks = []
all_filenames = []
with torch.no_grad():
for images, masks, filenames in tqdm(dataloader, desc='Testing'):
# Wait, how to get filenames? The Dataset doesn't return them. Need to modify Dataset to return filename.
Ah, here's a problem. For the visualization of test predictions, we need to know the filenames. But currently, the Dataset's __getitem__ returns image and mask only. So we need to modify the Dataset to also return the image filename.

Modify dataset.py's __getitem__:

return image, mask, image_name (without path)

So in Dataset's __getitem__:

def __getitem__(self, idx):
...
return image, mask, image_name

But mask is processed as before. Then, in the DataLoader, each batch will have images, masks, filenames.

But this requires modifying the Dataset class. So adding returning the image name.

Thus, in dataset.py:

class GrayscaleBinaryMaskDataset(Dataset):
...
def __getitem__(self, idx):
...
return image, mask, image_name

But then, mask processing needs to be adjusted. Previously, mask was transformed to tensor and long. Also, in the Dataset's __getitem__, after transforming:

mask = (mask > 0.5).float().squeeze().long()

Wait, no, earlier code steps:

In the __getitem__:

mask is loaded as PIL Image, converted to array thresholded, then back to PIL, then transformed with mask_transform (Resize and ToTensor), which gives mask as tensor of shape (1, H, W), values 0 or 1. Then:

mask = mask.squeeze().long() # shape (H, W)

But BCEWithLogitsLoss expects masks to be (B, H, W) for input (B, 1, H, W) outputs. Wait, the loss function:

output shape: (batch, 1, H, W)
masks shape: (batch, H, W) as long or float?

The BCEWithLogitsLoss expects targets to be the same shape as output. Wait, no:

BCEWithLogitsLoss: the targets should be the same shape as the input (logits). So output is (B, 1, H, W), masks should be (B, 1, H, W). But masks are binary.

Ah, this is a mistake. So in the Dataset, the mask should be a tensor of shape (1, H, W), with values 0 and 1.

So adjusting the Dataset's __getitem__:

mask = self.mask_transform(mask) # shape (1, H, W), float 0. or 1.
mask = (mask > 0.5).float() # ensure binary
mask = mask.long() # but BCEWithLogitsLoss expects float targets. Wait, no:

Wait, BCEWithLogitsLoss combines sigmoid and BCE loss. The targets should be the same shape as the output, with values 0. or 1. (float). So the mask should be float.

So in Dataset:

mask = self.mask_transform(mask)
mask = mask.float() # convert to float (already 0. or 1.)

So the mask tensor has shape (1, H, W), float.

Therefore, in the __getitem__:

return image, mask, image_name

Where image is (1, H, W), float.

Now, back to testing. The test function needs to collect predictions, ground truths, and filenames to visualize later.

So modifying the test function:

def test(model, dataloader, device, save_dir):
model.eval()
dice_scores = []
all_preds = []
all_masks = []
all_filenames = []
with torch.no_grad():
for images, masks, filenames in tqdm(dataloader, desc='Testing'):
images, masks = images.to(device), masks.to(device)
outputs = model(images)
# Compute Dice for each batch
batch_dice = dice_coefficient(outputs, masks)
dice_scores.extend(batch_dice.tolist())
# Save predictions and masks for visualization
preds = torch.sigmoid(outputs) > 0.5
all_preds.append(preds.cpu())
all_masks.append(masks.cpu())
all_filenames.extend(filenames)
# Concatenate all batches
all_preds = torch.cat(all_preds)
all_masks = torch.cat(all_masks)
# Visualize random 5 samples
visualize_predictions(all_preds, all_masks, all_filenames, save_dir)
# Save Dice scores to Excel
return dice_scores

But need to implement visualize_predictions, which creates a plot with 5 random samples.

Implementing visualize_predictions:

def visualize_predictions(preds, masks, filenames, save_dir):
indices = np.random.choice(len(preds), size=5, replace=False)
fig, axes = plt.subplots(5, 3, figsize=(15, 25))
for i, idx in enumerate(indices):
pred = preds[idx].squeeze().numpy()
mask = masks[idx].squeeze().numpy()
image_path = filenames[idx]
# Assuming images are in a directory, but during testing, how to get the actual image?
# Wait, no. The test function only has preds, masks, and filenames. But to visualize the input image, need to load it again. Which is not feasible here. Alternatively, during testing, also collect the images.

This suggests that the test function needs to collect the original images as well. But that would require storing all images in memory, which is not feasible for large datasets. Alternatively, during testing, for visualization, save the sample indices and then reload the images.

But this complicates things. Perhaps better to modify the test function to collect the images. However, this could consume memory. Alternatively, during testing, for each batch, store the images along with preds and masks.

Modify the test function:

def test(...):
...
all_images = []
...
for images, masks, filenames in ...:
...
all_images.append(images.cpu())
...
all_images = torch.cat(all_images)
visualize_predictions(all_images, all_masks, all_preds, all_filenames, indices, save_dir)

But this requires storing all test images in memory. If the test set is large, this might not be feasible. However, given the dataset size of 5000 samples and assuming resized to 256x256, it's manageable.

Alternatively, during visualization, select 5 random indices and load those images from disk. But how to get the image data without reloading? Since the visualization is done after testing, perhaps during testing, store the indices of the samples to visualize and then reload them.

But this adds complexity. Perhaps the simplest way is to collect images, masks, preds, and filenames during testing. So:

In test function:

all_images = []
all_masks = []
all_preds = []
all_filenames = []

Within the loop:

all_images.append(images.cpu())
all_masks.append(masks.cpu())
all_preds.append(preds.cpu())
all_filenames.extend(filenames)

After collecting all:

all_images = torch.cat(all_images)
all_masks = torch.cat(all_masks)
all_preds = torch.cat(all_preds)

Then, in visualize_predictions:

def visualize_predictions(images, masks, preds, filenames, save_dir):
indices = np.random.choice(len(images), size=5, replace=False)
fig, axes = plt.subplots(5, 3, figsize=(15, 25))
for row, idx in enumerate(indices):
image = images[idx].squeeze().numpy()
mask = masks[idx].squeeze().numpy()
pred = preds[idx].squeeze().numpy()
filename = filenames[idx]
# Plot each in subplot
axes[row, 0].imshow(image, cmap='gray')
axes[row, 0].set_title(f'Input: {filename}')
axes[row, 0].axis('off')
axes[row, 1].imshow(mask, cmap='gray')
axes[row, 1].set_title('Ground Truth')
axes[row, 1].axis('off')
axes[row, 2].imshow(pred, cmap='gray')
axes[row, 2].set_title('Prediction')
axes[row, 2].axis('off')
plt.tight_layout()
plt.savefig(os.path.join(save_dir, 'test_predictions.png'))
plt.close()

This way, the images are collected during testing and used for visualization.

Now, the main.py:

Handle data splitting using train_test_split. The dataset is loaded, then indices are split into train, val, test.

from sklearn.model_selection import train_test_split

dataset = GrayscaleBinaryMaskDataset(...)
indices = list(range(len(dataset)))
train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)
train_idx, val_idx = train_test_split(train_idx, test_size=0.1/0.8, random_state=42) # 0.1/0.8 = 12.5% of 80% is 10%

Wait, original split: 80% train, 10% val, 10% test. So total data is 100%. So first split into train+val and test (80%+10% vs 10% â†’ test_size=0.1). No, user said split into 80% train, 10% val, 10% test. So first split into 80% train and 20% remaining. Then split remaining into 50% val and 50% test.

train_size=0.8, test_size=0.2. Then split test into two equal parts.

train_idx, temp_idx = train_test_split(indices, test_size=0.2, random_state=42)
val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)

Yes. So:

train_dataset = Subset(dataset, train_idx)
val_dataset = Subset(dataset, val_idx)
test_dataset = Subset(dataset, test_idx)

DataLoaders are created with batch_size and shuffle.

Now, model summary using torchinfo:

from torchinfo import summary

model = UNet().to(device)
summary(model, input_size=(1, 1, 256, 256)) # batch_size, channels, H, W

Print the total number of parameters.

Putting all together in main.py:

if __name__ == '__main__':
# Parameters
image_dir = 'path/to/images'
mask_dir = None # same as image_dir
mask_suffix = '_seg'
img_size = 256
batch_size = 16
epochs = 20
lr = 0.001
save_path = 'results/'
os.makedirs(save_path, exist_ok=True)

# Dataset and DataLoader
dataset = GrayscaleBinaryMaskDataset(image_dir, mask_dir, mask_suffix, img_size)
indices = list(range(len(dataset)))
train_idx, temp_idx = train_test_split(indices, test_size=0.2, random_state=42)
val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)

train_loader = DataLoader(Subset(dataset, train_idx), batch_size=batch_size, shuffle=True)
val_loader = DataLoader(Subset(dataset, val_idx), batch_size=batch_size)
test_loader = DataLoader(Subset(dataset, test_idx), batch_size=batch_size)

# Model, loss, optimizer
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = UNet().to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# Training loop
train_losses = []
val_losses = []
val_dice_scores = []
start_time = time.time()

for epoch in range(epochs):
print(f'Epoch {epoch+1}/{epochs}')
train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)
val_loss, val_dice, dice_list = validate(model, val_loader, criterion, device)

train_losses.append(train_loss)
val_losses.append(val_loss)
val_dice_scores.append(dice_list)

# Save losses to Excel
pd.DataFrame([range(1, epoch+2), train_losses]).to_excel(os.path.join(save_path, 'train_losses.xlsx'), index=False, header=False)
pd.DataFrame([range(1, epoch+2), val_losses]).to_excel(os.path.join(save_path, 'val_losses.xlsx'), index=False, header=False)

# Save Dice scores for validation
pd.DataFrame(val_dice_scores).to_excel(os.path.join(save_path, 'validation_dice_scores.xlsx'), index=False)

total_time = time.time() - start_time
print(f'Training completed in {total_time:.2f} seconds')

# Save model
torch.save(model, os.path.join(save_path, 'unet_model.pth'))
torch.save(model.state_dict(), os.path.join(save_path, 'unet_model_state.pth'))

# Plot losses
plot_losses(train_losses, val_losses, save_path)

# Testing
test_dice = test(model, test_loader, device, save_path)
pd.DataFrame([test_dice]).to_excel(os.path.join(save_path, 'test_dice_scores.xlsx'), index=False)

# Model summary
from torchinfo import summary
summary(model, input_size=(1, 1, img_size, img_size))

But there are many missing functions here. Need to implement plot_losses, and the validate and test functions to return batch Dice scores.

Now, putting all these pieces into the respective files.

Potential errors and checks:

- The Dataset returns image, mask, filename. The mask must be a tensor of shape (1, H, W), float 0/1.

- BCEWithLogitsLoss expects the mask to be float.

- Device management (moving to GPU).

- Properly flattening and concatenating tensors during visualization.

- Handling the Excel files correctly, especially for Dice scores which are lists of lists.

- The validation function should return the epoch loss, mean Dice, and list of batch Dice scores.

Modifying validate function:

def validate(model, dataloader, criterion, device):
model.eval()
running_loss = 0.0
batch_dice_scores = []
with torch.no_grad():
for images, masks, _ in tqdm(dataloader, desc='Validating'):
images, masks = images.to(device), masks.to(device)
outputs = model(images)
loss = criterion(outputs, masks)
running_loss += loss.item() * images.size(0)
# Compute Dice per batch
dice = dice_coefficient(outputs, masks)
batch_dice_scores.extend(dice.tolist())
epoch_loss = running_loss / len(dataloader.dataset)
return epoch_loss, batch_dice_scores

Similarly, the dice_coefficient function:

def dice_coefficient(y_pred, y_true, smooth=1e-6):
y_pred = torch.sigmoid(y_pred)
y_pred_binary = (y_pred > 0.5).float()
intersection = (y_pred_binary * y_true).sum(dim=(1, 2, 3)) # (batch, 1, H, W)
union = y_pred_binary.sum(dim=(1, 2, 3)) + y_true.sum(dim=(1, 2, 3))
dice = (2. * intersection + smooth) / (union + smooth)
return dice

This assumes y_true has the same shape as y_pred (batch, 1, H, W).

Now, in the validate function, batch_dice_scores collects all batch Dice scores as a list. For each epoch, append this list to val_dice_scores.

In the main loop:

val_loss, batch_dice = validate(...)
val_losses.append(val_loss)
val_dice_scores.append(batch_dice)

Then, after each epoch, save the validation_dice_scores.xlsx as a DataFrame where each row is an epoch's batch Dice scores.

Similarly for testing.

Finally, the visualization function.

Now, considering all of this, the final code would be structured as follows.

But given the complexity, the code may have bugs that need to be ironed out, but the overall structure should be as outlined.
Below is a modular and comprehensive implementation of a U-Net model for binary segmentation of grayscale images using PyTorch, structured into four Python files as requested.

ðŸ“ dataset.py â€“ Custom Dataset Class
python


1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
âŒ„
âŒ„
âŒ„
âŒ„
âŒ„
âŒ„
import os
import torch
from torch.utils.data import Dataset, Subset
from torchvision import transforms
from PIL import Image
import numpy as np
from sklearn.model_selection import train_test_split

class GrayscaleBinaryMaskDataset(Dataset):
    def __init__(self, image_dir, mask_dir=None, mask_suffix='_seg', img_size=256):
        self.image_dir = image_dir
        self.mask_dir = mask_dir if mask_dir else image_dir
        self.mask_suffix = mask_suffix
        self.img_size = img_size
        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]

        # Transformations
        self.image_transform = transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
        ])
        self.mask_transform = transforms.Compose([
            transforms.Resize((img_size, img_size), interpolation=Image.NEAREST),
            transforms.ToTensor(),
        ])

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        image_name = self.image_files[idx]
        base_name = os.path.splitext(image_name)[0]
        mask_name = f"{base_name}{self.mask_suffix}.png"

        image_path = os.path.join(self.image_dir, image_name)
        mask_path = os.path.join(self.mask_dir, mask_name)

        # Load images
        image = Image.open(image_path).convert('L')
        mask = Image.open(mask_path).convert('L')

        # Apply transformations
        image = self.image_transform(image)
        mask = self.mask_transform(mask)

        # Threshold mask to binary (0 or 1)
        mask = (mask > 0.5).float()

        return image, mask, image_name
ðŸ“ model.py â€“ U-Net Architecture
python


1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
âŒ„
âŒ„
âŒ„
âŒ„
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchinfo import summary

class UNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=1):
        super(UNet, self).__init__()

        def CBR(in_channels, out_channels):
            return nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 3, padding=1),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True),
                nn.Conv2d(out_channels, out_channels, 3, padding=1),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True)
            )

        # Encoder
        self.enc1 = CBR(in_channels, 64)
        self.enc2 = CBR(64, 128)
        self.enc3 = CBR(128, 256)
        self.enc4 = CBR(256, 512)
        self.bottleneck = CBR(512, 1024)

        self.pool = nn.MaxPool2d(2)

        # Decoder
        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
        self.dec4 = CBR(1024, 512)
        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.dec3 = CBR(512, 256)
        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec2 = CBR(256, 128)
        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec1 = CBR(128, 64)

        self.final_conv = nn.Conv2d(64, out_channels, 1)

    def forward(self, x):
        # Encoder
        conv1 = self.enc1(x)
        x = self.pool(conv1)
        conv2 = self.enc2(x)
        x = self.pool(conv2)
        conv3 = self.enc3(x)
        x = self.pool(conv3)
        conv4 = self.enc4(x)
        x = self.pool(conv4)

        x = self.bottleneck(x)

        # Decoder
        x = self.upconv4(x)
        x = torch.cat([x, conv4], dim=1)
        x = self.dec4(x)

        x = self.upconv3(x)
        x = torch.cat([x, conv3], dim=1)
        x = self.dec3(x)

        x = self.upconv2(x)
        x = torch.cat([x, conv2], dim=1)
        x = self.dec2(x)

        x = self.upconv1(x)
        x = torch.cat([x, conv1], dim=1)
        x = self.dec1(x)

        return self.final_conv(x)

ðŸ“ train.py â€“ Training, Validation, and Testing Functions

import torch
import torch.nn as nn
from tqdm import tqdm
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt

def dice_coefficient(y_pred, y_true, smooth=1e-6):
    y_pred = torch.sigmoid(y_pred) > 0.5
    intersection = (y_pred * y_true).sum(dim=(1, 2, 3))
    union = y_pred.sum(dim=(1, 2, 3)) + y_true.sum(dim=(1, 2, 3))
    return (2. * intersection + smooth) / (union + smooth)

def train_one_epoch(model, dataloader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    for images, masks, _ in tqdm(dataloader, desc='Training'):
        images, masks = images.to(device), masks.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * images.size(0)
    return running_loss / len(dataloader.dataset)

def validate(model, dataloader, criterion, device):
    model.eval()
    running_loss = 0.0
    dice_scores = []
    with torch.no_grad():
        for images, masks, _ in tqdm(dataloader, desc='Validating'):
            images, masks = images.to(device), masks.to(device)
            outputs = model(images)
            loss = criterion(outputs, masks)
            running_loss += loss.item() * images.size(0)
            dice_scores.extend(dice_coefficient(outputs, masks).cpu().numpy())
    return running_loss / len(dataloader.dataset), dice_scores

def test(model, dataloader, device, save_path):
    model.eval()
    dice_scores = []
    all_images = []
    all_masks = []
    all_preds = []
    all_names = []

    with torch.no_grad():
        for images, masks, names in tqdm(dataloader, desc='Testing'):
            images, masks = images.to(device), masks.to(device)
            outputs = model(images)
            preds = torch.sigmoid(outputs) > 0.5

            dice_scores.extend(dice_coefficient(outputs, masks).cpu().numpy())
            all_images.append(images.cpu())
            all_masks.append(masks.cpu())
            all_preds.append(preds.cpu())
            all_names.extend(names)

    # Concatenate
    all_images = torch.cat(all_images)
    all_masks = torch.cat(all_masks)
    all_preds = torch.cat(all_preds)

    # Visualize predictions
    visualize_predictions(all_images, all_masks, all_preds, all_names, save_path)

    return dice_scores

def visualize_predictions(images, masks, preds, names, save_path):
    idxs = np.random.choice(len(images), size=5, replace=False)
    fig, axes = plt.subplots(5, 3, figsize=(12, 20))

    for i, idx in enumerate(idxs):
        axes[i, 0].imshow(images[idx].squeeze(), cmap='gray')
        axes[i, 0].set_title(f'Input: {names[idx]}')
        axes[i, 0].axis('off')

        axes[i, 1].imshow(masks[idx].squeeze(), cmap='gray')
        axes[i, 1].set_title('Ground Truth')
        axes[i, 1].axis('off')

        axes[i, 2].imshow(preds[idx].squeeze(), cmap='gray')
        axes[i, 2].set_title('Prediction')
        axes[i, 2].axis('off')

    plt.tight_layout()
    plt.savefig(os.path.join(save_path, 'test_predictions.png'))
    plt.close()

def plot_losses(train_losses, val_losses, save_path):
    plt.figure(figsize=(10, 5))
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Training and Validation Loss')
    plt.savefig(os.path.join(save_path, 'loss_curve.png'))
    plt.close()

ðŸ“ main.py â€“ Main Execution Script

import os
import torch
from torch.utils.data import DataLoader
from dataset import GrayscaleBinaryMaskDataset
from model import UNet
from train import train_one_epoch, validate, test, plot_losses
import pandas as pd
import time

if __name__ == '__main__':
    # Hyperparameters
    image_dir = './data/images'
    mask_suffix = '_seg'
    batch_size = 16
    epochs = 20
    lr = 0.001
    img_size = 256
    save_path = './results'
    os.makedirs(save_path, exist_ok=True)

    # Dataset and DataLoader
    dataset = GrayscaleBinaryMaskDataset(image_dir=image_dir, mask_suffix=mask_suffix, img_size=img_size)
    indices = list(range(len(dataset)))

    # Split data
    train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)
    train_idx, val_idx = train_test_split(train_idx, test_size=0.125, random_state=42)  # 10% of total

    train_loader = DataLoader(Subset(dataset, train_idx), batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(Subset(dataset, val_idx), batch_size=batch_size)
    test_loader = DataLoader(Subset(dataset, test_idx), batch_size=batch_size)

    print(f"Train size: {len(train_idx)}, Val size: {len(val_idx)}, Test size: {len(test_idx)}")

    # Model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = UNet().to(device)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    # Model Summary
    from torchinfo import summary
    summary(model, input_size=(1, 1, img_size, img_size))

    # Training Loop
    train_losses = []
    val_losses = []
    val_dice_all = []

    start_time = time.time()

    for epoch in range(epochs):
        print(f"\nEpoch {epoch+1}/{epochs}")
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)
        val_loss, val_dice = validate(model, val_loader, criterion, device)

        train_losses.append(train_loss)
        val_losses.append(val_loss)
        val_dice_all.append(val_dice)

        # Save losses
        pd.DataFrame([list(range(1, epoch+2)), train_losses]).to_excel(os.path.join(save_path, 'train_losses.xlsx'), index=False, header=False)
        pd.DataFrame([list(range(1, epoch+2)), val_losses]).to_excel(os.path.join(save_path, 'val_losses.xlsx'), index=False, header=False)
        pd.DataFrame(val_dice_all).to_excel(os.path.join(save_path, 'validation_dice_scores.xlsx'), index=False)

    total_time = time.time() - start_time
    print(f"\nTraining completed in {total_time:.2f} seconds.")

    # Save model
    torch.save(model.state_dict(), os.path.join(save_path, 'model_state.pth'))
    torch.save(model, os.path.join(save_path, 'model_full.pth'))

    # Plot losses
    plot_losses(train_losses, val_losses, save_path)

    # Test
    test_dice = test(model, test_loader, device, save_path)
    pd.DataFrame([test_dice]).to_excel(os.path.join(save_path, 'test_dice_scores.xlsx'), index=False)

profile
Qwen3-235B-A22B
Thinking completed
Â·
38,912 tokens budget
Okay, the user is encountering a NameError in their Python script. The error message says that 'train_test_split' is not defined. Let me look at the code they provided to figure out why this is happening.

Looking at the main.py file, I see that they import train_test_split from sklearn.model_selection at the top of the file. Wait, no, actually, in the code they pasted, in main.py, the imports are listed as:

import os
import torch
from torch.utils.data import DataLoader
from dataset import GrayscaleBinaryMaskDataset
from model import UNet
from train import train_one_epoch, validate, test, plot_losses
import pandas as pd
import time

But in the dataset.py file, they do import train_test_split. However, in main.py, when they call train_test_split(indices, test_size=0.2, ...), that function isn't imported in main.py. The user must have forgotten to import it there.

Ah, right! In the code provided for main.py, even though the dataset.py uses train_test_split, the main.py script does not have an import statement for it. So when they try to split the data in main.py, Python doesn't recognize the function, leading to the NameError.

So the fix is straightforward: add 'from sklearn.model_selection import train_test_split' to the import statements in main.py. That way, the function is available when they call it.

Let me check the original code again to confirm. Yes, in the main.py code provided, there's no such import. Therefore, adding that line will resolve the error.

Another thing to verify: sometimes, people might have the function imported but maybe misspell it. But in this case, the error is clearly about the name not being defined, so the import is missing.

So the solution is to update the imports in main.py to include train_test_split from sklearn.model_selection.
To resolve the NameError you encountered:



1
NameError: name 'train_test_split' is not defined
you simply need to import the train_test_split function in your main.py file, as it is used directly in the code but currently not imported.

âœ… Fix: Add the Import Statement
Update the import section at the top of your main.py file to include:

python


1
from sklearn.model_selection import train_test_split
Hereâ€™s the corrected import block:

python


1
2
3
4
5
6
7
8
9
import os
import torch
from torch.utils.data import DataLoader
from dataset import GrayscaleBinaryMaskDataset
from model import UNet
from train import train_one_epoch, validate, test, plot_losses
import pandas as pd
import time
from sklearn.model_selection import train_test_split  # ðŸ”¥ Added this line
ðŸ“Œ Why This Happens
Even though train_test_split is used in dataset.py, the function call in main.py requires the function to be explicitly imported in that file too. Python does not automatically share imports between modules.

âœ… Final Note
After applying this fix, your code should run without the NameError. Make sure all required packages are installed:

bash


1
pip install torch torchvision pandas matplotlib scikit-learn tqdm torchinfo
Let me know if you encounter any other issues!

C:\Users\qy44lyfe\.conda\envs\Swallowenv\python.exe "D:/qy44lyfe/LLM segmentation/github repo/LLM_based_Segmentation/Modells 2025/Qwen 3_235B/main.py"
Traceback (most recent call last):
  File "D:\qy44lyfe\LLM segmentation\github repo\LLM_based_Segmentation\Modells 2025\Qwen 3_235B\main.py", line 31, in <module>
    train_loader = DataLoader(Subset(dataset, train_idx), batch_size=batch_size, shuffle=True)
NameError: name 'Subset' is not defined

Process finished with exit code 1

profile
Qwen3-235B-A22B
5:23 pm
Thinking completed
Â·
38,912 tokens budget
Okay, the user is getting a NameError about 'Subset' not being defined. Let me check where that's coming from.

In the main.py file, they're using DataLoader with Subset from torch.utils.data. But they might not have imported Subset in main.py.

Right, the Dataset class in dataset.py imports Subset, but in main.py, when creating the DataLoaders, they're using Subset directly. So they need to import it in main.py as well.

So the solution is to add 'from torch.utils.data import Subset' to the imports in main.py. That should fix the NameError. Let me make sure all the necessary imports are included there now.
To resolve the NameError: name 'Subset' is not defined you encountered in your main.py, you need to import the Subset class from torch.utils.data, as it is used directly when creating subsets of your dataset for training, validation, and testing.

âœ… Fix: Add the Missing Import
Update the import section at the top of your main.py file to include:
from torch.utils.data import Subset

profile
Qwen3-235B-A22B
5:23 pm
Thinking completed
Â·
38,912 tokens budget
Okay, the user is getting a NameError about 'Subset' not being defined. Let me check where that's coming from.

In the main.py file, they're using DataLoader with Subset from torch.utils.data. But they might not have imported Subset in main.py.

Right, the Dataset class in dataset.py imports Subset, but in main.py, when creating the DataLoaders, they're using Subset directly. So they need to import it in main.py as well.

So the solution is to add 'from torch.utils.data import Subset' to the imports in main.py. That should fix the NameError. Let me make sure all the necessary imports are included there now.
To resolve the NameError: name 'Subset' is not defined you encountered in your main.py, you need to import the Subset class from torch.utils.data, as it is used directly when creating subsets of your dataset for training, validation, and testing.

âœ… Fix: Add the Missing Import
Update the import section at the top of your main.py file to include:

from torch.utils.data import Subset
Hereâ€™s the corrected import block:
import os
import torch
from torch.utils.data import DataLoader, Subset  # ðŸ”¥ Added 'Subset' here
from dataset import GrayscaleBinaryMaskDataset
from model import UNet
from train import train_one_epoch, validate, test, plot_losses
import pandas as pd
import time
from sklearn.model_selection import train_test_split

ðŸ“Œ Why This Happens
The Subset class is used to create subsets of your full dataset (e.g., train/val/test splits).
Even though Subset may be used in other files (like dataset.py), you must explicitly import it wherever it's used â€” including in main.py.
âœ… Final Note
After applying this fix, your code should run without this NameError. Ensure all required packages are installed:

bash

1
pip install torch torchvision pandas matplotlib scikit-learn tqdm torchinfo
Let me know if you hit any other issues!

Traceback (most recent call last):
  File "D:\qy44lyfe\LLM segmentation\github repo\LLM_based_Segmentation\Modells 2025\Qwen 3_235B\main.py", line 41, in <module>
    criterion = nn.BCEWithLogitsLoss()
NameError: name 'nn' is not defined

Process finished with exit code 1

profile
Qwen3-235B-A22B
5:27 pm
Thinking completed
Â·
38,912 tokens budget
Okay, the user is encountering another NameError in their main.py file. The error message says that 'nn' is not defined. Let me look at the code they provided earlier.

In the main.py, they have the line:
criterion = nn.BCEWithLogitsLoss()

But the error indicates that 'nn' isn't imported. Looking at their imports in main.py, I see that they import torch as 'torch' and from torch.utils.data import DataLoader and Subset. However, they haven't imported torch.nn as 'nn'.

So, the fix here is straightforward. They need to import torch.nn as 'nn' in the main.py file. The line should be:
import torch.nn as nn

Wait, but in the code they provided earlier, in the main.py, the imports start with:
import os
import torch
from torch.utils.data import DataLoader, Subset
...

They imported torch but didn't assign 'nn' as an alias. So when they use 'nn.BCEWithLogitsLoss()', Python doesn't recognize 'nn' because it wasn't imported.

The solution is to add 'import torch.nn as nn' to the imports in main.py. That way, 'nn' becomes a reference to torch.nn, and the code can access BCEWithLogitsLoss correctly.

I should explain that they need to add this import statement to resolve the NameError. Also, remind them that even though torch is already imported, the submodules like nn aren't automatically imported unless specified.

Another thing to check: in the model.py, they have 'import torch.nn as nn', which is correct there. But in main.py, they're using nn for the loss function, so the same import is needed there as well.

Yes, that's the issue. Adding 'import torch.nn as nn' in main.py will define 'nn', allowing the code to proceed without that NameError.
To resolve the NameError: name 'nn' is not defined you're seeing in your main.py, you need to import torch.nn as nn â€” because you're using it to define the loss function:

python


1
criterion = nn.BCEWithLogitsLoss()
âœ… Fix: Add the Missing Import
Update the import section at the top of your main.py file to include:

python


import torch.nn as nn
Hereâ€™s the corrected import block:

python

import os
import torch
import torch.nn as nn  # ðŸ”¥ Added this line
from torch.utils.data import DataLoader, Subset
from dataset import GrayscaleBinaryMaskDataset
from model import UNet
from train import train_one_epoch, validate, test, plot_losses
import pandas as pd
import time
from sklearn.model_selection import train_test_split
ðŸ“Œ Why This Happens
You imported torch, but did not import its submodules like torch.nn.
The nn module contains useful components like loss functions (BCEWithLogitsLoss), layers, etc., and must be explicitly imported.
âœ… Final Note
After adding this import, your code should proceed without this NameError.

Make sure all required packages are installed:

bash
1
pip install torch torchvision pandas matplotlib scikit-learn tqdm torchinfo
Let me know if you hit any more issues!



