Skip to content
Chat history

You said:
ï¶	in separate scrips below, provide a complete ready-to-run Python implementation for a U-Net architecture using PyTorch, for binary segmentation of grayscale images. The dataset should consist of grayscale images and corresponding binary masks, which are located in 2 different folders, OR in the same path (the masks have the same name as the images, or with a suffix, like: "number.png" and the masks "number_seg.png", around 5000 samples in total. access and split directly using train_test_split on the index list and Subset, and do NOT duplicate or copy to new directory). It should work and provide correct predictions, and save the losses and dice scores as instructed below. The code should be structured into importable scripts with the following:

1.	A custom Dataset class for loading the grayscale images and corresponding binary masks. (only the png files, ignore meta files or additional files in the directory)
2.	A UNet model class definition using PyTorch, optimized for binary segmentation (with small foreground, mostly background)
3.	Functions for the training procedure, validation procedure, and Testing procedure.

Considerations for each part:
4.	Ensure needed transforms on input image and mask pairs (Re-size images correctly, ensure gray scale and binaryâ€¦)
5.	Avoid overfitting and data leakage (without using augmentations), choose appropriate hyperparameters, learning rate, batch size, etc

â€¢	For Training and Validation:

6.	Compute training and validation losses.
7.	Track and save the average training loss for each epoch in a separate Excel file (train_losses.xlsx) in a given save path. The first row should contain the epoch numbers, and the second row should contain the corresponding average training loss for each epoch.
8.	Similarly, save the validation loss for each epoch in an Excel file (val_losses.xlsx) with the same structure.
9.	At the end of training, save both the entire model and the model's state dictionary (.pth files) in the save path
10.	Calculate total training time from start to end, and print at the end of training
11.	The progress, phase (train/validation), and all info should be shown epoch by epoch using tqdm progress bars during training, validation, and testing.
12.	A function to visualize training losses and validation losses over epochs afterwards, on the same plot. Save plot in a specified save path as png. 
13.	During Validation: mean Dice scores should be calculated over each batch, for each epoch, and stored in one Excel file (epochs as rows and batches as columns). The Excel file (named â€œvalidation_dice_scoresâ€ should be saved at the specified save path after validation is done.
14.	Do the same for the Testing and store dice the Dice scores the same way and save the excel file as â€œtest_dice_scoresâ€ after testing is done.

â€¢	For Testing:

15.	A function to visualize the predicted masks, the input image, and the ground truth, for 5 random samples, after testing is done. (subplots for 5 random samples in one plot, in 5 rows and 3 columns (title at top: Input Image, Ground Truth, Prediction). Also display image file name string above each image and mask. save the plot as .png).

â€¢	For Main:

16.	Shuffle the data, split the dataset into: 80% training, 10% validation, and 10% test sets using train_test_split. Print the sizes of each (how many images), and copy the splitted data (images and their respective masks) 
17.	DataLoader setup for training, validation, and testing sets.
18.	Ensure correct data loading, print data loader sizes.
19.	Also usig torchinfo, print model summary, and total number of model parameters (n learnable params)

20.	All the above functionalities should be implemented in a modular way so that they can be imported as scripts or classes.
21.	All visualizations should just be saved as png. No need to display the plots during running.
22.	Ensure that everything can be executed in an if __name__ == "__main__": block with instructions for loading the dataset, configuring hyperparameters, and saving the trained model.
The directory structure should only be:
unet_segmentation/
â”œâ”€â”€ dataset.py
â”œâ”€â”€ model.py
â”œâ”€â”€ train.py
â”œâ”€â”€ main.py
ChatGPT said:
Here is a complete modular implementation of a U-Net pipeline in PyTorch for binary segmentation of grayscale images using your specified structure and requirements. The implementation includes:

Proper train/val/test splitting using train_test_split on indices

Excel logging for losses and Dice scores

PNG visualizations

tqdm progress bars

No data duplication

torchinfo model summary

âœ… Folder Structure
unet_segmentation/
â”œâ”€â”€ dataset.py
â”œâ”€â”€ model.py
â”œâ”€â”€ train.py
â”œâ”€â”€ main.py
1. dataset.py â€“ Dataset class for loading grayscale images and binary masks
# unet_segmentation/dataset.py

import os
from PIL import Image
from torch.utils.data import Dataset
import torchvision.transforms as transforms


class SegmentationDataset(Dataset):
    def __init__(self, image_dir, mask_dir=None, same_folder=False, suffix="_seg", transform=None):
        self.image_dir = image_dir
        self.mask_dir = mask_dir
        self.same_folder = same_folder
        self.suffix = suffix
        self.transform = transform
        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(".png") and suffix not in f]

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        img_path = os.path.join(self.image_dir, img_name)

        # Load image (grayscale)
        image = Image.open(img_path).convert("L")

        # Load mask
        if self.same_folder:
            mask_name = img_name.replace(".png", f"{self.suffix}.png")
            mask_path = os.path.join(self.image_dir, mask_name)
        else:
            mask_name = img_name
            mask_path = os.path.join(self.mask_dir, mask_name)

        mask = Image.open(mask_path).convert("L")

        # Resize and ToTensor
        resize = transforms.Resize((256, 256))
        to_tensor = transforms.ToTensor()
        image = to_tensor(resize(image))
        mask = to_tensor(resize(mask))

        # Binarize mask
        mask = (mask > 0.5).float()

        return image, mask, img_name
2. model.py â€“ U-Net model for binary segmentation
# unet_segmentation/model.py

import torch
import torch.nn as nn


class UNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=1, features=[64, 128, 256, 512]):
        super(UNet, self).__init__()

        self.encoder = nn.ModuleList()
        self.decoder = nn.ModuleList()

        # Downsampling
        for feature in features:
            self.encoder.append(self.double_conv(in_channels, feature))
            in_channels = feature

        # Upsampling
        for feature in reversed(features):
            self.decoder.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2))
            self.decoder.append(self.double_conv(feature*2, feature))

        self.bottleneck = self.double_conv(features[-1], features[-1]*2)
        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)

    def double_conv(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        skip_connections = []
        for down in self.encoder:
            x = down(x)
            skip_connections.append(x)
            x = nn.MaxPool2d(kernel_size=2)(x)

        x = self.bottleneck(x)
        skip_connections = skip_connections[::-1]

        for idx in range(0, len(self.decoder), 2):
            x = self.decoder[idx](x)
            skip_connection = skip_connections[idx//2]

            if x.shape != skip_connection.shape:
                x = torch.nn.functional.interpolate(x, size=skip_connection.shape[2:])

            x = torch.cat((skip_connection, x), dim=1)
            x = self.decoder[idx+1](x)

        return torch.sigmoid(self.final_conv(x))
3. train.py â€“ Training, validation, testing, and utilities
# unet_segmentation/train.py

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from tqdm import tqdm
import time
import random
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import os


def dice_score(pred, target, threshold=0.5):
    pred = (pred > threshold).float()
    smooth = 1e-6
    intersection = (pred * target).sum(dim=(1,2,3))
    union = pred.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))
    dice = (2 * intersection + smooth) / (union + smooth)
    return dice.mean().item()


def save_losses(losses, filename, save_path):
    df = pd.DataFrame([list(range(1, len(losses)+1)), losses])
    df.to_excel(os.path.join(save_path, filename), header=False, index=False)


def save_dice_scores(dice_scores, filename, save_path):
    df = pd.DataFrame(dice_scores)
    df.to_excel(os.path.join(save_path, filename), index=False)


def plot_losses(train_losses, val_losses, save_path):
    plt.figure()
    plt.plot(train_losses, label="Train Loss")
    plt.plot(val_losses, label="Val Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.savefig(os.path.join(save_path, "loss_curve.png"))
    plt.close()


def visualize_predictions(model, dataloader, save_path, device):
    model.eval()
    samples = random.sample(range(len(dataloader.dataset)), 5)
    inputs, masks, names = zip(*[dataloader.dataset[i] for i in samples])

    fig, axs = plt.subplots(5, 3, figsize=(12, 20))
    axs[0][0].set_title("Input Image")
    axs[0][1].set_title("Ground Truth")
    axs[0][2].set_title("Prediction")

    with torch.no_grad():
        for i in range(5):
            image = inputs[i].unsqueeze(0).to(device)
            output = model(image).squeeze().cpu().numpy()
            axs[i][0].imshow(inputs[i].squeeze(), cmap="gray")
            axs[i][0].set_ylabel(names[i], fontsize=8)
            axs[i][1].imshow(masks[i].squeeze(), cmap="gray")
            axs[i][2].imshow(output > 0.5, cmap="gray")

    for ax in axs.flat:
        ax.axis("off")

    plt.tight_layout()
    plt.savefig(os.path.join(save_path, "prediction_visualization.png"))
    plt.close()


def train_one_epoch(model, loader, optimizer, criterion, device):
    model.train()
    running_loss = 0
    loop = tqdm(loader, desc="Training", leave=False)
    for inputs, masks, _ in loop:
        inputs, masks = inputs.to(device), masks.to(device)
        preds = model(inputs)
        loss = criterion(preds, masks)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * inputs.size(0)
    return running_loss / len(loader.dataset)


def validate(model, loader, criterion, device, save_path, epoch):
    model.eval()
    val_loss = 0
    dice_scores = []

    loop = tqdm(loader, desc="Validation", leave=False)
    for inputs, masks, _ in loop:
        inputs, masks = inputs.to(device), masks.to(device)
        with torch.no_grad():
            preds = model(inputs)
            loss = criterion(preds, masks)
            val_loss += loss.item() * inputs.size(0)
            batch_dice = dice_score(preds, masks)
            dice_scores.append(batch_dice)

    return val_loss / len(loader.dataset), dice_scores


def test(model, loader, device, save_path):
    model.eval()
    dice_scores = []
    loop = tqdm(loader, desc="Testing")
    for inputs, masks, _ in loop:
        inputs, masks = inputs.to(device), masks.to(device)
        with torch.no_grad():
            preds = model(inputs)
            batch_dice = dice_score(preds, masks)
            dice_scores.append(batch_dice)
    return dice_scores
4. main.py â€“ Entry point and configuration
# unet_segmentation/main.py

import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import train_test_split
from torchinfo import summary
from dataset import SegmentationDataset
from model import UNet
from train import *

if __name__ == "__main__":
    # Paths
    image_dir = "path/to/images"
    mask_dir = "path/to/masks"  # or use same_folder=True
    save_path = "results/"
    os.makedirs(save_path, exist_ok=True)

    # Hyperparameters
    batch_size = 8
    lr = 1e-4
    num_epochs = 20
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Dataset
    dataset = SegmentationDataset(image_dir=image_dir, mask_dir=mask_dir, same_folder=False)
    indices = list(range(len(dataset)))
    train_idx, temp_idx = train_test_split(indices, test_size=0.2, random_state=42, shuffle=True)
    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)

    train_set = Subset(dataset, train_idx)
    val_set = Subset(dataset, val_idx)
    test_set = Subset(dataset, test_idx)

    print(f"Train: {len(train_set)}, Val: {len(val_set)}, Test: {len(test_set)}")

    # Dataloaders
    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)

    print(f"Train Loader: {len(train_loader)}, Val Loader: {len(val_loader)}, Test Loader: {len(test_loader)}")

    # Model
    model = UNet().to(device)
    summary(model, input_size=(1, 1, 256, 256))
    print(f"Total parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}")

    # Optimizer and Loss
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.BCELoss()

    # Training Loop
    train_losses, val_losses, val_dices = [], [], []
    start_time = time.time()

    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch+1}/{num_epochs}")
        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)
        val_loss, val_dice = validate(model, val_loader, criterion, device, save_path, epoch)

        train_losses.append(train_loss)
        val_losses.append(val_loss)
        val_dices.append(val_dice)

    end_time = time.time()
    print(f"\nTraining completed in {(end_time - start_time):.2f} seconds")

    # Save results
    save_losses(train_losses, "train_losses.xlsx", save_path)
    save_losses(val_losses, "val_losses.xlsx", save_path)
    save_dice_scores(val_dices, "validation_dice_scores.xlsx", save_path)
    plot_losses(train_losses, val_losses, save_path)
    torch.save(model, os.path.join(save_path, "unet_model.pth"))
    torch.save(model.state_dict(), os.path.join(save_path, "unet_state_dict.pth"))

    # Testing
    test_dices = test(model, test_loader, device, save_path)
    save_dice_scores(test_dices, "test_dice_scores.xlsx", save_path)

    # Visualize Predictions
    visualize_predictions(model, test_loader, save_path, device)
Let me know if you want this zipped, or also extended with requirements.txt or additional enhancements like learning rate schedulers, metrics, or augmentations.

You said:
it is not taking the masks correctly: C:\Users\qy44lyfe\.conda\envs\Swallowenv\python.exe "D:/qy44lyfe/LLM segmentation/github repo/LLM_based_Segmentation/run_to_run variability test/GPT 4o-9/main.py"
Train: 799, Val: 100, Test: 100
Train Loader: 100, Val Loader: 13, Test Loader: 13
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
UNet                                     [1, 1, 256, 256]          --
â”œâ”€ModuleList: 1-1                        --                        --
â”‚    â””â”€Sequential: 2-1                   [1, 64, 256, 256]         --
â”‚    â”‚    â””â”€Conv2d: 3-1                  [1, 64, 256, 256]         640
â”‚    â”‚    â””â”€ReLU: 3-2                    [1, 64, 256, 256]         --
â”‚    â”‚    â””â”€Conv2d: 3-3                  [1, 64, 256, 256]         36,928
â”‚    â”‚    â””â”€ReLU: 3-4                    [1, 64, 256, 256]         --
â”‚    â””â”€Sequential: 2-2                   [1, 128, 128, 128]        --
â”‚    â”‚    â””â”€Conv2d: 3-5                  [1, 128, 128, 128]        73,856
â”‚    â”‚    â””â”€ReLU: 3-6                    [1, 128, 128, 128]        --
â”‚    â”‚    â””â”€Conv2d: 3-7                  [1, 128, 128, 128]        147,584
â”‚    â”‚    â””â”€ReLU: 3-8                    [1, 128, 128, 128]        --
â”‚    â””â”€Sequential: 2-3                   [1, 256, 64, 64]          --
â”‚    â”‚    â””â”€Conv2d: 3-9                  [1, 256, 64, 64]          295,168
â”‚    â”‚    â””â”€ReLU: 3-10                   [1, 256, 64, 64]          --
â”‚    â”‚    â””â”€Conv2d: 3-11                 [1, 256, 64, 64]          590,080
â”‚    â”‚    â””â”€ReLU: 3-12                   [1, 256, 64, 64]          --
â”‚    â””â”€Sequential: 2-4                   [1, 512, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-13                 [1, 512, 32, 32]          1,180,160
â”‚    â”‚    â””â”€ReLU: 3-14                   [1, 512, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-15                 [1, 512, 32, 32]          2,359,808
â”‚    â”‚    â””â”€ReLU: 3-16                   [1, 512, 32, 32]          --
â”œâ”€Sequential: 1-2                        [1, 1024, 16, 16]         --
â”‚    â””â”€Conv2d: 2-5                       [1, 1024, 16, 16]         4,719,616
â”‚    â””â”€ReLU: 2-6                         [1, 1024, 16, 16]         --
â”‚    â””â”€Conv2d: 2-7                       [1, 1024, 16, 16]         9,438,208
â”‚    â””â”€ReLU: 2-8                         [1, 1024, 16, 16]         --
â”œâ”€ModuleList: 1-3                        --                        --
â”‚    â””â”€ConvTranspose2d: 2-9              [1, 512, 32, 32]          2,097,664
â”‚    â””â”€Sequential: 2-10                  [1, 512, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-17                 [1, 512, 32, 32]          4,719,104
â”‚    â”‚    â””â”€ReLU: 3-18                   [1, 512, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-19                 [1, 512, 32, 32]          2,359,808
â”‚    â”‚    â””â”€ReLU: 3-20                   [1, 512, 32, 32]          --
â”‚    â””â”€ConvTranspose2d: 2-11             [1, 256, 64, 64]          524,544
â”‚    â””â”€Sequential: 2-12                  [1, 256, 64, 64]          --
â”‚    â”‚    â””â”€Conv2d: 3-21                 [1, 256, 64, 64]          1,179,904
â”‚    â”‚    â””â”€ReLU: 3-22                   [1, 256, 64, 64]          --
â”‚    â”‚    â””â”€Conv2d: 3-23                 [1, 256, 64, 64]          590,080
â”‚    â”‚    â””â”€ReLU: 3-24                   [1, 256, 64, 64]          --
â”‚    â””â”€ConvTranspose2d: 2-13             [1, 128, 128, 128]        131,200
â”‚    â””â”€Sequential: 2-14                  [1, 128, 128, 128]        --
â”‚    â”‚    â””â”€Conv2d: 3-25                 [1, 128, 128, 128]        295,040
â”‚    â”‚    â””â”€ReLU: 3-26                   [1, 128, 128, 128]        --
â”‚    â”‚    â””â”€Conv2d: 3-27                 [1, 128, 128, 128]        147,584
â”‚    â”‚    â””â”€ReLU: 3-28                   [1, 128, 128, 128]        --
â”‚    â””â”€ConvTranspose2d: 2-15             [1, 64, 256, 256]         32,832
â”‚    â””â”€Sequential: 2-16                  [1, 64, 256, 256]         --
â”‚    â”‚    â””â”€Conv2d: 3-29                 [1, 64, 256, 256]         73,792
â”‚    â”‚    â””â”€ReLU: 3-30                   [1, 64, 256, 256]         --
â”‚    â”‚    â””â”€Conv2d: 3-31                 [1, 64, 256, 256]         36,928
â”‚    â”‚    â””â”€ReLU: 3-32                   [1, 64, 256, 256]         --
â”œâ”€Conv2d: 1-4                            [1, 1, 256, 256]          65
==========================================================================================
Total params: 31,030,593
Trainable params: 31,030,593
Non-trainable params: 0
Total mult-adds (G): 54.57
==========================================================================================
Input size (MB): 0.26
Forward/backward pass size (MB): 319.29
Params size (MB): 124.12
Estimated Total Size (MB): 443.68
==========================================================================================
Total parameters: 31030593

Epoch 1/2
Traceback (most recent call last):
  File "D:\qy44lyfe\LLM segmentation\github repo\LLM_based_Segmentation\run_to_run variability test\GPT 4o-9\main.py", line 60, in <module>
    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)
  File "D:\qy44lyfe\LLM segmentation\github repo\LLM_based_Segmentation\run_to_run variability test\GPT 4o-9\train.py", line 76, in train_one_epoch
    for inputs, masks, _ in loop:
  File "C:\Users\qy44lyfe\.conda\envs\Swallowenv\lib\site-packages\tqdm\std.py", line 1182, in __iter__
    for obj in iterable:
  File "C:\Users\qy44lyfe\.conda\envs\Swallowenv\lib\site-packages\torch\utils\data\dataloader.py", line 631, in __next__
    data = self._next_data()
  File "C:\Users\qy44lyfe\.conda\envs\Swallowenv\lib\site-packages\torch\utils\data\dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "C:\Users\qy44lyfe\.conda\envs\Swallowenv\lib\site-packages\torch\utils\data\_utils\fetch.py", line 49, in fetch
    data = self.dataset.__getitems__(possibly_batched_index)
  File "C:\Users\qy44lyfe\.conda\envs\Swallowenv\lib\site-packages\torch\utils\data\dataset.py", line 399, in __getitems__
    return [self.dataset[self.indices[idx]] for idx in indices]
  File "C:\Users\qy44lyfe\.conda\envs\Swallowenv\lib\site-packages\torch\utils\data\dataset.py", line 399, in <listcomp>
    return [self.dataset[self.indices[idx]] for idx in indices]
  File "D:\qy44lyfe\LLM segmentation\github repo\LLM_based_Segmentation\run_to_run variability test\GPT 4o-9\dataset.py", line 36, in __getitem__
    mask = Image.open(mask_path).convert("L")
  File "C:\Users\qy44lyfe\.conda\envs\Swallowenv\lib\site-packages\PIL\Image.py", line 3131, in open
    fp = builtins.open(filename, "rb")
FileNotFoundError: [Errno 2] No such file or directory: 'D:\\qy44lyfe\\LLM segmentation\\Data sets\\Brain Meningioma\\Masks\\Tr-me_1141.jpg'

Process finished with exit code 1
ChatGPT said:
Thanks for the detailed error message! Let's go through and fix the issue step by step.

â— Problem Summary
Your dataset has images and masks in separate folders, and the mask path being accessed in the error is:

'D:\\qy44lyfe\\LLM segmentation\\Data sets\\Brain Meningioma\\Masks\\Tr-me_1141.jpg'
ðŸ”Ž What's wrong?
Your code is expecting masks to have the same filename as the image, but you might have:

Image: Tr-me_1141.jpg or .png

Mask: Tr-me_1141_seg.png or Tr-me_1141_mask.png or something similar.

But your current dataset class is just using the same filename for image and mask.

âœ… Fix
We need to update the __getitem__ method in dataset.py to:

Ensure only .png files are loaded (you may have .jpg â€” fix this).

Add support for mask suffixes like _seg, _mask, etc.

Warn or raise an error if a corresponding mask is not found (instead of crashing later).

âœ… Updated dataset.py
Replace your __getitem__ method with the following more robust version:

# unet_segmentation/dataset.py

import os
from PIL import Image
from torch.utils.data import Dataset
import torchvision.transforms as transforms


class SegmentationDataset(Dataset):
    def __init__(self, image_dir, mask_dir=None, same_folder=False, suffix="_seg", transform=None):
        self.image_dir = image_dir
        self.mask_dir = mask_dir if mask_dir is not None else image_dir
        self.same_folder = same_folder
        self.suffix = suffix
        self.transform = transform

        # Only accept image files
        valid_ext = [".png", ".jpg", ".jpeg"]
        self.image_files = [f for f in os.listdir(image_dir) if os.path.splitext(f)[1].lower() in valid_ext and suffix not in f]

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        img_path = os.path.join(self.image_dir, img_name)

        # Load image
        image = Image.open(img_path).convert("L")

        # Build mask name
        base_name, ext = os.path.splitext(img_name)
        mask_name = f"{base_name}{self.suffix}{ext}"
        mask_path = os.path.join(self.mask_dir, mask_name)

        if not os.path.exists(mask_path):
            raise FileNotFoundError(f"Mask file not found: {mask_path}")

        # Load mask
        mask = Image.open(mask_path).convert("L")

        # Resize and ToTensor
        resize = transforms.Resize((256, 256))
        to_tensor = transforms.ToTensor()
        image = to_tensor(resize(image))
        mask = to_tensor(resize(mask))

        # Binarize mask
        mask = (mask > 0.5).float()

        return image, mask, img_name
ðŸ›  How to Use It
In main.py, pass the correct suffix (e.g. _seg, _mask) if your mask filenames differ.

Example
If your image is:

Images/
â””â”€â”€ Tr-me_1141.png
And mask is:

Masks/
â””â”€â”€ Tr-me_1141_mask.png
Then in main.py:

dataset = SegmentationDataset(
    image_dir=image_dir,
    mask_dir=mask_dir,
    same_folder=False,
    suffix="_mask"
)
ðŸ”„ Optional: Print file paths for debugging
To confirm things are loading correctly before training, you can add:

for i in range(3):
    img, mask, name = dataset[i]
    print(f"Loaded: {name}")
âœ… Result
This fix ensures:

Correct mask matching by suffix

Support for .jpg, .jpeg, .png

Mask mismatch throws a clear error

Let me know your exact image/mask filename pattern if you'd like me to automatically detect it instead!



No file chosenNo file chosen
ChatGPT can make mistakes. Check important info. See Cookie Preferences.
