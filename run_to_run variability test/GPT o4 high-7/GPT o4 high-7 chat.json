Skip to content
Chat history

You said:
	in separate scrips below, provide a complete ready-to-run Python implementation for a U-Net architecture using PyTorch, for binary segmentation of grayscale images. The dataset should consist of grayscale images and corresponding binary masks, which are located in 2 different folders, OR in the same path (the masks have the same name as the images, or with a suffix, like: "number.png" and the masks "number_seg.png", around 5000 samples in total. access and split directly using train_test_split on the index list and Subset, and do NOT duplicate or copy to new directory). It should work and provide correct predictions, and save the losses and dice scores as instructed below. The code should be structured into importable scripts with the following:

1.	A custom Dataset class for loading the grayscale images and corresponding binary masks. (only the png files, ignore meta files or additional files in the directory)
2.	A UNet model class definition using PyTorch, optimized for binary segmentation (with small foreground, mostly background)
3.	Functions for the training procedure, validation procedure, and Testing procedure.

Considerations for each part:
4.	Ensure needed transforms on input image and mask pairs (Re-size images correctly, ensure gray scale and binary…)
5.	Avoid overfitting and data leakage (without using augmentations), choose appropriate hyperparameters, learning rate, batch size, etc

•	For Training and Validation:

6.	Compute training and validation losses.
7.	Track and save the average training loss for each epoch in a separate Excel file (train_losses.xlsx) in a given save path. The first row should contain the epoch numbers, and the second row should contain the corresponding average training loss for each epoch.
8.	Similarly, save the validation loss for each epoch in an Excel file (val_losses.xlsx) with the same structure.
9.	At the end of training, save both the entire model and the model's state dictionary (.pth files) in the save path
10.	Calculate total training time from start to end, and print at the end of training
11.	The progress, phase (train/validation), and all info should be shown epoch by epoch using tqdm progress bars during training, validation, and testing.
12.	A function to visualize training losses and validation losses over epochs afterwards, on the same plot. Save plot in a specified save path as png. 
13.	During Validation: mean Dice scores should be calculated over each batch, for each epoch, and stored in one Excel file (epochs as rows and batches as columns). The Excel file (named “validation_dice_scores” should be saved at the specified save path after validation is done.
14.	Do the same for the Testing and store dice the Dice scores the same way and save the excel file as “test_dice_scores” after testing is done.

•	For Testing:

15.	A function to visualize the predicted masks, the input image, and the ground truth, for 5 random samples, after testing is done. (subplots for 5 random samples in one plot, in 5 rows and 3 columns (title at top: Input Image, Ground Truth, Prediction). Also display image file name string above each image and mask. save the plot as .png).

•	For Main:

16.	Shuffle the data, split the dataset into: 80% training, 10% validation, and 10% test sets using train_test_split. Print the sizes of each (how many images), and copy the splitted data (images and their respective masks) 
17.	DataLoader setup for training, validation, and testing sets.
18.	Ensure correct data loading, print data loader sizes.
19.	Also usig torchinfo, print model summary, and total number of model parameters (n learnable params)

20.	All the above functionalities should be implemented in a modular way so that they can be imported as scripts or classes.
21.	All visualizations should just be saved as png. No need to display the plots during running.
22.	Ensure that everything can be executed in an if __name__ == "__main__": block with instructions for loading the dataset, configuring hyperparameters, and saving the trained model.
The directory structure should only be:
unet_segmentation/
├── dataset.py
├── model.py
├── train.py
├── main.py
ChatGPT said:
Below is a fully modular U-Net segmentation pipeline in four scripts. Just drop them into a directory unet_segmentation/, install the requirements (torch torchvision pandas matplotlib pillow scikit-learn tqdm torchinfo openpyxl), and run python main.py.

dataset.py

import os
from PIL import Image
from torch.utils.data import Dataset
from torchvision import transforms

class GrayscaleSegmentationDataset(Dataset):
    """
    Loads grayscale images and their binary masks from a single folder or separate
    folders. Masks must end in '_seg.png' (or same name in a mask_folder).
    Ignores any non-png files.
    """
    def __init__(self, image_folder, mask_folder=None, mask_suffix='_seg.png', transform=None):
        """
        image_folder: path to images
        mask_folder: optional separate path to masks; if None, masks in image_folder
        mask_suffix: if masks are named like '001_seg.png'
        transform: torchvision.transforms to apply to both image & mask
        """
        self.image_folder  = image_folder
        self.mask_folder   = mask_folder or image_folder
        self.mask_suffix   = mask_suffix
        self.transform     = transform

        # collect all png images
        self.image_paths = sorted([
            os.path.join(image_folder, f)
            for f in os.listdir(image_folder)
            if f.lower().endswith('.png') and not f.lower().endswith(mask_suffix)
        ])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        base      = os.path.splitext(os.path.basename(img_path))[0]
        mask_name = base + self.mask_suffix
        mask_path = os.path.join(self.mask_folder, mask_name)

        # open
        image = Image.open(img_path).convert('L')
        mask  = Image.open(mask_path).convert('L')  # still one channel

        if self.transform:
            # apply same transforms to image and mask
            image, mask = self.transform(image, mask)

        return image, mask, os.path.basename(img_path)
model.py

import torch
import torch.nn as nn
import torch.nn.functional as F

class DoubleConv(nn.Module):
    """(conv => BN => ReLU) * 2"""
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(in_ch,  out_ch, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
        )
    def forward(self, x):
        return self.net(x)

class Down(nn.Module):
    """Downscaling with maxpool then double conv"""
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.net = nn.Sequential(
            nn.MaxPool2d(2),
            DoubleConv(in_ch, out_ch)
        )
    def forward(self, x):
        return self.net(x)

class Up(nn.Module):
    """Upscaling then double conv"""
    def __init__(self, in_ch, out_ch, bilinear=True):
        super().__init__()
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        else:
            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, kernel_size=2, stride=2)
        self.conv = DoubleConv(in_ch, out_ch)

    def forward(self, x1, x2):
        x1 = self.up(x1)
        # pad if odd sizes
        diffY = x2.size()[2] - x1.size()[2]
        diffX = x2.size()[3] - x1.size()[3]
        x1 = F.pad(x1, [diffX//2, diffX - diffX//2,
                        diffY//2, diffY - diffY//2])
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)

class OutConv(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=1)

    def forward(self, x):
        return self.conv(x)

class UNet(nn.Module):
    def __init__(self, n_channels=1, n_classes=1, bilinear=True):
        super().__init__()
        self.n_channels = n_channels
        self.n_classes  = n_classes
        self.bilinear   = bilinear

        self.inc   = DoubleConv(n_channels, 64)
        self.down1 = Down(64, 128)
        self.down2 = Down(128, 256)
        self.down3 = Down(256, 512)
        factor      = 2 if bilinear else 1
        self.down4 = Down(512, 1024 // factor)
        self.up1   = Up(1024, 512 // factor, bilinear)
        self.up2   = Up(512, 256 // factor, bilinear)
        self.up3   = Up(256, 128 // factor, bilinear)
        self.up4   = Up(128, 64, bilinear)
        self.outc  = OutConv(64, n_classes)

    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.down1(x1)
        x3 = self.down2(x2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)
        x  = self.up1(x5, x4)
        x  = self.up2(x, x3)
        x  = self.up3(x, x2)
        x  = self.up4(x, x1)
        logits = self.outc(x)
        return logits
train.py

import time
import os
import torch
import torch.nn.functional as F
from tqdm import tqdm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def dice_coeff_batch(preds, targets, smooth=1e-6):
    """
    preds: (B,1,H,W) raw logits
    targets: (B,1,H,W) binary masks {0,1}
    returns: list of dice per sample
    """
    preds = torch.sigmoid(preds)
    preds = (preds > 0.5).float()
    dice_scores = []
    for p, t in zip(preds, targets):
        p_flat = p.view(-1)
        t_flat = t.view(-1)
        intersection = (p_flat * t_flat).sum()
        dice = (2.*intersection + smooth) / (p_flat.sum() + t_flat.sum() + smooth)
        dice_scores.append(dice.item())
    return dice_scores

def train_epoch(model, loader, optimizer, device):
    model.train()
    running_loss = 0.0
    for images, masks, _ in tqdm(loader, desc='Train', leave=False):
        images = images.to(device)
        masks  = masks.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = F.binary_cross_entropy_with_logits(outputs, masks)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * images.size(0)
    avg_loss = running_loss / len(loader.dataset)
    return avg_loss

def validate_epoch(model, loader, device, save_path, epoch):
    model.eval()
    running_loss = 0.0
    all_dices = []
    batch_idx = []
    with torch.no_grad():
        for i, (images, masks, _) in enumerate(tqdm(loader, desc='Val', leave=False)):
            images = images.to(device)
            masks  = masks.to(device)
            outputs = model(images)
            loss = F.binary_cross_entropy_with_logits(outputs, masks)
            running_loss += loss.item() * images.size(0)
            dices = dice_coeff_batch(outputs, masks)
            all_dices.append(dices)
            batch_idx.append(i)
    avg_loss = running_loss / len(loader.dataset)
    # save dice for this epoch
    df = pd.DataFrame(all_dices).T  # rows=epoch? we'll stack later
    dice_file = os.path.join(save_path, 'validation_dice_scores.xlsx')
    with pd.ExcelWriter(dice_file, mode='a' if os.path.exists(dice_file) else 'w') as writer:
        df.to_excel(writer, sheet_name=f'epoch_{epoch}', index=False)
    return avg_loss

def test_epoch(model, loader, device, save_path):
    model.eval()
    all_dices = []
    with torch.no_grad():
        for i, (images, masks, _) in enumerate(tqdm(loader, desc='Test', leave=False)):
            images = images.to(device)
            masks  = masks.to(device)
            outputs = model(images)
            dices = dice_coeff_batch(outputs, masks)
            all_dices.append(dices)
    df = pd.DataFrame(all_dices).T
    df.to_excel(os.path.join(save_path, 'test_dice_scores.xlsx'), index=False)
    return

def plot_losses(train_losses, val_losses, save_path):
    plt.figure()
    epochs = list(range(1, len(train_losses)+1))
    plt.plot(epochs, train_losses, label='Train Loss')
    plt.plot(epochs, val_losses,   label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Loss vs. Epoch')
    plt.savefig(os.path.join(save_path, 'loss_curve.png'), bbox_inches='tight')
    plt.close()
main.py

import os
import time
import torch
import numpy as np
import pandas as pd
from torch.utils.data import Subset, DataLoader
from sklearn.model_selection import train_test_split
from torchvision.transforms import Compose, Resize, ToTensor
from torch.optim import Adam
from torchinfo import summary

from dataset import GrayscaleSegmentationDataset
from model import UNet
from train import train_epoch, validate_epoch, test_epoch, plot_losses

def paired_transform(image, mask, size=(256,256)):
    """
    Resize, ToTensor, and normalize to [0,1].
    image, mask: PIL.Images
    """
    transform = Compose([
        Resize(size),
        ToTensor(),
    ])
    return transform(image), transform(mask)

if __name__ == "__main__":
    # --- configs ---
    image_folder = 'path/to/images'
    mask_folder  = None          # or 'path/to/masks'
    mask_suffix  = '_seg.png'
    save_path    = 'checkpoints'
    batch_size   = 8
    lr           = 1e-4
    num_epochs   = 50
    device       = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    os.makedirs(save_path, exist_ok=True)

    # --- dataset & splits ---
    full_ds = GrayscaleSegmentationDataset(
        image_folder, mask_folder, mask_suffix,
        transform=paired_transform
    )
    indices = list(range(len(full_ds)))
    train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)
    val_idx, test_idx  = train_test_split(test_idx,  test_size=0.5, random_state=42)
    print(f"Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}")

    train_ds = Subset(full_ds, train_idx)
    val_ds   = Subset(full_ds, val_idx)
    test_ds  = Subset(full_ds, test_idx)

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=4)
    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=4)
    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=4)

    # --- model, optimizer, summary ---
    model = UNet(n_channels=1, n_classes=1).to(device)
    optimizer = Adam(model.parameters(), lr=lr)
    summary(model, input_size=(batch_size,1,256,256))
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total learnable parameters: {total_params}")

    # --- training loop ---
    train_losses, val_losses = [], []
    t0 = time.time()
    for epoch in range(1, num_epochs+1):
        print(f"\nEpoch {epoch}/{num_epochs}")
        tr_loss = train_epoch(model, train_loader, optimizer, device)
        val_loss = validate_epoch(model, val_loader, device, save_path, epoch)
        train_losses.append(tr_loss)
        val_losses.append(val_loss)

        # save epoch losses to excel
        pd.DataFrame([list(range(1, epoch+1)), train_losses])\
          .to_excel(os.path.join(save_path, 'train_losses.xlsx'), index=False, header=False)
        pd.DataFrame([list(range(1, epoch+1)), val_losses])\
          .to_excel(os.path.join(save_path, 'val_losses.xlsx'),   index=False, header=False)

    total_time = time.time() - t0
    print(f"\nTotal training time: {total_time/60:.2f} minutes")

    # --- save model ---
    torch.save(model, os.path.join(save_path, 'unet_model_full.pth'))
    torch.save(model.state_dict(), os.path.join(save_path, 'unet_model_state.pth'))

    # --- plot losses ---
    plot_losses(train_losses, val_losses, save_path)

    # --- testing & visualization ---
    test_epoch(model, test_loader, device, save_path)

    # visualize 5 random samples
    import random
    import matplotlib.pyplot as plt
    from torch.sigmoid import sigmoid

    samples = random.sample(range(len(test_ds)), 5)
    fig, axes = plt.subplots(5, 3, figsize=(9, 15))
    fig.suptitle('Input | Ground Truth | Prediction', fontsize=16)
    for i, idx in enumerate(samples):
        img, mask, name = test_ds[idx]
        inp = img.unsqueeze(0).to(device)
        with torch.no_grad():
            out = sigmoid(model(inp)).cpu().squeeze(0)
        pred = (out > 0.5).float()

        axes[i,0].imshow(img.squeeze(), cmap='gray')
        axes[i,1].imshow(mask.squeeze(), cmap='gray')
        axes[i,2].imshow(pred.squeeze(),cmap='gray')
        for j, title in enumerate([name, name, name]):
            axes[i,j].set_title(title)
        for ax in axes[i]:
            ax.axis('off')

    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.savefig(os.path.join(save_path, 'test_visualization.png'))
    plt.close()
How it ties together
dataset.py defines GrayscaleSegmentationDataset, which loads only .png files, finding masks by suffix.

model.py implements a clean, standard U-Net for single-channel→single-channel segmentation.

train.py contains train_epoch, validate_epoch, test_epoch, dice scoring, and loss-curve plotting.

main.py sets hyperparameters, splits into 80/10/10 via train_test_split (no file copying), builds DataLoaders, prints sizes, runs training with per-epoch Excel logging for losses & dice, saves the model and state dict, prints total time, and finally runs testing + a 5-sample visualization.

All outputs (Excel files, plots, model files) go into your save_path. You can tweak image sizes, hyperparameters, and paths at the top of main.py. Enjoy!

You said:
C:\Users\qy44lyfe\.conda\envs\Swallowenv\python.exe "D:/qy44lyfe/LLM segmentation/github repo/LLM_based_Segmentation/run_to_run variability test/GPT o4 high-7/main.py" --image_dir "D:\qy44lyfe\LLM segmentation\Data sets\Brain Meningioma\images" --mask_dir "D:\qy44lyfe\LLM segmentation\Data sets\Brain Meningioma\Masks" --save_path "D:\qy44lyfe\LLM segmentation\Results\Models Comparison\Models run to run comparison\GPT o4 high-7"
Train: 799, Val: 100, Test: 100
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
UNet                                          [8, 1, 256, 256]          --
├─DoubleConv: 1-1                             [8, 64, 256, 256]         --
│    └─Sequential: 2-1                        [8, 64, 256, 256]         --
│    │    └─Conv2d: 3-1                       [8, 64, 256, 256]         576
│    │    └─BatchNorm2d: 3-2                  [8, 64, 256, 256]         128
│    │    └─ReLU: 3-3                         [8, 64, 256, 256]         --
│    │    └─Conv2d: 3-4                       [8, 64, 256, 256]         36,864
│    │    └─BatchNorm2d: 3-5                  [8, 64, 256, 256]         128
│    │    └─ReLU: 3-6                         [8, 64, 256, 256]         --
├─Down: 1-2                                   [8, 128, 128, 128]        --
│    └─Sequential: 2-2                        [8, 128, 128, 128]        --
│    │    └─MaxPool2d: 3-7                    [8, 64, 128, 128]         --
│    │    └─DoubleConv: 3-8                   [8, 128, 128, 128]        221,696
├─Down: 1-3                                   [8, 256, 64, 64]          --
│    └─Sequential: 2-3                        [8, 256, 64, 64]          --
│    │    └─MaxPool2d: 3-9                    [8, 128, 64, 64]          --
│    │    └─DoubleConv: 3-10                  [8, 256, 64, 64]          885,760
├─Down: 1-4                                   [8, 512, 32, 32]          --
│    └─Sequential: 2-4                        [8, 512, 32, 32]          --
│    │    └─MaxPool2d: 3-11                   [8, 256, 32, 32]          --
│    │    └─DoubleConv: 3-12                  [8, 512, 32, 32]          3,540,992
├─Down: 1-5                                   [8, 512, 16, 16]          --
│    └─Sequential: 2-5                        [8, 512, 16, 16]          --
│    │    └─MaxPool2d: 3-13                   [8, 512, 16, 16]          --
│    │    └─DoubleConv: 3-14                  [8, 512, 16, 16]          4,720,640
├─Up: 1-6                                     [8, 256, 32, 32]          --
│    └─Upsample: 2-6                          [8, 512, 32, 32]          --
│    └─DoubleConv: 2-7                        [8, 256, 32, 32]          --
│    │    └─Sequential: 3-15                  [8, 256, 32, 32]          2,950,144
├─Up: 1-7                                     [8, 128, 64, 64]          --
│    └─Upsample: 2-8                          [8, 256, 64, 64]          --
│    └─DoubleConv: 2-9                        [8, 128, 64, 64]          --
│    │    └─Sequential: 3-16                  [8, 128, 64, 64]          737,792
├─Up: 1-8                                     [8, 64, 128, 128]         --
│    └─Upsample: 2-10                         [8, 128, 128, 128]        --
│    └─DoubleConv: 2-11                       [8, 64, 128, 128]         --
│    │    └─Sequential: 3-17                  [8, 64, 128, 128]         184,576
├─Up: 1-9                                     [8, 64, 256, 256]         --
│    └─Upsample: 2-12                         [8, 64, 256, 256]         --
│    └─DoubleConv: 2-13                       [8, 64, 256, 256]         --
│    │    └─Sequential: 3-18                  [8, 64, 256, 256]         110,848
├─OutConv: 1-10                               [8, 1, 256, 256]          --
│    └─Conv2d: 2-14                           [8, 1, 256, 256]          65
===============================================================================================
Total params: 13,390,209
Trainable params: 13,390,209
Non-trainable params: 0
Total mult-adds (G): 246.76
===============================================================================================
Input size (MB): 2.10
Forward/backward pass size (MB): 3594.52
Params size (MB): 53.56
Estimated Total Size (MB): 3650.18
===============================================================================================
Total learnable parameters: 13390209

Epoch 1/2
Train:   0%|          | 0/100 [00:00<?, ?it/s]
Epoch 2/2
                                                    
Total training time: 1.91 minutes
Traceback (most recent call last):
  File "D:\qy44lyfe\LLM segmentation\github repo\LLM_based_Segmentation\run_to_run variability test\GPT o4 high-7\main.py", line 96, in <module>
    from torch.sigmoid import sigmoid
ModuleNotFoundError: No module named 'torch.sigmoid'

Process finished with exit code 1
ChatGPT said:
The error comes from this line in main.py:

from torch.sigmoid import sigmoid
PyTorch doesn’t expose sigmoid as a submodule—you can either import it directly from the main torch namespace, or just call it off the torch module. Here are two equivalent fixes:

Option A: Import from torch
Replace the bad import:

- from torch.sigmoid import sigmoid
+ from torch import sigmoid
And then leave the rest of your visualization code as-is:

with torch.no_grad():
    out = sigmoid(model(inp)).cpu().squeeze(0)
    pred = (out > 0.5).float()
Option B: Call it from torch directly
Simply remove the import entirely and, in your loop, call:

-     with torch.no_grad():
-         out = sigmoid(model(inp)).cpu().squeeze(0)
+     with torch.no_grad():
+         out = torch.sigmoid(model(inp)).cpu().squeeze(0)
      pred = (out > 0.5).float()
Either approach will resolve the ModuleNotFoundError. After making this change, re-run:

python main.py --image_dir "D:/…/images" \
               --mask_dir  "D:/…/Masks" \
               --save_path "D:/…/Results/…/GPT o4 high-7"
and your visualization step should complete without that import error.




No file chosenNo file chosen
ChatGPT can make mistakes. Check important info. See Cookie Preferences.
