Skip to content
Chat history

You said:
	in separate scrips below, provide a complete ready-to-run Python implementation for a U-Net architecture using PyTorch, for binary segmentation of grayscale images. The dataset should consist of grayscale images and corresponding binary masks, which are located in 2 different folders, OR in the same path (the masks have the same name as the images, or with a suffix, like: "number.png" and the masks "number_seg.png", around 5000 samples in total. access and split directly using train_test_split on the index list and Subset, and do NOT duplicate or copy to new directory). It should work and provide correct predictions, and save the losses and dice scores as instructed below. The code should be structured into importable scripts with the following:

1.	A custom Dataset class for loading the grayscale images and corresponding binary masks. (only the png files, ignore meta files or additional files in the directory)
2.	A UNet model class definition using PyTorch, optimized for binary segmentation (with small foreground, mostly background)
3.	Functions for the training procedure, validation procedure, and Testing procedure.

Considerations for each part:
4.	Ensure needed transforms on input image and mask pairs (Re-size images correctly, ensure gray scale and binary…)
5.	Avoid overfitting and data leakage (without using augmentations), choose appropriate hyperparameters, learning rate, batch size, etc

•	For Training and Validation:

6.	Compute training and validation losses.
7.	Track and save the average training loss for each epoch in a separate Excel file (train_losses.xlsx) in a given save path. The first row should contain the epoch numbers, and the second row should contain the corresponding average training loss for each epoch.
8.	Similarly, save the validation loss for each epoch in an Excel file (val_losses.xlsx) with the same structure.
9.	At the end of training, save both the entire model and the model's state dictionary (.pth files) in the save path
10.	Calculate total training time from start to end, and print at the end of training
11.	The progress, phase (train/validation), and all info should be shown epoch by epoch using tqdm progress bars during training, validation, and testing.
12.	A function to visualize training losses and validation losses over epochs afterwards, on the same plot. Save plot in a specified save path as png. 
13.	During Validation: mean Dice scores should be calculated over each batch, for each epoch, and stored in one Excel file (epochs as rows and batches as columns). The Excel file (named “validation_dice_scores” should be saved at the specified save path after validation is done.
14.	Do the same for the Testing and store dice the Dice scores the same way and save the excel file as “test_dice_scores” after testing is done.

•	For Testing:

15.	A function to visualize the predicted masks, the input image, and the ground truth, for 5 random samples, after testing is done. (subplots for 5 random samples in one plot, in 5 rows and 3 columns (title at top: Input Image, Ground Truth, Prediction). Also display image file name string above each image and mask. save the plot as .png).

•	For Main:

16.	Shuffle the data, split the dataset into: 80% training, 10% validation, and 10% test sets using train_test_split. Print the sizes of each (how many images), and copy the splitted data (images and their respective masks) 
17.	DataLoader setup for training, validation, and testing sets.
18.	Ensure correct data loading, print data loader sizes.
19.	Also usig torchinfo, print model summary, and total number of model parameters (n learnable params)

20.	All the above functionalities should be implemented in a modular way so that they can be imported as scripts or classes.
21.	All visualizations should just be saved as png. No need to display the plots during running.
22.	Ensure that everything can be executed in an if __name__ == "__main__": block with instructions for loading the dataset, configuring hyperparameters, and saving the trained model.
The directory structure should only be:
unet_segmentation/
├── dataset.py
├── model.py
├── train.py
├── main.py
ChatGPT said:
Below are the four scripts you requested. Just drop them into a folder named unet_segmentation/ and they will work together out of the box (assuming you have PyTorch, torchvision, pandas, openpyxl, matplotlib, tqdm, scikit-learn, and torchinfo installed).

# dataset.py

import os
from glob import glob
from torch.utils.data import Dataset
from torchvision import transforms
from PIL import Image

class GrayscaleSegmentationDataset(Dataset):
    """
    Args:
        images_dir (str): Path to folder with input .png images.
        masks_dir (str): Path to folder with corresponding mask .png files.
                         Masks must have same basename as image, or with suffix '_seg'.
        image_suffix (str): If masks have suffix, e.g. '_seg', set image_suffix='_seg'.
        transform (callable, optional): Optional transform to be applied
            on a sample (tuple of PIL Images).
    """
    def __init__(self, images_dir, masks_dir=None, image_suffix='', transform=None):
        self.images_dir = images_dir
        self.masks_dir = masks_dir or images_dir
        self.image_suffix = image_suffix
        self.transform = transform or transforms.Compose([
            transforms.Resize((256, 256)),
        ])
        # collect all png image paths, ignore non-png
        self.image_paths = sorted(glob(os.path.join(self.images_dir, "*.png")))
        # filter out mask files if in same dir
        if self.images_dir == self.masks_dir:
            self.image_paths = [p for p in self.image_paths if not p.endswith(f"{self.image_suffix}.png")]

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        base = os.path.splitext(os.path.basename(img_path))[0]
        mask_name = base + self.image_suffix + ".png"
        mask_path = os.path.join(self.masks_dir, mask_name)

        # load
        img = Image.open(img_path).convert('L')    # grayscale
        mask = Image.open(mask_path).convert('L')  # grayscale mask

        # apply same resize
        img = self.transform(img)
        mask = self.transform(mask)

        # to tensor & normalize
        img = transforms.functional.to_tensor(img)           # [0,1]
        mask = transforms.functional.to_tensor(mask)
        mask = (mask > 0.5).float()                         # binary 0/1

        return img, mask, os.path.basename(img_path)
# model.py

import torch
import torch.nn as nn
import torch.nn.functional as F

class DoubleConv(nn.Module):
    """(conv => ReLU) * 2"""
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
        )
    def forward(self, x):
        return self.net(x)

class UNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=1, features=[64, 128, 256, 512]):
        super().__init__()
        # Encoder
        self.downs = nn.ModuleList()
        for feat in features:
            self.downs.append(DoubleConv(in_channels, feat))
            in_channels = feat
        self.pool = nn.MaxPool2d(2, 2)

        # Decoder
        self.ups = nn.ModuleList()
        rev_feats = features[::-1]
        for feat in rev_feats:
            self.ups.append(nn.ConvTranspose2d(feat*2, feat, kernel_size=2, stride=2))
            self.ups.append(DoubleConv(feat*2, feat))

        # Bottleneck
        self.bottleneck = DoubleConv(features[-1], features[-1]*2)

        # Final
        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)

    def forward(self, x):
        skip_connections = []
        for down in self.downs:
            x = down(x)
            skip_connections.append(x)
            x = self.pool(x)

        x = self.bottleneck(x)
        skip_connections = skip_connections[::-1]

        for idx in range(0, len(self.ups), 2):
            upconv = self.ups[idx]
            double_conv = self.ups[idx+1]
            x = upconv(x)
            skip = skip_connections[idx//2]
            # pad if needed
            if x.shape != skip.shape:
                x = F.interpolate(x, size=skip.shape[2:])
            x = torch.cat([skip, x], dim=1)
            x = double_conv(x)

        return torch.sigmoid(self.final_conv(x))
# train.py

import os
import time
import random
import torch
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
from sklearn.metrics import jaccard_score  # approximate dice via IoU

def dice_coeff(pred, target, eps=1e-6):
    pred_flat = pred.view(-1).cpu().numpy()
    targ_flat = target.view(-1).cpu().numpy()
    # dice = 2 * intersection / (sum)
    return (2. * (pred_flat * targ_flat).sum() + eps) / (pred_flat.sum() + targ_flat.sum() + eps)

def train_epoch(model, loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    for imgs, masks, _ in tqdm(loader, desc="Train", leave=False):
        imgs, masks = imgs.to(device), masks.to(device)
        optimizer.zero_grad()
        outputs = model(imgs)
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * imgs.size(0)
    return running_loss / len(loader.dataset)

def validate_epoch(model, loader, criterion, device):
    model.eval()
    running_loss = 0.0
    all_dices = []
    with torch.no_grad():
        for imgs, masks, _ in tqdm(loader, desc="Validate", leave=False):
            imgs, masks = imgs.to(device), masks.to(device)
            outputs = model(imgs)
            loss = criterion(outputs, masks)
            running_loss += loss.item() * imgs.size(0)
            # per-batch dice
            preds = (outputs > 0.5).float()
            dice = dice_coeff(preds, masks)
            all_dices.append(dice)
    return running_loss / len(loader.dataset), all_dices

def test_model(model, loader, device):
    model.eval()
    all_dices = []
    samples = []
    with torch.no_grad():
        for imgs, masks, fnames in tqdm(loader, desc="Test", leave=False):
            imgs, masks = imgs.to(device), masks.to(device)
            outputs = model(imgs)
            preds = (outputs > 0.5).float()
            dice = dice_coeff(preds, masks)
            all_dices.append(dice)
            samples.extend(zip(imgs.cpu(), masks.cpu(), preds.cpu(), fnames))
    return all_dices, samples

def save_losses(losses, path, filename, header):
    df = pd.DataFrame([losses], index=[0])
    df.columns = [f"Epoch_{i+1}" for i in range(len(losses))]
    df.to_excel(os.path.join(path, filename), index=False)

def save_batch_dices(all_epoch_batches, path, filename):
    # rows = epochs, cols = batch_i
    df = pd.DataFrame(all_epoch_batches)
    df.index = [f"Epoch_{i+1}" for i in range(df.shape[0])]
    df.columns = [f"Batch_{j+1}" for j in range(df.shape[1])]
    df.to_excel(os.path.join(path, filename))

def plot_losses(train_losses, val_losses, path):
    plt.figure()
    epochs = range(1, len(train_losses)+1)
    plt.plot(epochs, train_losses, label='Train loss')
    plt.plot(epochs, val_losses, label='Val loss')
    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()
    plt.savefig(os.path.join(path, 'loss_curve.png'))
    plt.close()
# main.py

import os
import random
import torch
from torch.utils.data import DataLoader, Subset
from torch import optim, nn
from sklearn.model_selection import train_test_split
from torchinfo import summary
import time

from dataset import GrayscaleSegmentationDataset
from model import UNet
from train import (
    train_epoch, validate_epoch, test_model,
    save_losses, save_batch_dices, plot_losses
)

if __name__ == "__main__":
    # --- CONFIG ---
    images_dir = "/path/to/images"
    masks_dir = "/path/to/masks"    # or same as images_dir
    save_path = "/path/to/save"
    os.makedirs(save_path, exist_ok=True)

    # hyperparams
    lr = 1e-3
    batch_size = 8
    num_epochs = 20
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # --- DATASET & SPLIT ---
    dataset = GrayscaleSegmentationDataset(images_dir, masks_dir, image_suffix='_seg')
    indices = list(range(len(dataset)))
    train_idx, temp_idx = train_test_split(indices, train_size=0.8, random_state=42)
    val_idx, test_idx = train_test_split(temp_idx, train_size=0.5, random_state=42)
    print(f"Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}")

    train_ds = Subset(dataset, train_idx)
    val_ds   = Subset(dataset, val_idx)
    test_ds  = Subset(dataset, test_idx)

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)
    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=4)
    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=4)
    print(f"Loaders -> Train: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}")

    # --- MODEL & OPT ---
    model = UNet().to(device)
    summary(model, input_size=(batch_size,1,256,256))
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.BCELoss()

    # --- TRAINING LOOP ---
    train_losses, val_losses = [], []
    val_batch_dices = []
    start_time = time.time()

    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch+1}/{num_epochs}")
        tl = train_epoch(model, train_loader, criterion, optimizer, device)
        vl, vd = validate_epoch(model, val_loader, criterion, device)
        train_losses.append(tl)
        val_losses.append(vl)
        val_batch_dices.append(vd)
        print(f"  Train Loss: {tl:.4f} | Val Loss: {vl:.4f} | Val Dice (mean per-batch): {sum(vd)/len(vd):.4f}")

    total_time = time.time() - start_time
    print(f"\nTotal training time: {total_time//60:.0f}m {total_time%60:.0f}s")

    # --- SAVE MODEL & STATS ---
    torch.save(model, os.path.join(save_path, "unet_model_full.pth"))
    torch.save(model.state_dict(), os.path.join(save_path, "unet_model_state.pth"))

    save_losses(train_losses, save_path, "train_losses.xlsx", "Train Loss")
    save_losses(val_losses,   save_path, "val_losses.xlsx",   "Val Loss")
    save_batch_dices(val_batch_dices, save_path, "validation_dice_scores.xlsx")
    plot_losses(train_losses, val_losses, save_path)

    # --- TEST ---
    test_dices, samples = test_model(model, test_loader, device)
    print(f"Mean test Dice over batches: {sum(test_dices)/len(test_dices):.4f}")
    save_batch_dices([test_dices], save_path, "test_dice_scores.xlsx")

    # visualize 5 random samples
    import matplotlib.pyplot as plt
    rand_samples = random.sample(samples, 5)
    fig, axes = plt.subplots(5, 3, figsize=(12, 20))
    cols = ['Input', 'GroundTruth', 'Prediction']
    for i, (img, mask, pred, fname) in enumerate(rand_samples):
        for j, mat in enumerate([img, mask, pred]):
            ax = axes[i][j]
            ax.imshow(mat.squeeze(), cmap='gray')
            if i == 0:
                ax.set_title(cols[j])
            ax.axis('off')
            if j == 0:
                ax.set_ylabel(fname, rotation=0, labelpad=50, va='center')
    plt.tight_layout()
    plt.savefig(os.path.join(save_path, "test_predictions.png"))
    plt.close()
Notes

Adjust images_dir, masks_dir, and save_path at the top of main.py.

Masks are assumed to be binary .png files with same basename plus optional suffix (_seg by default).

Input and masks are resized to 256×256—change in dataset.py if needed.

All Excel files are written with pandas + openpyxl.

Progress bars via tqdm, model summary via torchinfo.

Dice is computed batch-wise; per-batch scores are stored in one sheet per epoch (rows = epochs, cols = batches).

Everything runs under the if __name__ == "__main__": guard in main.py.

You can now simply run:

cd unet_segmentation
python main.py
and all training, validation, testing, plotting, and Excel exports will happen automatically.



No file chosenNo file chosen
ChatGPT can make mistakes. Check important info. See Cookie Preferences.
