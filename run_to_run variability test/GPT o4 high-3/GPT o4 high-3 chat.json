 in separate scripts below, provide a complete ready-to-run Python implementation for a U-Net architecture using PyTorch, for binary segmentation of grayscale images. The dataset should consist of grayscale images and corresponding binary masks, which are located in 2 different folders, OR in the same path (the masks have the same name as the images, or with a suffix, like: "number.png" and the masks "number_seg.png", around 5000 samples in total. access and split directly using train_test_split on the index list and Subset, and do NOT duplicate or copy to new directory). It should work and provide correct predictions, and save the losses and dice scores as instructed below. write in separate scripts. The code should be structured into importable scripts with the following: 1. A custom Dataset class for loading the grayscale images and corresponding binary masks. (only the png files, ignore meta files or additional files in the directory) 2. A UNet model class definition using PyTorch, optimized for binary segmentation (with small foreground, mostly background) 3. Functions for the training procedure, validation procedure, and Testing procedure. Considerations for each part: 4. Ensure needed transforms on input image and mask pairs (Re-size images correctly, ensure gray scale and binary…) 5. Avoid overfitting and data leakage (without using augmentations), choose appropriate hyperparameters, learning rate, batch size, etc • For Training and Validation: 6. Compute training and validation losses. 7. Track and save the average training loss for each epoch in a separate Excel file (train_losses.xlsx) in a given save path. The first row should contain the epoch numbers, and the second row should contain the corresponding average training loss for each epoch. 8. Similarly, save the validation loss for each epoch in an Excel file (val_losses.xlsx) with the same structure. 9. At the end of training, save both the entire model and the model's state dictionary (.pth files) in the save path 10. Calculate total training time from start to end, and print at the end of training 11. The progress, phase (train/validation), and all info should be shown epoch by epoch using tqdm progress bars during training, validation, and testing. 12. A function to visualize training losses and validation losses over epochs afterwards, on the same plot. Save plot in a specified save path as png. 13. During Validation: mean Dice scores should be calculated over each batch, for each epoch, and stored in an Excel file (epochs as rows and batches as columns). The Excel file (named “validation_dice_scores” should be saved at the specified save path after validation is done. 14. Do the same for the Testing and store dice the Dice scores the same way and save the excel file as “test_dice_scores” after testing is done. • For Testing: 15. A function to visualize the predicted masks, the input image, and the ground truth, for 5 random samples, after testing is done. (subplots for 5 random samples in one plot, in 5 rows and 3 columns (title at top: Input Image, Ground Truth, Prediction). Also display image file name string above each image and mask. save the plot as .png). • For Main: 16. Shuffle the data, split the dataset into: 80% training, 10% validation, and 10% test sets using train_test_split. Print the sizes of each (how many images), and copy the splitted data (images and their respective masks) 17. DataLoader setup for training, validation, and testing sets. 18. Ensure correct data loading, print data loader sizes. 19. Also usig torchinfo, print model summary, and total number of model parameters (n learnable params) 20. All the above functionalities should be implemented in a modular way so that they can be imported as scripts or classes. 21. All visualizations should just be saved as png. No need to display the plots during running. 22. Ensure that everything can be executed in an if __name__ == "__main__": block with instructions for loading the dataset, configuring hyperparameters, and saving the trained model. The directory structure should only be: unet_segmentation/ ├── dataset.py ├── model.py ├── train.py ├── main.py
ChatGPT said:

Below are the four modular scripts you can drop into unet_segmentation/ and run. Adjust paths and hyper-parameters in main.py as needed.

# unet_segmentation/dataset.py

import os
from PIL import Image
from torch.utils.data import Dataset
import torchvision.transforms as T


class SegmentationDataset(Dataset):
    """
    Dataset for grayscale images with binary masks.
    Expects all .png images in images_dir and corresponding masks in masks_dir
    (same filename or with suffix "_seg").
    """

    def __init__(self, images_dir, masks_dir=None, mask_suffix='_seg', img_size=(256, 256)):
        """
        Args:
            images_dir (str): path to folder with input images (.png).
            masks_dir  (str): path to folder with masks. If None, masks in same folder using suffix.
            mask_suffix (str): suffix before the .png in mask filenames.
            img_size   (tuple): (H, W) to resize both image and mask.
        """
        self.images_dir = images_dir
        self.masks_dir = masks_dir or images_dir
        self.mask_suffix = mask_suffix
        self.img_size = img_size

        # list all png files in images_dir
        self.image_files = sorted([
            f for f in os.listdir(images_dir)
            if f.lower().endswith('.png')
        ])

        # transforms
        self.img_transform = T.Compose([
            T.Resize(self.img_size),
            T.Grayscale(num_output_channels=1),
            T.ToTensor(),
            T.Normalize([0.5], [0.5])
        ])
        self.mask_transform = T.Compose([
            T.Resize(self.img_size),
            T.Grayscale(num_output_channels=1),
            T.ToTensor(),  # gives [0,1]
        ])

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        img_path = os.path.join(self.images_dir, img_name)
        # determine mask filename
        if self.masks_dir == self.images_dir:
            base, ext = os.path.splitext(img_name)
            mask_name = base + self.mask_suffix + ext
        else:
            mask_name = img_name
        mask_path = os.path.join(self.masks_dir, mask_name)

        # load
        image = Image.open(img_path)
        mask = Image.open(mask_path)

        # transforms
        image = self.img_transform(image)
        mask = self.mask_transform(mask)
        # ensure binary 0/1
        mask = (mask > 0.5).float()

        return image, mask, img_name

# unet_segmentation/model.py

import torch
import torch.nn as nn
import torch.nn.functional as F


class DoubleConv(nn.Module):
    """(Conv => BN => ReLU) * 2"""

    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        return self.double_conv(x)


class Down(nn.Module):
    """Downscaling with MaxPool then DoubleConv"""

    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.down = nn.Sequential(
            nn.MaxPool2d(2),
            DoubleConv(in_ch, out_ch)
        )

    def forward(self, x):
        return self.down(x)


class Up(nn.Module):
    """Upscaling then DoubleConv"""

    def __init__(self, in_ch, out_ch, bilinear=True):
        super().__init__()
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
            self.conv = DoubleConv(in_ch, out_ch)
        else:
            self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2)
            self.conv = DoubleConv(in_ch, out_ch)

    def forward(self, x1, x2):
        x1 = self.up(x1)
        # pad if needed
        diffY = x2.size()[2] - x1.size()[2]
        diffX = x2.size()[3] - x1.size()[3]
        x1 = F.pad(x1, [diffX//2, diffX - diffX//2,
                        diffY//2, diffY - diffY//2])
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)


class OutConv(nn.Module):
    """Final 1×1 conv"""

    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv = nn.Conv2d(in_ch, out_ch, 1)

    def forward(self, x):
        return self.conv(x)


class UNet(nn.Module):
    def __init__(self, n_channels=1, n_classes=1, bilinear=True):
        super().__init__()
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.bilinear = bilinear

        self.inc = DoubleConv(n_channels, 64)
        self.down1 = Down(64, 128)
        self.down2 = Down(128, 256)
        self.down3 = Down(256, 512)
        factor = 2 if bilinear else 1
        self.down4 = Down(512, 1024 // factor)
        self.up1 = Up(1024, 512 // factor, bilinear)
        self.up2 = Up(512, 256 // factor, bilinear)
        self.up3 = Up(256, 128 // factor, bilinear)
        self.up4 = Up(128, 64, bilinear)
        self.outc = OutConv(64, n_classes)

    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.down1(x1)
        x3 = self.down2(x2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)
        x = self.up1(x5, x4)
        x = self.up2(x, x3)
        x = self.up3(x, x2)
        x = self.up4(x, x1)
        logits = self.outc(x)
        return torch.sigmoid(logits)

# unet_segmentation/train.py

import time
import os
import torch
import torch.nn as nn
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt


def dice_coeff(pred, target, eps=1e-6):
    # pred, target: (B, 1, H, W)
    pred = (pred > 0.5).float()
    inter = (pred * target).sum(dim=(1,2,3))
    union = pred.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))
    dice = (2 * inter + eps) / (union + eps)
    return dice


def train_epoch(model, loader, optimizer, criterion, device):
    model.train()
    running_loss = 0.0
    for images, masks, _ in tqdm(loader, desc="Train", leave=False):
        images = images.to(device)
        masks = masks.to(device)
        preds = model(images)
        loss = criterion(preds, masks)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * images.size(0)
    return running_loss / len(loader.dataset)


def validate_epoch(model, loader, criterion, device, save_path, epoch):
    model.eval()
    running_loss = 0.0
    all_dice = []
    with torch.no_grad():
        for images, masks, _ in tqdm(loader, desc="Validate", leave=False):
            images = images.to(device)
            masks = masks.to(device)
            preds = model(images)
            loss = criterion(preds, masks)
            running_loss += loss.item() * images.size(0)
            all_dice.append(dice_coeff(preds, masks).cpu().numpy())
    avg_loss = running_loss / len(loader.dataset)
    # save dice per batch
    df = pd.DataFrame(all_dice)
    df.to_excel(os.path.join(save_path, f'validation_dice_scores_epoch{epoch}.xlsx'),
                index=False)
    return avg_loss


def test_model(model, loader, device, save_path):
    model.eval()
    all_dice = []
    samples = []
    with torch.no_grad():
        for images, masks, names in tqdm(loader, desc="Test", leave=False):
            images = images.to(device)
            masks = masks.to(device)
            preds = model(images)
            all_dice.append(dice_coeff(preds, masks).cpu().numpy())
            # collect for visualization
            for i in range(images.size(0)):
                samples.append((images[i].cpu(), masks[i].cpu(), preds[i].cpu(), names[i]))
    # save test dice
    df = pd.DataFrame(all_dice)
    df.to_excel(os.path.join(save_path, 'test_dice_scores.xlsx'), index=False)
    return samples


def plot_losses(train_losses, val_losses, save_path):
    plt.figure()
    epochs = list(range(1, len(train_losses)+1))
    plt.plot(epochs, train_losses, label='Train Loss')
    plt.plot(epochs, val_losses, label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.savefig(os.path.join(save_path, 'loss_curve.png'))
    plt.close()

# unet_segmentation/main.py

import os
import argparse
import torch
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import train_test_split
import pandas as pd
from torch import optim, nn
from torchinfo import summary
from dataset import SegmentationDataset
from model import UNet
from train import (train_epoch, validate_epoch, test_model,
                   plot_losses)

def save_loss_excel(losses, filename):
    # first row epochs, second row losses
    df = pd.DataFrame([list(range(1, len(losses)+1)), losses])
    df.to_excel(filename, header=False, index=False)

def visualize_samples(samples, save_path):
    import matplotlib.pyplot as plt
    import random
    chosen = random.sample(samples, 5)
    fig, axes = plt.subplots(5, 3, figsize=(9, 15))
    col_titles = ['Input Image', 'Ground Truth', 'Prediction']
    for ax, title in zip(axes[0], col_titles):
        ax.set_title(title)
    for i, (img, mask, pred, name) in enumerate(chosen):
        img = img.squeeze(0).numpy()
        mask = mask.squeeze(0).numpy()
        pred = (pred.squeeze(0).numpy() > 0.5).astype(float)
        for j, data in enumerate([img, mask, pred]):
            ax = axes[i, j]
            ax.imshow(data, cmap='gray')
            ax.axis('off')
            if j == 0:
                ax.set_ylabel(name, rotation=0, labelpad=50, va='center')
    plt.tight_layout()
    plt.savefig(os.path.join(save_path, 'test_samples.png'))
    plt.close()

def main(args):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # dataset
    ds = SegmentationDataset(args.images_dir, args.masks_dir, img_size=(args.img_size, args.img_size))
    n = len(ds)
    idx = list(range(n))
    # splits
    train_idx, temp_idx = train_test_split(idx, test_size=0.2, random_state=42, shuffle=True)
    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42, shuffle=True)
    print(f"Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}")

    train_ds = Subset(ds, train_idx)
    val_ds   = Subset(ds, val_idx)
    test_ds  = Subset(ds, test_idx)

    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=4)
    val_loader   = DataLoader(val_ds,   batch_size=args.batch_size, shuffle=False, num_workers=4)
    test_loader  = DataLoader(test_ds,  batch_size=args.batch_size, shuffle=False, num_workers=4)
    print(f"Loaders ➤ Train batches: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}")

    # model
    model = UNet(n_channels=1, n_classes=1).to(device)
    summary(model, input_size=(args.batch_size, 1, args.img_size, args.img_size))
    print(f"Total params: {sum(p.numel() for p in model.parameters() if p.requires_grad)}")

    # optimizer, criterion
    optimizer = optim.Adam(model.parameters(), lr=args.lr)
    criterion = nn.BCELoss()

    # training loop
    train_losses, val_losses = [], []
    start_time = time.time()
    for epoch in range(1, args.epochs + 1):
        print(f"Epoch {epoch}/{args.epochs}")
        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
        val_loss   = validate_epoch(model, val_loader, criterion, device, args.save_path, epoch)
        print(f"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
        train_losses.append(train_loss)
        val_losses.append(val_loss)
    total_time = time.time() - start_time
    print(f"Training completed in {total_time/60:.2f} min")

    # save losses
    os.makedirs(args.save_path, exist_ok=True)
    save_loss_excel(train_losses, os.path.join(args.save_path, 'train_losses.xlsx'))
    save_loss_excel(val_losses,   os.path.join(args.save_path, 'val_losses.xlsx'))
    # save model
    torch.save(model, os.path.join(args.save_path, 'unet_full.pth'))
    torch.save(model.state_dict(), os.path.join(args.save_path, 'unet_state.pth'))
    # plot curves
    plot_losses(train_losses, val_losses, args.save_path)

    # testing
    samples = test_model(model, test_loader, device, args.save_path)
    visualize_samples(samples, args.save_path)


if __name__ == "__main__":
    import time
    parser = argparse.ArgumentParser()
    parser.add_argument('--images_dir', type=str, required=True, help='path to images folder')
    parser.add_argument('--masks_dir',  type=str, default=None, help='path to masks folder (if different)')
    parser.add_argument('--save_path',  type=str, required=True, help='where to save outputs')
    parser.add_argument('--img_size',   type=int, default=256, help='resize H and W')
    parser.add_argument('--batch_size', type=int, default=8)
    parser.add_argument('--lr',         type=float, default=1e-4)
    parser.add_argument('--epochs',     type=int, default=50)
    args = parser.parse_args()
    main(args)


Usage example (from project root):

python -m unet_segmentation.main \
  --images_dir /path/to/images \
  --masks_dir  /path/to/masks \
  --save_path  /path/to/output \
  --img_size   256 \
  --batch_size 8 \
  --lr         1e-4 \
  --epochs     50


This will:

Split the 5 000 samples into 80/10/10 subsets (no copying of files).

Train U-Net with BCE loss, Adam optimizer.

Save per-epoch train/val losses to train_losses.xlsx / val_losses.xlsx.

Save per-epoch batch-wise Dice scores in validation_dice_scores_epoch{EPOCH}.xlsx.

Save test batch Dice scores in test_dice_scores.xlsx.