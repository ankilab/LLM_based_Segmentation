Skip to content
Chat history

You said:
ï¶ in separate scripts below, provide a complete ready-to-run Python implementation for a U-Net architecture using PyTorch, for binary segmentation of grayscale images. The dataset should consist of grayscale images and corresponding binary masks, which are located in 2 different folders, OR in the same path (the masks have the same name as the images, or with a suffix, like: "number.png" and the masks "number_seg.png", around 5000 samples in total. access and split directly using train_test_split on the index list and Subset, and do NOT duplicate or copy to new directory). It should work and provide correct predictions, and save the losses and dice scores as instructed below. write in separate scripts. The code should be structured into importable scripts with the following: 1. A custom Dataset class for loading the grayscale images and corresponding binary masks. (only the png files, ignore meta files or additional files in the directory) 2. A UNet model class definition using PyTorch, optimized for binary segmentation (with small foreground, mostly background) 3. Functions for the training procedure, validation procedure, and Testing procedure. Considerations for each part: 4. Ensure needed transforms on input image and mask pairs (Re-size images correctly, ensure gray scale and binaryâ€¦) 5. Avoid overfitting and data leakage (without using augmentations), choose appropriate hyperparameters, learning rate, batch size, etc â€¢ For Training and Validation: 6. Compute training and validation losses. 7. Track and save the average training loss for each epoch in a separate Excel file (train_losses.xlsx) in a given save path. The first row should contain the epoch numbers, and the second row should contain the corresponding average training loss for each epoch. 8. Similarly, save the validation loss for each epoch in an Excel file (val_losses.xlsx) with the same structure. 9. At the end of training, save both the entire model and the model's state dictionary (.pth files) in the save path 10. Calculate total training time from start to end, and print at the end of training 11. The progress, phase (train/validation), and all info should be shown epoch by epoch using tqdm progress bars during training, validation, and testing. 12. A function to visualize training losses and validation losses over epochs afterwards, on the same plot. Save plot in a specified save path as png. 13. During Validation: mean Dice scores should be calculated over each batch, for each epoch, and stored in an Excel file (epochs as rows and batches as columns). The Excel file (named â€œvalidation_dice_scoresâ€ should be saved at the specified save path after validation is done. 14. Do the same for the Testing and store dice the Dice scores the same way and save the excel file as â€œtest_dice_scoresâ€ after testing is done. â€¢ For Testing: 15. A function to visualize the predicted masks, the input image, and the ground truth, for 5 random samples, after testing is done. (subplots for 5 random samples in one plot, in 5 rows and 3 columns (title at top: Input Image, Ground Truth, Prediction). Also display image file name string above each image and mask. save the plot as .png). â€¢ For Main: 16. Shuffle the data, split the dataset into: 80% training, 10% validation, and 10% test sets using train_test_split. Print the sizes of each (how many images), and copy the splitted data (images and their respective masks) 17. DataLoader setup for training, validation, and testing sets. 18. Ensure correct data loading, print data loader sizes. 19. Also usig torchinfo, print model summary, and total number of model parameters (n learnable params) 20. All the above functionalities should be implemented in a modular way so that they can be imported as scripts or classes. 21. All visualizations should just be saved as png. No need to display the plots during running. 22. Ensure that everything can be executed in an if __name__ == "__main__": block with instructions for loading the dataset, configuring hyperparameters, and saving the trained model. The directory structure should only be: unet_segmentation/ â”œâ”€â”€ dataset.py â”œâ”€â”€ model.py â”œâ”€â”€ train.py â”œâ”€â”€ main.py
ChatGPT said:
Great! Below is a modular and complete PyTorch implementation for a binary U-Net segmentation pipeline for grayscale images. The project is structured exactly as requested under the unet_segmentation/ directory:

ðŸ”§ Directory Structure
unet_segmentation/
â”œâ”€â”€ dataset.py
â”œâ”€â”€ model.py
â”œâ”€â”€ train.py
â”œâ”€â”€ main.py
âœ… 1. dataset.py â€” Custom Dataset Class
# unet_segmentation/dataset.py

import os
import glob
from PIL import Image
from torch.utils.data import Dataset
import torch
from torchvision import transforms


class GrayscaleSegmentationDataset(Dataset):
    def __init__(self, image_dir, mask_dir=None, suffix="_seg", transform=None):
        self.image_dir = image_dir
        self.mask_dir = mask_dir if mask_dir else image_dir
        self.suffix = suffix
        self.transform = transform

        self.image_paths = sorted([p for p in glob.glob(os.path.join(self.image_dir, "*.png")) if self.suffix not in os.path.basename(p)])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        image_name = os.path.basename(image_path)
        mask_name = image_name.replace(".png", f"{self.suffix}.png")
        mask_path = os.path.join(self.mask_dir, mask_name)

        image = Image.open(image_path).convert("L")
        mask = Image.open(mask_path).convert("L")

        # Convert to tensors and normalize
        image = transforms.ToTensor()(image)  # [1, H, W]
        mask = transforms.ToTensor()(mask)    # [1, H, W]

        # Ensure binary mask
        mask = (mask > 0.5).float()

        return image, mask, image_name
âœ… 2. model.py â€” U-Net Model
# unet_segmentation/model.py

import torch
import torch.nn as nn
import torch.nn.functional as F


class DoubleConv(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.conv(x)


class UNet(nn.Module):
    def __init__(self, in_ch=1, out_ch=1):
        super().__init__()
        self.down1 = DoubleConv(in_ch, 64)
        self.pool1 = nn.MaxPool2d(2)
        self.down2 = DoubleConv(64, 128)
        self.pool2 = nn.MaxPool2d(2)
        self.down3 = DoubleConv(128, 256)
        self.pool3 = nn.MaxPool2d(2)
        self.down4 = DoubleConv(256, 512)
        self.pool4 = nn.MaxPool2d(2)

        self.middle = DoubleConv(512, 1024)

        self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
        self.dec4 = DoubleConv(1024, 512)
        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.dec3 = DoubleConv(512, 256)
        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec2 = DoubleConv(256, 128)
        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec1 = DoubleConv(128, 64)

        self.final = nn.Conv2d(64, out_ch, 1)

    def forward(self, x):
        d1 = self.down1(x)
        d2 = self.down2(self.pool1(d1))
        d3 = self.down3(self.pool2(d2))
        d4 = self.down4(self.pool3(d3))
        m = self.middle(self.pool4(d4))

        u4 = self.dec4(torch.cat([self.up4(m), d4], dim=1))
        u3 = self.dec3(torch.cat([self.up3(u4), d3], dim=1))
        u2 = self.dec2(torch.cat([self.up2(u3), d2], dim=1))
        u1 = self.dec1(torch.cat([self.up1(u2), d1], dim=1))
        return torch.sigmoid(self.final(u1))
âœ… 3. train.py â€” Training, Validation, Testing, Utilities
# unet_segmentation/train.py

import os
import time
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import jaccard_score


def dice_score(pred, target, threshold=0.5):
    pred = (pred > threshold).float()
    intersection = (pred * target).sum((1, 2, 3))
    union = pred.sum((1, 2, 3)) + target.sum((1, 2, 3))
    dice = (2. * intersection + 1e-8) / (union + 1e-8)
    return dice.mean().item()


def train_one_epoch(model, loader, criterion, optimizer, device):
    model.train()
    epoch_loss = 0
    loop = tqdm(loader, desc="Training", leave=False)
    for images, masks, _ in loop:
        images, masks = images.to(device), masks.to(device)
        preds = model(images)
        loss = criterion(preds, masks)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    return epoch_loss / len(loader)


def validate(model, loader, criterion, device, save_path, epoch):
    model.eval()
    epoch_loss = 0
    dice_scores = []
    dice_per_batch = []

    loop = tqdm(loader, desc="Validating", leave=False)
    with torch.no_grad():
        for images, masks, _ in loop:
            images, masks = images.to(device), masks.to(device)
            preds = model(images)
            loss = criterion(preds, masks)
            epoch_loss += loss.item()
            dice = dice_score(preds, masks)
            dice_scores.append(dice)
            dice_per_batch.append(dice)

    if epoch == 0:
        df = pd.DataFrame(columns=[f"Batch_{i}" for i in range(len(dice_per_batch))])
    else:
        df = pd.read_excel(os.path.join(save_path, "validation_dice_scores.xlsx"), index_col=0)

    df.loc[f"Epoch_{epoch}"] = dice_per_batch
    df.to_excel(os.path.join(save_path, "validation_dice_scores.xlsx"))

    return epoch_loss / len(loader)


def test(model, loader, device, save_path):
    model.eval()
    dice_per_batch = []
    predictions = []

    loop = tqdm(loader, desc="Testing", leave=False)
    with torch.no_grad():
        for images, masks, names in loop:
            images, masks = images.to(device), masks.to(device)
            preds = model(images)
            dice = dice_score(preds, masks)
            dice_per_batch.append(dice)
            predictions.append((images.cpu(), masks.cpu(), preds.cpu(), names))

    df = pd.DataFrame(columns=[f"Batch_{i}" for i in range(len(dice_per_batch))])
    df.loc["Dice"] = dice_per_batch
    df.to_excel(os.path.join(save_path, "test_dice_scores.xlsx"))

    return predictions


def plot_losses(train_losses, val_losses, save_path):
    plt.figure()
    plt.plot(train_losses, label="Train Loss")
    plt.plot(val_losses, label="Val Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid()
    plt.tight_layout()
    plt.savefig(os.path.join(save_path, "loss_curve.png"))
    plt.close()


def visualize_predictions(predictions, save_path):
    import matplotlib.pyplot as plt

    samples = predictions[:5]
    fig, axs = plt.subplots(5, 3, figsize=(12, 15))
    titles = ["Input Image", "Ground Truth", "Prediction"]

    for i, (img, mask, pred, name) in enumerate(samples):
        axs[i, 0].imshow(img[0][0], cmap='gray')
        axs[i, 1].imshow(mask[0][0], cmap='gray')
        axs[i, 2].imshow(pred[0][0] > 0.5, cmap='gray')
        for j in range(3):
            axs[i, j].axis("off")
            axs[i, j].set_title(titles[j])
            axs[i, j].text(0.5, -0.15, name[0], size=10, ha="center", transform=axs[i, j].transAxes)

    plt.tight_layout()
    plt.savefig(os.path.join(save_path, "test_visualization.png"))
    plt.close()
âœ… 4. main.py â€” Main Runner
# unet_segmentation/main.py

import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import train_test_split
from unet_segmentation.dataset import GrayscaleSegmentationDataset
from unet_segmentation.model import UNet
from unet_segmentation.train import *
from torchinfo import summary

if __name__ == "__main__":
    # Paths
    image_dir = "path/to/images"
    mask_dir = "path/to/masks"  # or same as image_dir
    save_path = "results/"
    os.makedirs(save_path, exist_ok=True)

    # Device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Dataset
    dataset = GrayscaleSegmentationDataset(image_dir=image_dir, mask_dir=mask_dir)
    indices = list(range(len(dataset)))
    train_idx, testval_idx = train_test_split(indices, test_size=0.2, random_state=42, shuffle=True)
    val_idx, test_idx = train_test_split(testval_idx, test_size=0.5, random_state=42)

    train_set = Subset(dataset, train_idx)
    val_set = Subset(dataset, val_idx)
    test_set = Subset(dataset, test_idx)

    print(f"Train: {len(train_set)} | Val: {len(val_set)} | Test: {len(test_set)}")

    train_loader = DataLoader(train_set, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_set, batch_size=8)
    test_loader = DataLoader(test_set, batch_size=1)

    print(f"Train loader: {len(train_loader)} | Val loader: {len(val_loader)} | Test loader: {len(test_loader)}")

    # Model
    model = UNet().to(device)
    summary(model, input_size=(1, 1, 256, 256))
    print(f"Total parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}")

    # Training settings
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.BCELoss()
    epochs = 20

    train_losses, val_losses = [], []
    start_time = time.time()

    for epoch in range(epochs):
        print(f"Epoch {epoch+1}/{epochs}")
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)
        val_loss = validate(model, val_loader, criterion, device, save_path, epoch)

        train_losses.append(train_loss)
        val_losses.append(val_loss)

    duration = time.time() - start_time
    print(f"Total training time: {duration:.2f} seconds")

    # Save model
    torch.save(model, os.path.join(save_path, "model_full.pth"))
    torch.save(model.state_dict(), os.path.join(save_path, "model_state_dict.pth"))

    # Save losses
    pd.DataFrame([list(range(1, epochs + 1)), train_losses]).to_excel(os.path.join(save_path, "train_losses.xlsx"), index=False, header=False)
    pd.DataFrame([list(range(1, epochs + 1)), val_losses]).to_excel(os.path.join(save_path, "val_losses.xlsx"), index=False, header=False)

    # Plot losses
    plot_losses(train_losses, val_losses, save_path)

    # Test
    predictions = test(model, test_loader, device, save_path)
    visualize_predictions(predictions, save_path)
âœ… Final Notes:
Make sure to replace "path/to/images" and "path/to/masks" with your actual dataset path.

Assumes images are grayscale .png files.

Handles same-name images/masks with _seg suffix.

Would you like this entire project zipped or any part explained/modified further (e.g., adding learning rate scheduler, augmentations, etc.)?

You said:
C:\Users\qy44lyfe\.conda\envs\Swallowenv\python.exe "D:/qy44lyfe/LLM segmentation/github repo/LLM_based_Segmentation/run_to_run variability test/GPT 4o-4/main.py"
Train: 799 | Val: 100 | Test: 100
Train loader: 100 | Val loader: 13 | Test loader: 100
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
UNet                                     [1, 1, 256, 256]          --
â”œâ”€DoubleConv: 1-1                        [1, 64, 256, 256]         --
â”‚    â””â”€Sequential: 2-1                   [1, 64, 256, 256]         --
â”‚    â”‚    â””â”€Conv2d: 3-1                  [1, 64, 256, 256]         640
â”‚    â”‚    â””â”€BatchNorm2d: 3-2             [1, 64, 256, 256]         128
â”‚    â”‚    â””â”€ReLU: 3-3                    [1, 64, 256, 256]         --
â”‚    â”‚    â””â”€Conv2d: 3-4                  [1, 64, 256, 256]         36,928
â”‚    â”‚    â””â”€BatchNorm2d: 3-5             [1, 64, 256, 256]         128
â”‚    â”‚    â””â”€ReLU: 3-6                    [1, 64, 256, 256]         --
â”œâ”€MaxPool2d: 1-2                         [1, 64, 128, 128]         --
â”œâ”€DoubleConv: 1-3                        [1, 128, 128, 128]        --
â”‚    â””â”€Sequential: 2-2                   [1, 128, 128, 128]        --
â”‚    â”‚    â””â”€Conv2d: 3-7                  [1, 128, 128, 128]        73,856
â”‚    â”‚    â””â”€BatchNorm2d: 3-8             [1, 128, 128, 128]        256
â”‚    â”‚    â””â”€ReLU: 3-9                    [1, 128, 128, 128]        --
â”‚    â”‚    â””â”€Conv2d: 3-10                 [1, 128, 128, 128]        147,584
â”‚    â”‚    â””â”€BatchNorm2d: 3-11            [1, 128, 128, 128]        256
â”‚    â”‚    â””â”€ReLU: 3-12                   [1, 128, 128, 128]        --
â”œâ”€MaxPool2d: 1-4                         [1, 128, 64, 64]          --
â”œâ”€DoubleConv: 1-5                        [1, 256, 64, 64]          --
â”‚    â””â”€Sequential: 2-3                   [1, 256, 64, 64]          --
â”‚    â”‚    â””â”€Conv2d: 3-13                 [1, 256, 64, 64]          295,168
â”‚    â”‚    â””â”€BatchNorm2d: 3-14            [1, 256, 64, 64]          512
â”‚    â”‚    â””â”€ReLU: 3-15                   [1, 256, 64, 64]          --
â”‚    â”‚    â””â”€Conv2d: 3-16                 [1, 256, 64, 64]          590,080
â”‚    â”‚    â””â”€BatchNorm2d: 3-17            [1, 256, 64, 64]          512
â”‚    â”‚    â””â”€ReLU: 3-18                   [1, 256, 64, 64]          --
â”œâ”€MaxPool2d: 1-6                         [1, 256, 32, 32]          --
â”œâ”€DoubleConv: 1-7                        [1, 512, 32, 32]          --
â”‚    â””â”€Sequential: 2-4                   [1, 512, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-19                 [1, 512, 32, 32]          1,180,160
â”‚    â”‚    â””â”€BatchNorm2d: 3-20            [1, 512, 32, 32]          1,024
â”‚    â”‚    â””â”€ReLU: 3-21                   [1, 512, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-22                 [1, 512, 32, 32]          2,359,808
â”‚    â”‚    â””â”€BatchNorm2d: 3-23            [1, 512, 32, 32]          1,024
â”‚    â”‚    â””â”€ReLU: 3-24                   [1, 512, 32, 32]          --
â”œâ”€MaxPool2d: 1-8                         [1, 512, 16, 16]          --
â”œâ”€DoubleConv: 1-9                        [1, 1024, 16, 16]         --
â”‚    â””â”€Sequential: 2-5                   [1, 1024, 16, 16]         --
â”‚    â”‚    â””â”€Conv2d: 3-25                 [1, 1024, 16, 16]         4,719,616
â”‚    â”‚    â””â”€BatchNorm2d: 3-26            [1, 1024, 16, 16]         2,048
â”‚    â”‚    â””â”€ReLU: 3-27                   [1, 1024, 16, 16]         --
â”‚    â”‚    â””â”€Conv2d: 3-28                 [1, 1024, 16, 16]         9,438,208
â”‚    â”‚    â””â”€BatchNorm2d: 3-29            [1, 1024, 16, 16]         2,048
â”‚    â”‚    â””â”€ReLU: 3-30                   [1, 1024, 16, 16]         --
â”œâ”€ConvTranspose2d: 1-10                  [1, 512, 32, 32]          2,097,664
â”œâ”€DoubleConv: 1-11                       [1, 512, 32, 32]          --
â”‚    â””â”€Sequential: 2-6                   [1, 512, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-31                 [1, 512, 32, 32]          4,719,104
â”‚    â”‚    â””â”€BatchNorm2d: 3-32            [1, 512, 32, 32]          1,024
â”‚    â”‚    â””â”€ReLU: 3-33                   [1, 512, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-34                 [1, 512, 32, 32]          2,359,808
â”‚    â”‚    â””â”€BatchNorm2d: 3-35            [1, 512, 32, 32]          1,024
â”‚    â”‚    â””â”€ReLU: 3-36                   [1, 512, 32, 32]          --
â”œâ”€ConvTranspose2d: 1-12                  [1, 256, 64, 64]          524,544
â”œâ”€DoubleConv: 1-13                       [1, 256, 64, 64]          --
â”‚    â””â”€Sequential: 2-7                   [1, 256, 64, 64]          --
â”‚    â”‚    â””â”€Conv2d: 3-37                 [1, 256, 64, 64]          1,179,904
â”‚    â”‚    â””â”€BatchNorm2d: 3-38            [1, 256, 64, 64]          512
â”‚    â”‚    â””â”€ReLU: 3-39                   [1, 256, 64, 64]          --
â”‚    â”‚    â””â”€Conv2d: 3-40                 [1, 256, 64, 64]          590,080
â”‚    â”‚    â””â”€BatchNorm2d: 3-41            [1, 256, 64, 64]          512
â”‚    â”‚    â””â”€ReLU: 3-42                   [1, 256, 64, 64]          --
â”œâ”€ConvTranspose2d: 1-14                  [1, 128, 128, 128]        131,200
â”œâ”€DoubleConv: 1-15                       [1, 128, 128, 128]        --
â”‚    â””â”€Sequential: 2-8                   [1, 128, 128, 128]        --
â”‚    â”‚    â””â”€Conv2d: 3-43                 [1, 128, 128, 128]        295,040
â”‚    â”‚    â””â”€BatchNorm2d: 3-44            [1, 128, 128, 128]        256
â”‚    â”‚    â””â”€ReLU: 3-45                   [1, 128, 128, 128]        --
â”‚    â”‚    â””â”€Conv2d: 3-46                 [1, 128, 128, 128]        147,584
â”‚    â”‚    â””â”€BatchNorm2d: 3-47            [1, 128, 128, 128]        256
â”‚    â”‚    â””â”€ReLU: 3-48                   [1, 128, 128, 128]        --
â”œâ”€ConvTranspose2d: 1-16                  [1, 64, 256, 256]         32,832
â”œâ”€DoubleConv: 1-17                       [1, 64, 256, 256]         --
â”‚    â””â”€Sequential: 2-9                   [1, 64, 256, 256]         --
â”‚    â”‚    â””â”€Conv2d: 3-49                 [1, 64, 256, 256]         73,792
â”‚    â”‚    â””â”€BatchNorm2d: 3-50            [1, 64, 256, 256]         128
â”‚    â”‚    â””â”€ReLU: 3-51                   [1, 64, 256, 256]         --
â”‚    â”‚    â””â”€Conv2d: 3-52                 [1, 64, 256, 256]         36,928
â”‚    â”‚    â””â”€BatchNorm2d: 3-53            [1, 64, 256, 256]         128
â”‚    â”‚    â””â”€ReLU: 3-54                   [1, 64, 256, 256]         --
â”œâ”€Conv2d: 1-18                           [1, 1, 256, 256]          65
==========================================================================================
Total params: 31,042,369
Trainable params: 31,042,369
Non-trainable params: 0
Total mult-adds (G): 54.57
==========================================================================================
Input size (MB): 0.26
Forward/backward pass size (MB): 575.14
Params size (MB): 124.17
Estimated Total Size (MB): 699.58
==========================================================================================
Training:   0%|          | 0/100 [00:00<?, ?it/s]Total parameters: 31042369
Epoch 1/2
Traceback (most recent call last):
  File "D:\qy44lyfe\LLM segmentation\github repo\LLM_based_Segmentation\run_to_run variability test\GPT 4o-4\main.py", line 56, in <module>
    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)
  File "D:\qy44lyfe\LLM segmentation\github repo\LLM_based_Segmentation\run_to_run variability test\GPT 4o-4\train.py", line 27, in train_one_epoch
    for images, masks, _ in loop:
  File "C:\Users\qy44lyfe\.conda\envs\Swallowenv\lib\site-packages\tqdm\std.py", line 1182, in __iter__
    for obj in iterable:
  File "C:\Users\qy44lyfe\.conda\envs\Swallowenv\lib\site-packages\torch\utils\data\dataloader.py", line 631, in __next__
    data = self._next_data()
  File "C:\Users\qy44lyfe\.conda\envs\Swallowenv\lib\site-packages\torch\utils\data\dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "C:\Users\qy44lyfe\.conda\envs\Swallowenv\lib\site-packages\torch\utils\data\_utils\fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "C:\Users\qy44lyfe\.conda\envs\Swallowenv\lib\site-packages\torch\utils\data\_utils\collate.py", line 277, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "C:\Users\qy44lyfe\.conda\envs\Swallowenv\lib\site-packages\torch\utils\data\_utils\collate.py", line 144, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "C:\Users\qy44lyfe\.conda\envs\Swallowenv\lib\site-packages\torch\utils\data\_utils\collate.py", line 144, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "C:\Users\qy44lyfe\.conda\envs\Swallowenv\lib\site-packages\torch\utils\data\_utils\collate.py", line 121, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "C:\Users\qy44lyfe\.conda\envs\Swallowenv\lib\site-packages\torch\utils\data\_utils\collate.py", line 174, in collate_tensor_fn
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 512, 512] at entry 0 and [1, 354, 318] at entry 7

Process finished with exit code 1
ChatGPT said:
Thanks for sharing the full traceback â€” this is a very common issue when working with medical images or datasets where image sizes vary. Here's a clear breakdown of the error and the fix.

âŒ Error Explanation
RuntimeError: stack expects each tensor to be equal size, but got [1, 512, 512] at entry 0 and [1, 354, 318] at entry 7
This error means that your dataset contains images (and/or masks) with different shapes (e.g. 512x512 vs 354x318), but DataLoader tries to stack them into a batch tensor â€” which fails unless all shapes match.

âœ… Solution: Resize Images and Masks in dataset.py
We should resize all images and masks to the same shape, e.g., 256x256, in the __getitem__ function of your GrayscaleSegmentationDataset.

ðŸ”§ Update dataset.py like this:
# unet_segmentation/dataset.py

import os
import glob
from PIL import Image
from torch.utils.data import Dataset
import torch
from torchvision import transforms


class GrayscaleSegmentationDataset(Dataset):
    def __init__(self, image_dir, mask_dir=None, suffix="_seg", target_size=(256, 256)):
        self.image_dir = image_dir
        self.mask_dir = mask_dir if mask_dir else image_dir
        self.suffix = suffix
        self.target_size = target_size

        self.image_paths = sorted([p for p in glob.glob(os.path.join(self.image_dir, "*.png")) if self.suffix not in os.path.basename(p)])

        self.image_transform = transforms.Compose([
            transforms.Grayscale(num_output_channels=1),
            transforms.Resize(self.target_size),
            transforms.ToTensor()
        ])

        self.mask_transform = transforms.Compose([
            transforms.Grayscale(num_output_channels=1),
            transforms.Resize(self.target_size),
            transforms.ToTensor()
        ])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        image_name = os.path.basename(image_path)
        mask_name = image_name.replace(".png", f"{self.suffix}.png")
        mask_path = os.path.join(self.mask_dir, mask_name)

        image = Image.open(image_path)
        mask = Image.open(mask_path)

        image = self.image_transform(image)
        mask = self.mask_transform(mask)
        mask = (mask > 0.5).float()  # Ensure binary mask

        return image, mask, image_name
ðŸ“Œ Optional: Parametrize target_size
If you want to make the image size easily configurable in main.py, you could also pass target_size as a parameter from main.py when instantiating the dataset.

âœ… What to do next
Update your dataset.py as shown above.

Re-run your training script (main.py).

You should see that training starts correctly without shape mismatch errors.

Would you like me to also:

Add automatic size detection and resizing based on the largest shape?

Add a warning if aspect ratios vary to avoid distortions?

Add padding instead of resizing to preserve aspect ratios?

Let me know how you'd like to handle it.




No file chosenNo file chosen
ChatGPT can make mistakes. Check important info. See Cookie Preferences.
